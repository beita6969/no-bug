#!/usr/bin/env python3
"""
Dataset-Specific Judge Prompt Loader
é’ˆå¯¹ä¸åŒæ•°æ®é›†åŠ è½½ä¸“å±çš„LLM Judgeæç¤ºè¯
"""
import yaml
from pathlib import Path
from typing import Dict, Optional


class JudgePromptLoader:
    """åŠ è½½å’Œç®¡ç†æ•°æ®é›†ä¸“å±çš„Judge Prompt"""

    def __init__(self, config_path: str = None):
        """
        Args:
            config_path: judge_prompts.yamlçš„è·¯å¾„
        """
        if config_path is None:
            # é»˜è®¤è·¯å¾„
            config_path = Path(__file__).parent.parent / "config" / "judge_prompts.yaml"

        self.config_path = Path(config_path)
        self.config = self._load_config()
        self.global_config = self.config.get('global', {})
        self.dataset_mapping = self.config.get('dataset_mapping', {}).get('by_source', {})

    def _load_config(self) -> Dict:
        """åŠ è½½YAMLé…ç½®æ–‡ä»¶"""
        try:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                return yaml.safe_load(f)
        except Exception as e:
            print(f"âš ï¸  æ— æ³•åŠ è½½Judgeé…ç½®æ–‡ä»¶ {self.config_path}: {e}")
            return {}

    def get_judge_prompt(
        self,
        source: Optional[str] = None,
        problem_type: Optional[str] = None
    ) -> str:
        """
        è·å–æ•°æ®é›†ä¸“å±çš„Judge Prompt

        Args:
            source: æ•°æ®é›†æ¥æºï¼ˆå¦‚'gsm8k', 'math', 'hotpotqa'ï¼‰
            problem_type: é—®é¢˜ç±»å‹ï¼ˆå¦‚'math', 'code', 'qa'ï¼‰- ç”¨äºfallback

        Returns:
            æ ¼å¼åŒ–åçš„Judge Prompt
        """
        # 1. å°è¯•æ ¹æ®sourceè·å–ä¸“å±é…ç½®
        dataset_key = None
        if source:
            source_lower = source.lower()
            dataset_key = self.dataset_mapping.get(source_lower)

        # 2. å¦‚æœæ‰¾åˆ°æ•°æ®é›†é…ç½®
        if dataset_key and dataset_key in self.config:
            dataset_config = self.config[dataset_key]
            if dataset_config.get('enabled', True):
                prompt = dataset_config.get('judge_prompt', '')
                if prompt:
                    # å…ˆæ³¨å…¥output_formatï¼ˆæ›¿æ¢{{output_format}}å ä½ç¬¦ï¼‰
                    output_format = self.global_config.get('output_format', '')
                    prompt = prompt.replace('{{output_format}}', output_format)
                    # è¿”å›çš„promptç°åœ¨åªåŒ…å«{{problem}}, {{prediction}}, {{ground_truth}}
                    return prompt

        # 3. Fallback: ä½¿ç”¨é€šç”¨promptï¼ˆæ ¹æ®problem_typeï¼‰
        return self._get_fallback_prompt(problem_type)

    def _get_fallback_prompt(self, problem_type: Optional[str]) -> str:
        """è·å–é€šç”¨çš„Fallback Prompt"""
        output_format = self.global_config.get('output_format', '')

        # æ ¹æ®é—®é¢˜ç±»å‹è¿”å›åŸºç¡€prompt
        if problem_type == 'math':
            base_prompt = """You are a mathematical equivalence evaluator.

**Task**: Determine if the predicted answer is mathematically equivalent to the ground truth.

**Prediction**: {prediction}
**Ground Truth**: {ground_truth}

**Evaluation Steps**:
1. Extract final numerical answers from both texts
2. Normalize formats (remove units, standardize notation)
3. Compare values with tolerance 0.01
"""
        elif problem_type == 'qa':
            base_prompt = """You are an answer equivalence evaluator.

**Task**: Determine if the predicted answer is equivalent to the ground truth.

**Prediction**: {prediction}
**Ground Truth**: {ground_truth}

**Evaluation Steps**:
1. Normalize both answers (lowercase, remove articles/punctuation)
2. Check for exact match or substring containment
3. Allow common entity variations
"""
        else:
            # é€šç”¨prompt
            base_prompt = """You are a precise answer equivalence evaluator.

**Task**: Determine if the predicted answer is equivalent to the ground truth.

**Prediction**: {prediction}
**Ground Truth**: {ground_truth}

**Evaluation**: Compare the semantic meaning of both answers.
"""

        return base_prompt + "\n" + output_format

    def get_dataset_config(self, source: Optional[str]) -> Dict:
        """è·å–å®Œæ•´çš„æ•°æ®é›†é…ç½®"""
        if not source:
            return {}

        source_lower = source.lower()
        dataset_key = self.dataset_mapping.get(source_lower)

        if dataset_key and dataset_key in self.config:
            return self.config[dataset_key]

        return {}

    def should_use_test_execution(self, source: Optional[str]) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥ä½¿ç”¨æµ‹è¯•æ‰§è¡Œè€ŒéLLM Judge"""
        if not source:
            return False

        dataset_config = self.get_dataset_config(source)

        # æ£€æŸ¥evaluation_methodå­—æ®µ
        eval_method = dataset_config.get('evaluation_method', '')
        return eval_method == 'test_execution'

    def get_stats(self) -> Dict:
        """è·å–é…ç½®ç»Ÿè®¡ä¿¡æ¯"""
        enabled_datasets = []
        disabled_datasets = []

        for dataset_key, config in self.config.items():
            if isinstance(config, dict) and 'enabled' in config:
                if config['enabled']:
                    enabled_datasets.append(dataset_key)
                else:
                    disabled_datasets.append(dataset_key)

        return {
            'total_datasets': len(enabled_datasets) + len(disabled_datasets),
            'enabled_datasets': enabled_datasets,
            'disabled_datasets': disabled_datasets,
            'dataset_mappings': self.dataset_mapping
        }


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    loader = JudgePromptLoader()

    print("ğŸ“‹ Judge Prompté…ç½®ç»Ÿè®¡:")
    stats = loader.get_stats()
    print(f"  æ€»æ•°æ®é›†: {stats['total_datasets']}")
    print(f"  å¯ç”¨: {', '.join(stats['enabled_datasets'])}")
    print(f"  ç¦ç”¨: {', '.join(stats['disabled_datasets'])}")

    print("\nğŸ” æµ‹è¯•ä¸åŒæ•°æ®é›†çš„Prompt:")

    # æµ‹è¯•GSM8K
    prompt = loader.get_judge_prompt(source='gsm8k', problem_type='math')
    print(f"\n[GSM8K] Prompté•¿åº¦: {len(prompt)} å­—ç¬¦")
    print(f"åŒ…å«'####': {'####' in prompt}")
    print(f"åŒ…å«'<<calc>>': {'<<calc>>' in prompt}")

    # æµ‹è¯•HotpotQA
    prompt = loader.get_judge_prompt(source='hotpotqa', problem_type='qa')
    print(f"\n[HotpotQA] Prompté•¿åº¦: {len(prompt)} å­—ç¬¦")
    print(f"åŒ…å«'PROHIBITION': {'PROHIBITION' in prompt}")
    print(f"åŒ…å«'option letter': {'option letter' in prompt}")

    # æµ‹è¯•Codeæ•°æ®é›†
    should_execute = loader.should_use_test_execution('humaneval')
    print(f"\n[HumanEval] æ˜¯å¦ä½¿ç”¨æµ‹è¯•æ‰§è¡Œ: {should_execute}")
