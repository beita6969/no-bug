#!/usr/bin/env python3
"""
WandBç›‘æ§ç³»ç»Ÿå®ç°ç¤ºä¾‹

è¿™ä¸ªæ–‡ä»¶åŒ…å«äº†åœ¨grpo_trainer.pyä¸­éœ€è¦æ·»åŠ çš„å…·ä½“ä»£ç ç‰‡æ®µ
"""

from collections import defaultdict
import numpy as np
import wandb


class DatasetMetricsCollector:
    """
    æ•°æ®é›†ç»´åº¦çš„æŒ‡æ ‡æ”¶é›†å™¨

    ä½¿ç”¨æ–¹æ³•:
        collector = DatasetMetricsCollector()
        collector.add_result(source='gsm8k', correctness=1.0, reward=1.0)
        metrics = collector.get_wandb_logs(step=100)
        wandb.log(metrics, step=100)
    """

    def __init__(self):
        self.reset()

    def reset(self):
        """é‡ç½®æ‰€æœ‰ç»Ÿè®¡æ•°æ®"""
        self.dataset_stats = defaultdict(lambda: {
            'correctness': [],
            'rewards': [],
            'costs': [],
        })

    def add_result(self, source: str, correctness: float, reward: float, cost: float = 0.0):
        """
        æ·»åŠ å•ä¸ªæ ·æœ¬ç»“æœ

        Args:
            source: æ•°æ®é›†æ¥æº (å¦‚'gsm8k', 'math', 'hotpotqa')
            correctness: æ­£ç¡®æ€§åˆ†æ•° (0.0 æˆ– 1.0)
            reward: å¥–åŠ±å€¼
            cost: æ‰§è¡Œæˆæœ¬
        """
        stats = self.dataset_stats[source]
        stats['correctness'].append(correctness)
        stats['rewards'].append(reward)
        stats['costs'].append(cost)

    def get_wandb_logs(self, step: int, prefix: str = "dataset") -> dict:
        """
        ç”ŸæˆWandBæ—¥å¿—å­—å…¸

        Args:
            step: å½“å‰è®­ç»ƒæ­¥æ•°
            prefix: æ—¥å¿—å‰ç¼€ (å¦‚'dataset'æˆ–'val')

        Returns:
            é€‚åˆwandb.log()çš„å­—å…¸
        """
        logs = {}

        for source, stats in self.dataset_stats.items():
            if not stats['correctness']:
                continue

            # è®¡ç®—å‡†ç¡®ç‡ (ä½¿ç”¨0.9é˜ˆå€¼é€‚åº”äºŒå…ƒå¥–åŠ±)
            num_correct = sum(1 for c in stats['correctness'] if c >= 0.9)
            num_total = len(stats['correctness'])
            accuracy = (num_correct / num_total * 100) if num_total > 0 else 0.0

            # è®¡ç®—å¹³å‡å¥–åŠ±
            avg_reward = np.mean(stats['rewards']) if stats['rewards'] else 0.0

            # è®¡ç®—å¹³å‡æˆæœ¬
            avg_cost = np.mean(stats['costs']) if stats['costs'] else 0.0

            # æ·»åŠ åˆ°æ—¥å¿—
            logs[f"{prefix}/{source}/accuracy"] = accuracy
            logs[f"{prefix}/{source}/count"] = num_total
            logs[f"{prefix}/{source}/avg_reward"] = avg_reward

            if avg_cost > 0:
                logs[f"{prefix}/{source}/avg_cost"] = avg_cost

        return logs

    def print_summary(self):
        """æ‰“å°ç»Ÿè®¡æ‘˜è¦ (ç”¨äºè°ƒè¯•)"""
        print("\nğŸ“Š æ•°æ®é›†ç»Ÿè®¡æ‘˜è¦:")
        for source, stats in sorted(self.dataset_stats.items()):
            if not stats['correctness']:
                continue

            num_correct = sum(1 for c in stats['correctness'] if c >= 0.9)
            num_total = len(stats['correctness'])
            accuracy = (num_correct / num_total * 100) if num_total > 0 else 0.0

            print(f"  {source:15s}: {num_correct:3d}/{num_total:3d} = {accuracy:5.1f}%")


class JudgeMetricsCollector:
    """
    LLM Judgeæ€§èƒ½ç›‘æ§å™¨

    ä½¿ç”¨æ–¹æ³•:
        collector = JudgeMetricsCollector()
        collector.update_from_reward_computer(reward_computer)
        metrics = collector.get_wandb_logs()
        wandb.log(metrics, step=step)
    """

    def __init__(self):
        self.judge_stats = {
            'total_evaluations': 0,
            'llm_judge_success': 0,
            'llm_judge_parse_failures': 0,
            'llm_judge_api_failures': 0,
            'correct_predictions': 0,
            'incorrect_predictions': 0,
        }

    def update_from_reward_computer(self, reward_computer):
        """
        ä»RewardComputerè¯»å–ç»Ÿè®¡æ•°æ®

        Args:
            reward_computer: RewardComputerå®ä¾‹
        """
        if hasattr(reward_computer, 'eval_stats'):
            self.judge_stats = reward_computer.eval_stats.copy()

    def get_wandb_logs(self) -> dict:
        """
        ç”ŸæˆWandBæ—¥å¿—å­—å…¸

        Returns:
            é€‚åˆwandb.log()çš„å­—å…¸
        """
        logs = {}
        total = self.judge_stats['total_evaluations']

        if total == 0:
            return logs

        # æˆåŠŸç‡ç»Ÿè®¡
        logs['judge/success_rate'] = self.judge_stats['llm_judge_success'] / total
        logs['judge/parse_failure_rate'] = self.judge_stats['llm_judge_parse_failures'] / total
        logs['judge/api_failure_rate'] = self.judge_stats['llm_judge_api_failures'] / total
        logs['judge/total_calls'] = total

        # åˆ¤å†³åˆ†å¸ƒ
        judged = self.judge_stats['correct_predictions'] + self.judge_stats['incorrect_predictions']
        if judged > 0:
            logs['judge/correct_ratio'] = self.judge_stats['correct_predictions'] / judged
            logs['judge/correct_count'] = self.judge_stats['correct_predictions']
            logs['judge/incorrect_count'] = self.judge_stats['incorrect_predictions']

        return logs

    def print_summary(self):
        """æ‰“å°ç»Ÿè®¡æ‘˜è¦ (ç”¨äºè°ƒè¯•)"""
        total = self.judge_stats['total_evaluations']
        if total == 0:
            print("\nğŸ¤– LLM Judgeç»Ÿè®¡: æ— è¯„ä¼°è®°å½•")
            return

        print(f"\nğŸ¤– LLM Judgeç»Ÿè®¡ (æ€»è®¡: {total} æ¬¡):")
        print(f"  æˆåŠŸ: {self.judge_stats['llm_judge_success']} ({self.judge_stats['llm_judge_success']/total*100:.1f}%)")
        print(f"  è§£æå¤±è´¥: {self.judge_stats['llm_judge_parse_failures']} ({self.judge_stats['llm_judge_parse_failures']/total*100:.1f}%)")
        print(f"  APIå¤±è´¥: {self.judge_stats['llm_judge_api_failures']} ({self.judge_stats['llm_judge_api_failures']/total*100:.1f}%)")

        judged = self.judge_stats['correct_predictions'] + self.judge_stats['incorrect_predictions']
        if judged > 0:
            accuracy = self.judge_stats['correct_predictions'] / judged * 100
            print(f"  åˆ¤å†³å‡†ç¡®ç‡: {accuracy:.1f}% (æ­£ç¡®: {self.judge_stats['correct_predictions']}, é”™è¯¯: {self.judge_stats['incorrect_predictions']})")


class CostTracker:
    """
    æˆæœ¬è¿½è¸ªå™¨

    ä½¿ç”¨æ–¹æ³•:
        tracker = CostTracker()
        tracker.add_cost(cost=0.01, is_executor=True)
        metrics = tracker.get_wandb_logs()
        wandb.log(metrics, step=step)
    """

    def __init__(self):
        self.reset()

    def reset(self):
        """é‡ç½®ç»Ÿè®¡"""
        self.total_cost = 0.0
        self.total_samples = 0
        self.executor_calls = 0
        self.judge_calls = 0

    def add_cost(self, cost: float, is_executor: bool = True):
        """
        æ·»åŠ æˆæœ¬è®°å½•

        Args:
            cost: æˆæœ¬å€¼
            is_executor: æ˜¯å¦ä¸ºexecutorè°ƒç”¨ (å¦åˆ™ä¸ºjudgeè°ƒç”¨)
        """
        self.total_cost += cost
        self.total_samples += 1

        if is_executor:
            self.executor_calls += 1
        else:
            self.judge_calls += 1

    def get_wandb_logs(self) -> dict:
        """
        ç”ŸæˆWandBæ—¥å¿—å­—å…¸

        Returns:
            é€‚åˆwandb.log()çš„å­—å…¸
        """
        logs = {
            'cost/total_cost': self.total_cost,
            'cost/total_samples': self.total_samples,
            'cost/executor_calls': self.executor_calls,
            'cost/judge_calls': self.judge_calls,
        }

        if self.total_samples > 0:
            logs['cost/avg_cost_per_sample'] = self.total_cost / self.total_samples

        return logs

    def print_summary(self):
        """æ‰“å°ç»Ÿè®¡æ‘˜è¦ (ç”¨äºè°ƒè¯•)"""
        print(f"\nğŸ’° æˆæœ¬ç»Ÿè®¡:")
        print(f"  æ€»æˆæœ¬: ${self.total_cost:.4f}")
        print(f"  æ ·æœ¬æ•°: {self.total_samples}")
        print(f"  Executorè°ƒç”¨: {self.executor_calls}")
        print(f"  Judgeè°ƒç”¨: {self.judge_calls}")

        if self.total_samples > 0:
            avg_cost = self.total_cost / self.total_samples
            print(f"  å¹³å‡æˆæœ¬/æ ·æœ¬: ${avg_cost:.6f}")


# ============================================================================
# é›†æˆåˆ°grpo_trainer.pyçš„ç¤ºä¾‹ä»£ç 
# ============================================================================

def example_integration_train_step():
    """
    æ¼”ç¤ºå¦‚ä½•åœ¨train_step()ä¸­é›†æˆæ•°æ®é›†ç›‘æ§

    è¿™æ®µä»£ç åº”è¯¥æ’å…¥åˆ°grpo_trainer.pyçš„train_step()æ–¹æ³•ä¸­
    """

    # ==================== åœ¨train_step()å¼€å§‹å¤„åˆå§‹åŒ– ====================
    # (æ’å…¥åˆ°ç¬¬307è¡Œå)

    # åˆå§‹åŒ–æ•°æ®é›†æŒ‡æ ‡æ”¶é›†å™¨
    dataset_collector = DatasetMetricsCollector()

    # ==================== åœ¨æ ·æœ¬å¾ªç¯ä¸­æ”¶é›†æ•°æ® ====================
    # (ä¿®æ”¹ç¬¬312-437è¡Œçš„å¾ªç¯)

    for sample_idx, sample in enumerate(batch):
        problem = sample['problem']
        ground_truth = sample['ground_truth']
        problem_type = sample['problem_type']
        source = sample.get('source', 'unknown')  # ğŸ†• è·å–æ•°æ®é›†æ¥æº

        # ... (åŸæœ‰çš„å·¥ä½œæµç”Ÿæˆå’Œæ‰§è¡Œä»£ç )

        # è®¡ç®—å¥–åŠ±å’Œæ­£ç¡®æ€§
        if metadata['success']:
            reward = self.reward_computer.compute_reward(
                problem=problem,
                prediction=answer,
                ground_truth=ground_truth,
                problem_type=problem_type,
                metadata=metadata,
                test=sample.get('test', ''),
                entry_point=sample.get('entry_point', ''),
                source=source  # ğŸ†• ä¼ é€’source
            )

            correctness = reward  # äºŒå…ƒå¥–åŠ±: 1.0æˆ–0.0

            # ğŸ†• è®°å½•åˆ°æ•°æ®é›†æ”¶é›†å™¨
            dataset_collector.add_result(
                source=source,
                correctness=correctness,
                reward=reward,
                cost=cost
            )

        # ... (å…¶ä½™ä»£ç )

    # ==================== åœ¨stepæœ«å°¾è®°å½•åˆ°WandB ====================
    # (æ’å…¥åˆ°ç¬¬513è¡Œå‰)

    # è·å–æ•°æ®é›†ç»´åº¦æŒ‡æ ‡
    dataset_logs = dataset_collector.get_wandb_logs(step=step, prefix="dataset")
    wandb_log_data.update(dataset_logs)

    # æ‰“å°æ•°æ®é›†ç»Ÿè®¡æ‘˜è¦
    dataset_collector.print_summary()

    # æœ€ç»ˆè®°å½•
    wandb.log(wandb_log_data, step=step)


def example_integration_evaluate_on_val_set():
    """
    æ¼”ç¤ºå¦‚ä½•åœ¨evaluate_on_val_set()ä¸­é›†æˆæ•°æ®é›†ç›‘æ§

    è¿™æ®µä»£ç åº”è¯¥æ’å…¥åˆ°grpo_trainer.pyçš„evaluate_on_val_set()æ–¹æ³•ä¸­
    """

    # ==================== åœ¨evaluate_on_val_set()å¼€å§‹å¤„åˆå§‹åŒ– ====================
    # (æ’å…¥åˆ°ç¬¬674è¡Œå)

    # åˆå§‹åŒ–éªŒè¯é›†æ•°æ®é›†æŒ‡æ ‡æ”¶é›†å™¨
    val_dataset_collector = DatasetMetricsCollector()

    # ==================== åœ¨æ ·æœ¬å¾ªç¯ä¸­æ”¶é›†æ•°æ® ====================
    # (ä¿®æ”¹ç¬¬678-736è¡Œçš„å¾ªç¯)

    for idx, sample in enumerate(val_batch):
        problem = sample['problem']
        ground_truth = sample['ground_truth']
        problem_type = sample['problem_type']
        source = sample.get('source', 'unknown')  # ğŸ†• è·å–æ•°æ®é›†æ¥æº

        # ... (åŸæœ‰çš„å·¥ä½œæµç”Ÿæˆå’Œæ‰§è¡Œä»£ç )

        # è®¡ç®—æ­£ç¡®æ€§
        if metadata['success']:
            correctness = self.reward_computer.compute_reward(
                problem=problem,
                prediction=answer,
                ground_truth=ground_truth,
                problem_type=problem_type,
                test=sample.get('test', ''),
                entry_point=sample.get('entry_point', ''),
                source=source  # ğŸ†• ä¼ é€’source
            )

            # ğŸ†• è®°å½•åˆ°éªŒè¯é›†æ”¶é›†å™¨
            val_dataset_collector.add_result(
                source=source,
                correctness=correctness,
                reward=correctness,
                cost=cost
            )

        # ... (å…¶ä½™ä»£ç )

    # ==================== åœ¨evaluate_on_val_set()æœ«å°¾è®°å½•åˆ°WandB ====================
    # (æ’å…¥åˆ°ç¬¬800è¡Œ)

    # è·å–éªŒè¯é›†æ•°æ®é›†ç»´åº¦æŒ‡æ ‡
    val_dataset_logs = val_dataset_collector.get_wandb_logs(step=step, prefix="val")
    wandb.log(val_dataset_logs, step=step)

    # æ‰“å°éªŒè¯é›†æ•°æ®é›†ç»Ÿè®¡æ‘˜è¦
    val_dataset_collector.print_summary()


def example_judge_monitoring():
    """
    æ¼”ç¤ºå¦‚ä½•åœ¨train_step()ä¸­ç›‘æ§LLM Judgeæ€§èƒ½

    è¿™æ®µä»£ç åº”è¯¥æ’å…¥åˆ°grpo_trainer.pyçš„train_step()æœ«å°¾
    """

    # ==================== åœ¨train_step()æœ«å°¾æ·»åŠ  ====================
    # (æ’å…¥åˆ°ç¬¬513è¡Œå‰)

    # ç›‘æ§LLM Judgeæ€§èƒ½ (å¦‚æœå¯ç”¨)
    if self.reward_computer.use_llm_judge:
        judge_collector = JudgeMetricsCollector()
        judge_collector.update_from_reward_computer(self.reward_computer)

        # è·å–JudgeæŒ‡æ ‡
        judge_logs = judge_collector.get_wandb_logs()
        wandb_log_data.update(judge_logs)

        # æ‰“å°Judgeç»Ÿè®¡æ‘˜è¦ (æ¯10æ­¥)
        if step % 10 == 0:
            judge_collector.print_summary()


def example_cost_tracking():
    """
    æ¼”ç¤ºå¦‚ä½•åœ¨GRPOTrainerä¸­æ·»åŠ æˆæœ¬è¿½è¸ª

    è¿™æ®µä»£ç åº”è¯¥æ·»åŠ åˆ°grpo_trainer.pyçš„__init__()å’Œtrain_step()ä¸­
    """

    # ==================== åœ¨__init__()ä¸­åˆå§‹åŒ– ====================
    # (æ’å…¥åˆ°ç¬¬79è¡Œå)

    # åˆå§‹åŒ–æˆæœ¬è¿½è¸ªå™¨
    self.cost_tracker = CostTracker()

    # ==================== åœ¨train_step()ä¸­è®°å½•æˆæœ¬ ====================
    # (åœ¨ç¬¬353è¡Œåï¼Œæ¯æ¬¡æ‰§è¡Œå)

    # è®°å½•æ‰§è¡Œæˆæœ¬
    self.cost_tracker.add_cost(cost=cost, is_executor=True)

    # ==================== åœ¨train_step()æœ«å°¾è®°å½•åˆ°WandB ====================
    # (æ’å…¥åˆ°ç¬¬513è¡Œå‰)

    # è·å–æˆæœ¬æŒ‡æ ‡
    cost_logs = self.cost_tracker.get_wandb_logs()
    wandb_log_data.update(cost_logs)

    # æ‰“å°æˆæœ¬ç»Ÿè®¡ (æ¯50æ­¥)
    if step % 50 == 0:
        self.cost_tracker.print_summary()


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    print("ğŸ§ª æµ‹è¯•DatasetMetricsCollector")
    collector = DatasetMetricsCollector()

    # æ·»åŠ ä¸€äº›æµ‹è¯•æ•°æ®
    collector.add_result('gsm8k', correctness=1.0, reward=1.0, cost=0.01)
    collector.add_result('gsm8k', correctness=1.0, reward=1.0, cost=0.02)
    collector.add_result('gsm8k', correctness=0.0, reward=0.0, cost=0.015)
    collector.add_result('math', correctness=1.0, reward=1.0, cost=0.03)
    collector.add_result('math', correctness=0.0, reward=0.0, cost=0.025)

    # è·å–æ—¥å¿—
    logs = collector.get_wandb_logs(step=100)
    print("\nWandBæ—¥å¿—:")
    for key, value in logs.items():
        print(f"  {key}: {value}")

    # æ‰“å°æ‘˜è¦
    collector.print_summary()

    print("\n" + "="*60)
    print("ğŸ§ª æµ‹è¯•JudgeMetricsCollector")

    # æ¨¡æ‹ŸRewardComputerçš„ç»Ÿè®¡æ•°æ®
    class MockRewardComputer:
        def __init__(self):
            self.eval_stats = {
                'total_evaluations': 100,
                'llm_judge_success': 85,
                'llm_judge_parse_failures': 10,
                'llm_judge_api_failures': 5,
                'correct_predictions': 60,
                'incorrect_predictions': 25,
            }

    mock_rc = MockRewardComputer()
    judge_collector = JudgeMetricsCollector()
    judge_collector.update_from_reward_computer(mock_rc)

    # è·å–æ—¥å¿—
    judge_logs = judge_collector.get_wandb_logs()
    print("\nWandBæ—¥å¿—:")
    for key, value in judge_logs.items():
        print(f"  {key}: {value}")

    # æ‰“å°æ‘˜è¦
    judge_collector.print_summary()

    print("\n" + "="*60)
    print("ğŸ§ª æµ‹è¯•CostTracker")

    tracker = CostTracker()
    tracker.add_cost(0.01, is_executor=True)
    tracker.add_cost(0.02, is_executor=True)
    tracker.add_cost(0.005, is_executor=False)  # Judgeè°ƒç”¨

    # è·å–æ—¥å¿—
    cost_logs = tracker.get_wandb_logs()
    print("\nWandBæ—¥å¿—:")
    for key, value in cost_logs.items():
        print(f"  {key}: {value}")

    # æ‰“å°æ‘˜è¦
    tracker.print_summary()

    print("\nâœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
