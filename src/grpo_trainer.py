#!/usr/bin/env python3
"""
GRPOè®­ç»ƒå™¨ - åœ¨çº¿å­¦ä¹ æ¨¡å¼çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒå™¨
"""
import torch
import torch.nn.functional as F
import asyncio
import numpy as np
import yaml
from pathlib import Path
from typing import Dict, List, Tuple, Optional
from tqdm import tqdm
import time
import json
import wandb  # âœ¨ æ–°å¢wandbé›†æˆ

from peft import LoraConfig, get_peft_model
from transformers import AutoModelForCausalLM, AutoTokenizer

from data_manager import DataManager
from rl_workflow_generator import RLWorkflowGenerator
from aflow_executor import AFlowExecutor
from reward_computer import RewardComputer
from gpu_manager import GPUManager
from experience_buffer import ExperienceBuffer
from prompt_optimizer import PromptOptimizer
from operator_prompt_enhancer import OperatorPromptEnhancer


class GRPOTrainer:
    """GRPOè®­ç»ƒå™¨ï¼šåœ¨çº¿å­¦ä¹ æ¨¡å¼"""

    def __init__(self, config_path: str = "config/training.yaml"):
        """
        Args:
            config_path: è®­ç»ƒé…ç½®æ–‡ä»¶è·¯å¾„
        """
        # åŠ è½½é…ç½®
        with open(config_path, 'r') as f:
            self.config = yaml.safe_load(f)

        print("=" * 60)
        print("ğŸš€ åˆå§‹åŒ–GRPOè®­ç»ƒå™¨")
        print("=" * 60)

        # GPUç®¡ç†ï¼ˆä½¿ç”¨ç‰©ç†GPU IDï¼‰
        physical_gpus = self.config.get('physical_gpus', self.config['device_mapping'])
        self.gpu_manager = GPUManager(
            target_gpus=physical_gpus,
            protected_pids=self.config.get('protected_pids', []),
            auto_clean=False  # ç¦ç”¨è‡ªåŠ¨æ¸…ç†
        )

        # è·³è¿‡GPUç¯å¢ƒéªŒè¯ï¼Œç›´æ¥ä½¿ç”¨
        print(f"âœ… ä½¿ç”¨GPU {physical_gpus}ï¼ˆå·²ç¦ç”¨æ¸…ç†å’ŒéªŒè¯ï¼‰")

        # Temperature schedulingé…ç½®
        temp_config = self.config.get('temperature_schedule', {})
        self.temp_schedule = {
            'enabled': temp_config.get('enabled', True),
            'initial': temp_config.get('initial', 0.3),
            'final': temp_config.get('final', 0.8),
            'warmup_steps': temp_config.get('warmup_steps', 100)
        }
        print(f"\nğŸŒ¡ï¸  Temperature Scheduling:")
        print(f"  Enabled: {self.temp_schedule['enabled']}")
        if self.temp_schedule['enabled']:
            print(f"  Range: {self.temp_schedule['initial']} â†’ {self.temp_schedule['final']}")
            print(f"  Warmup: {self.temp_schedule['warmup_steps']} steps")

        # âœ¨ åˆå§‹åŒ–wandb
        self._initialize_wandb()

        # åˆå§‹åŒ–ç»„ä»¶
        self._initialize_components()

        print("=" * 60)
        print("âœ… GRPOè®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ")
        print("=" * 60)

    def _initialize_wandb(self):
        """åˆå§‹åŒ–wandbç›‘æ§"""
        # ä»é…ç½®æˆ–ç¯å¢ƒå˜é‡è·å–wandbè®¾ç½®
        wandb_config = self.config.get('wandb', {})

        # è®¾ç½®API key(å¦‚æœæä¾›çš„è¯)
        wandb_api_key = wandb_config.get('api_key', 'b42ca0000cf06f97b05eba34f58823ad5f3122a4')

        # å°è¯•ç™»å½•,å¦‚æœå¤±è´¥åˆ™ä½¿ç”¨offlineæ¨¡å¼
        try:
            if wandb_api_key and len(wandb_api_key) == 40:
                wandb.login(key=wandb_api_key)
                mode = "online"
            else:
                print("âš ï¸  wandb API keyæ— æ•ˆæˆ–æœªæä¾›,ä½¿ç”¨offlineæ¨¡å¼")
                mode = "offline"
        except Exception as e:
            print(f"âš ï¸  wandbç™»å½•å¤±è´¥: {e}, ä½¿ç”¨offlineæ¨¡å¼")
            mode = "offline"

        # åˆå§‹åŒ–wandb run
        wandb.init(
            project=wandb_config.get('project', 'aflow-roll-integration'),
            name=wandb_config.get('run_name', f"grpo-training-{time.strftime('%Y%m%d-%H%M%S')}"),
            mode=mode,  # onlineæˆ–offline
            config={
                # è®­ç»ƒé…ç½®
                "base_model": self.config['base_model'],
                "learning_rate": self.config['learning_rate'],
                "batch_size": self.config['rollout_batch_size'],
                "num_sequences": self.config['num_return_sequences_in_group'],
                "max_steps": self.config['max_steps'],
                "lora_rank": self.config['lora_rank'],
                "lora_alpha": self.config['lora_alpha'],
                # æ•°æ®é…ç½®
                "domain_ratios": self.config['domain_ratios'],
                # å¥–åŠ±é…ç½®
                "reward_weights": self.config.get('reward_weights', {}),
            },
            tags=["grpo", "aflow", "roll", "workflow-generation"],
            notes="GRPO training with improved reward function (ROLL+AgentFlow design)"
        )

        print("\nâœ… wandbåˆå§‹åŒ–å®Œæˆ")
        print(f"  æ¨¡å¼: {mode}")
        print(f"  é¡¹ç›®: {wandb.run.project}")
        print(f"  Runåç§°: {wandb.run.name}")
        if mode == "online":
            print(f"  Run URL: {wandb.run.url}")
        else:
            print(f"  ç¦»çº¿æ—¥å¿—: wandb/offline-run-*")

    def _initialize_components(self):
        """åˆå§‹åŒ–æ‰€æœ‰ç»„ä»¶"""

        # 1. æ•°æ®ç®¡ç†å™¨
        print("\nğŸ“‚ åˆå§‹åŒ–æ•°æ®ç®¡ç†å™¨...")
        self.data_manager = DataManager(
            data_dir=self.config['data_dir'],
            domain_ratios=self.config['domain_ratios']
        )
        self.data_manager.initialize()

        # 2. RLæ¨¡å‹ï¼ˆQwen2.5-7B + LoRAï¼‰
        print("\nğŸ¤– åŠ è½½RLæ¨¡å‹...")
        self._load_rl_model()

        # 3. RLå·¥ä½œæµç”Ÿæˆå™¨ï¼ˆå…±äº«å·²åŠ è½½çš„æ¨¡å‹ï¼‰
        print("\nğŸ”§ åˆå§‹åŒ–å·¥ä½œæµç”Ÿæˆå™¨...")
        self.generator = RLWorkflowGenerator(
            base_model=self.config['base_model'],  # ä¼ é€’è·¯å¾„ç”¨äºåŠ è½½tokenizer
            device_ids=self.config['device_mapping'],
            operator_descriptions_path=self.config.get('aflow_operator_descriptions_path')
        )
        # å…±äº«å·²åŠ è½½çš„æ¨¡å‹ï¼ˆé¿å…é‡å¤åŠ è½½ï¼‰
        self.generator.model = self.model
        self.generator.tokenizer = self.tokenizer

        # 4. ExperienceBuffer - é«˜è´¨é‡æ ·æœ¬ç®¡ç†ï¼ˆéœ€å…ˆåˆå§‹åŒ–ï¼Œç”¨äºåç»­ç»„ä»¶ï¼‰
        print("\nğŸ“š åˆå§‹åŒ–ExperienceBuffer...")
        experience_config = self.config.get('experience_buffer', {})
        self.experience_buffer = ExperienceBuffer(
            buffer_size=experience_config.get('buffer_size', 100),
            reward_threshold=experience_config.get('reward_threshold', 8.0),
            persistence_dir=experience_config.get('persistence_dir', 'data/experience_buffer'),
            problem_types=["math", "code", "qa"]
        )
        print(f"  Bufferå¤§å°: {self.experience_buffer.buffer_size}")
        print(f"  å¥–åŠ±é˜ˆå€¼: {self.experience_buffer.reward_threshold}")

        # 5. PromptOptimizer - Layer 1åŠ¨æ€æç¤ºè¯ä¼˜åŒ–
        print("\nâœ¨ åˆå§‹åŒ–PromptOptimizer (Layer 1)...")
        prompt_config = self.config.get('prompt_optimizer', {})
        self.prompt_optimizer = PromptOptimizer()
        self.use_dynamic_prompts = prompt_config.get('enabled', True)
        print(f"  åŠ¨æ€æç¤ºè¯: {'å¯ç”¨' if self.use_dynamic_prompts else 'ç¦ç”¨'}")

        # 6. OperatorPromptEnhancer - Layer 2 operatoræç¤ºè¯å¢å¼º
        print("\nğŸ”§ åˆå§‹åŒ–OperatorPromptEnhancer (Layer 2)...")
        operator_config = self.config.get('operator_prompt_enhancer', {})
        self.operator_enhancer = OperatorPromptEnhancer(
            enable_enhancement=operator_config.get('enabled', True)
        )
        print(f"  Operatorå¢å¼º: {'å¯ç”¨' if self.operator_enhancer.enable_enhancement else 'ç¦ç”¨'}")

        # 7. AFlowæ‰§è¡Œå™¨ï¼ˆä¼ å…¥operator_enhancerï¼‰
        print("\nâš™ï¸  åˆå§‹åŒ–AFlowæ‰§è¡Œå™¨...")
        timeout = self.config.get('execution_timeout', 180)  # é»˜è®¤180ç§’
        self.executor = AFlowExecutor(
            llm_config_path=self.config['aflow_config_path'],
            timeout=timeout,
            operator_enhancer=self.operator_enhancer  # ä¼ é€’Layer 2å¢å¼ºå™¨
        )
        print(f"  æ‰§è¡Œè¶…æ—¶: {timeout}ç§’")

        # 8. å¥–åŠ±è®¡ç®—å™¨
        print("\nğŸ¯ åˆå§‹åŒ–å¥–åŠ±è®¡ç®—å™¨...")
        self.reward_computer = RewardComputer(
            reward_weights=self.config.get('reward_weights'),
            use_llm_judge=True,  # å¯ç”¨LLM Judge (GPT OSS 120B @ port 8002)
            llm_config={
                "base_url": "http://localhost:8002/v1",
                "api_key": "sk-dummy",
                "model_name": "/home/yijia/lhy/openai/gpt-oss-120b"
            }
        )

        # 9. ä¼˜åŒ–å™¨
        print("\nğŸ”¬ åˆå§‹åŒ–ä¼˜åŒ–å™¨...")
        self.optimizer = torch.optim.AdamW(
            self.model.parameters(),
            lr=self.config['learning_rate'],
            weight_decay=self.config.get('weight_decay', 0.01)
        )

    def _load_rl_model(self):
        """åŠ è½½RLæ¨¡å‹ï¼ˆQwen2.5-7B + LoRAï¼‰"""
        device = f"cuda:{self.config['device_mapping'][0]}"

        # åŠ è½½tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.config['base_model'],
            trust_remote_code=True
        )
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token

        # åŠ è½½åŸºåº§æ¨¡å‹
        self.model = AutoModelForCausalLM.from_pretrained(
            self.config['base_model'],
            torch_dtype=torch.bfloat16 if self.config.get('bf16') else torch.float16,
            device_map={"": device},
            trust_remote_code=True
        )

        # åº”ç”¨LoRA
        if self.config.get('use_lora', True):
            lora_config = LoraConfig(
                r=self.config['lora_rank'],
                lora_alpha=self.config['lora_alpha'],
                target_modules=self.config['lora_target_modules'].split(','),
                lora_dropout=self.config['lora_dropout'],
                bias="none",
                task_type="CAUSAL_LM"
            )
            self.model = get_peft_model(self.model, lora_config)

            print(f"âœ… LoRAåº”ç”¨å®Œæˆ")
            self.model.print_trainable_parameters()

    def get_current_temperature(self, step: int) -> float:
        """
        è®¡ç®—å½“å‰stepçš„temperature

        ç­–ç•¥: çº¿æ€§ä»initialå‡è‡³final
        - æ—©æœŸ: ä½æ¸©åº¦ç”Ÿæˆç¡®å®šæ€§workflowï¼Œå»ºç«‹baseline
        - åæœŸ: é«˜æ¸©åº¦æ¢ç´¢å¤šæ ·æ€§workflow

        Args:
            step: å½“å‰è®­ç»ƒæ­¥æ•°

        Returns:
            å½“å‰çš„temperatureå€¼
        """
        if not self.temp_schedule['enabled']:
            return self.config['generation_config']['temperature']

        if step < self.temp_schedule['warmup_steps']:
            # Linear warmup
            progress = step / self.temp_schedule['warmup_steps']
            temp = (self.temp_schedule['initial'] +
                   progress * (self.temp_schedule['final'] - self.temp_schedule['initial']))
        else:
            temp = self.temp_schedule['final']

        return temp

    async def train_step(self, step: int) -> Dict:
        """
        å•æ­¥GRPOè®­ç»ƒï¼ˆåœ¨çº¿å­¦ä¹ ï¼‰

        Returns:
            metrics: è®­ç»ƒæŒ‡æ ‡
        """

        # 1. é‡‡æ ·batch
        batch = self.data_manager.sample_batch(
            batch_size=self.config['rollout_batch_size'],
            split="train"
        )

        # ç»Ÿè®¡
        batch_stats = self.data_manager.get_batch_stats(batch)
        print(f"\nğŸ“¦ Batch {step}: {len(batch)} æ ·æœ¬, åˆ†å¸ƒ: {batch_stats}")

        # è·å–å½“å‰temperatureï¼ˆåŠ¨æ€è°ƒåº¦ï¼‰
        current_temp = self.get_current_temperature(step)
        print(f"ğŸŒ¡ï¸  Temperature: {current_temp:.3f}")

        # 2. ä¸ºæ¯ä¸ªé—®é¢˜ç”ŸæˆKä¸ªå·¥ä½œæµï¼ˆGRPOç»„ï¼‰
        all_workflows = []
        all_problems = []
        all_answers = []
        all_rewards = []
        all_log_probs = []

        # âœ¨ æ–°å¢ï¼šå‡†ç¡®ç‡ç»Ÿè®¡
        correctness_scores = []  # å­˜å‚¨æ‰€æœ‰æ­£ç¡®æ€§åˆ†æ•°

        num_sequences = self.config['num_return_sequences_in_group']

        for sample_idx, sample in enumerate(tqdm(batch, desc="ç”Ÿæˆå’Œæ‰§è¡Œå·¥ä½œæµ"), 1):
            problem = sample['problem']
            ground_truth = sample['ground_truth']
            problem_type = sample['problem_type']

            # GRPOç»„
            group_workflows = []
            group_answers = []
            group_rewards = []
            group_log_probs = []
            group_correctness = []

            for i in range(num_sequences):
                # æ„å»ºåŠ¨æ€æç¤ºè¯ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                custom_prompt = None
                if self.use_dynamic_prompts:
                    custom_prompt = self.prompt_optimizer.build_dynamic_prompt(
                        problem=problem,
                        problem_type=problem_type
                    )

                # ç”Ÿæˆå·¥ä½œæµ
                result = self.generator.generate_workflow(
                    problem=problem,
                    problem_type=problem_type,
                    temperature=current_temp,  # ä½¿ç”¨åŠ¨æ€temperature
                    custom_prompt=custom_prompt
                )

                workflow_code = result['workflow_code']

                # è®¡ç®—logæ¦‚ç‡ï¼ˆæ—§ç­–ç•¥ï¼‰
                log_prob = await self._compute_log_prob(problem, workflow_code, problem_type)

                # æ‰§è¡Œå·¥ä½œæµ
                try:
                    answer, cost, metadata = await self.executor.execute_workflow(
                        workflow_code=workflow_code,
                        problem=problem,
                        problem_type=problem_type,
                        entry_point=sample.get('entry_point', ''),
                        test=sample.get('test', '')  # NEW: pass test cases for HumanEval
                    )

                    # è®¡ç®—å¥–åŠ±
                    if metadata['success']:
                        reward = self.reward_computer.compute_reward(
                            problem=problem,
                            prediction=answer,
                            ground_truth=ground_truth,
                            problem_type=problem_type,
                            metadata=metadata,
                            test=sample.get('test', ''),
                            entry_point=sample.get('entry_point', ''),
                            source=sample.get('source', None)  # ğŸ†• ä¼ é€’æ•°æ®é›†æ¥æº
                        )

                        # âœ¨ æ–°å¢ï¼šæ˜¾å¼è®¡ç®—å¹¶è®°å½•æ­£ç¡®æ€§
                        # compute_reward ç°åœ¨è¿”å› 1.0 (æ­£ç¡®) æˆ– 0.0 (é”™è¯¯)
                        correctness = reward 
                        correctness_scores.append(correctness)
                        group_correctness.append(correctness)

                        # åˆ¤æ–­æ˜¯å¦æ­£ç¡®ï¼ˆcorrectness == 1.0ï¼‰
                        is_correct = correctness > 0.5
                        status_icon = "âœ…" if is_correct else "âŒ"

                        # å®æ—¶æ—¥å¿—åˆ° wandb (æ ·æœ¬çº§åˆ«)
                        wandb.log({
                            f"sample/{problem_type}/correctness": correctness,
                            f"sample/{problem_type}/reward": reward,
                            f"sample/step": step,
                            f"sample/sample_id": sample_idx * 4 + i,
                        })

                        print(f"  {status_icon} æ­£ç¡®æ€§è¯„åˆ†: {correctness:.1f}/1.0 | é¢„æµ‹: {str(answer)[:50]} | çœŸå€¼: {str(ground_truth)[:50]}")
                    else:
                        reward = 0.0  # æ‰§è¡Œå¤±è´¥æƒ©ç½š
                        correctness = 0.0 # ç¡®ä¿correctnessè¢«å®šä¹‰
                        correctness_scores.append(0.0)
                        group_correctness.append(0.0)
                        print(f"  âŒ æ‰§è¡Œå¤±è´¥ | çœŸå€¼: {str(ground_truth)[:50]}")

                except Exception as e:
                    print(f"  âš ï¸  æ‰§è¡Œé”™è¯¯: {type(e).__name__}: {e}")
                    import traceback
                    traceback.print_exc()
                    answer = None
                    reward = -10.0
                    correctness_scores.append(-10.0)
                    group_correctness.append(-10.0)

                group_workflows.append(workflow_code)
                group_answers.append(answer)
                group_rewards.append(reward)
                group_log_probs.append(log_prob)

            # GRPOå…³é”®ï¼šç»„å†…å¥–åŠ±å½’ä¸€åŒ–
            mean_reward = np.mean(group_rewards)
            group_advantages = [r - mean_reward for r in group_rewards]

            # ğŸ’¾ æ”¶é›†é«˜è´¨é‡æ ·æœ¬åˆ°ExperienceBuffer
            for idx, (workflow, answer, reward) in enumerate(zip(group_workflows, group_answers, group_rewards)):
                # åªæ”¶é›†åŸå§‹å¥–åŠ±é«˜çš„æ ·æœ¬ï¼ˆéadvantageï¼‰
                if reward >= self.experience_buffer.reward_threshold:
                    sample = {
                        'problem': problem,
                        'workflow_code': workflow,
                        'answer': answer,
                        'ground_truth': ground_truth,
                        'reward': reward,
                        'correctness_score': correctness_scores[-len(group_rewards) + idx] if correctness_scores else 0,
                        'metadata': {
                            'problem_type': problem_type,
                            'step': step
                        },
                        'step': step
                    }
                    self.experience_buffer.add_sample(sample, problem_type)

            # æ”¶é›†
            all_workflows.extend(group_workflows)
            all_problems.extend([problem] * num_sequences)
            all_answers.extend(group_answers)
            all_rewards.extend(group_advantages)  # ä½¿ç”¨ä¼˜åŠ¿
            all_log_probs.extend(group_log_probs)

        # 3. ç­–ç•¥æ¢¯åº¦æ›´æ–°
        print(f"\nğŸ”„ æ›´æ–°ç­–ç•¥...")
        loss, kl_div = await self._update_policy(
            problems=all_problems,
            workflows=all_workflows,
            old_log_probs=all_log_probs,
            advantages=all_rewards,
            problem_types=[s['problem_type'] for s in batch for _ in range(num_sequences)]
        )

        # 4. æŒ‡æ ‡
        # âœ¨ æ–°å¢ï¼šè®¡ç®—å‡†ç¡®ç‡ç»Ÿè®¡
        num_correct = sum(1 for score in correctness_scores if score >= 0.9) # ä¿®æ”¹é˜ˆå€¼ä¸º0.9é€‚åº”äºŒå…ƒå¥–åŠ±
        num_total = len(correctness_scores)
        accuracy = (num_correct / num_total * 100) if num_total > 0 else 0.0
        avg_correctness = np.mean(correctness_scores) if correctness_scores else 0.0

        # è®¡ç®—é—®é¢˜ç±»å‹åˆ†å¸ƒçš„å‡†ç¡®ç‡
        problem_type_stats = {}
        for problem_type in ['math', 'code', 'qa']:
            type_scores = [s for s, p in zip(correctness_scores,
                          [s['problem_type'] for s in batch for _ in range(num_sequences)])
                          if p == problem_type]
            if type_scores:
                type_correct = sum(1 for s in type_scores if s >= 0.9) # ä¿®æ”¹é˜ˆå€¼
                type_accuracy = (type_correct / len(type_scores) * 100)
                type_avg = np.mean(type_scores)
                problem_type_stats[problem_type] = {
                    "accuracy": type_accuracy,
                    "avg_score": type_avg,
                    "count": len(type_scores)
                }

        metrics = {
            "step": step,
            "loss": loss,
            "kl_div": kl_div,
            "avg_reward": np.mean(all_rewards),
            "max_reward": np.max(all_rewards),
            "min_reward": np.min(all_rewards),
            "num_samples": len(all_workflows),
            # âœ¨ æ–°å¢å‡†ç¡®ç‡æŒ‡æ ‡
            "accuracy": accuracy,
            "num_correct": num_correct,
            "num_total": num_total,
            "avg_correctness_score": avg_correctness
        }

        print(f"\nğŸ¯ å‡†ç¡®ç‡ç»Ÿè®¡: {num_correct}/{num_total} = {accuracy:.1f}% (å¹³å‡æ­£ç¡®æ€§è¯„åˆ†: {avg_correctness:.2f}/10.0)")
        print(f"\nğŸ“Š é—®é¢˜ç±»å‹åˆ†å¸ƒ:")
        for ptype, stats in problem_type_stats.items():
            print(f"  {ptype}: {stats['accuracy']:.1f}% (avg: {stats['avg_score']:.2f}, n={stats['count']})")

        # âœ¨ è¯¦ç»† wandb logging (å®æ—¶ä»ªè¡¨æ¿)
        wandb_log_data = {
            "train/loss": loss,
            "train/kl_div": kl_div,
            "train/avg_reward": np.mean(all_rewards),
            "train/max_reward": np.max(all_rewards),
            "train/min_reward": np.min(all_rewards),
            "train/accuracy": accuracy,
            "train/avg_correctness_score": avg_correctness,
            "train/num_correct": num_correct,
            "train/num_total": num_total,
            "train/temperature": current_temp,  # è®°å½•å½“å‰temperature
            "train/step": step,
        }

        # æ·»åŠ é—®é¢˜ç±»å‹çš„åˆ†å¸ƒæŒ‡æ ‡
        for ptype, stats in problem_type_stats.items():
            wandb_log_data[f"train/accuracy_{ptype}"] = stats['accuracy']
            wandb_log_data[f"train/avg_score_{ptype}"] = stats['avg_score']
            wandb_log_data[f"train/count_{ptype}"] = stats['count']

        wandb.log(wandb_log_data, step=step)

        return metrics

    async def _compute_log_prob(
        self,
        problem: str,
        workflow_code: str,
        problem_type: str
    ) -> torch.Tensor:
        """è®¡ç®—å·¥ä½œæµçš„logæ¦‚ç‡ï¼ˆæ—§ç­–ç•¥ï¼‰"""

        self.model.eval()

        with torch.no_grad():
            # æ„å»ºå®Œæ•´æ–‡æœ¬
            prompt = self.generator._build_generation_prompt(problem, problem_type)
            full_text = prompt + workflow_code

            # Tokenize
            inputs = self.tokenizer(full_text, return_tensors="pt").to(self.model.device)

            # å‰å‘ä¼ æ’­
            outputs = self.model(**inputs, labels=inputs["input_ids"])

            # è´Ÿå¯¹æ•°ä¼¼ç„¶ -> logæ¦‚ç‡
            log_prob = -outputs.loss

            return log_prob.detach().cpu()

    async def _update_policy(
        self,
        problems: List[str],
        workflows: List[str],
        old_log_probs: List[torch.Tensor],
        advantages: List[float],
        problem_types: List[str]
    ) -> Tuple[float, float]:
        """æ›´æ–°ç­–ç•¥ï¼ˆGRPOï¼‰"""

        self.model.train()

        total_loss = 0.0
        total_kl = 0.0
        num_updates = 0

        # æ¢¯åº¦ç´¯ç§¯
        grad_accum_steps = self.config.get('gradient_accumulation_steps', 1)

        for i in range(0, len(workflows), grad_accum_steps):
            batch_slice = slice(i, min(i + grad_accum_steps, len(workflows)))

            batch_loss = 0.0
            batch_kl = 0.0

            for j in range(i, min(i + grad_accum_steps, len(workflows))):
                problem = problems[j]
                workflow = workflows[j]
                old_log_prob = old_log_probs[j]
                advantage = advantages[j]
                problem_type = problem_types[j]

                # è®¡ç®—æ–°logæ¦‚ç‡
                new_log_prob = await self._compute_log_prob_trainable(problem, workflow, problem_type)

                # é‡è¦æ€§é‡‡æ ·æ¯”
                ratio = torch.exp(new_log_prob - old_log_prob.to(self.model.device))

                # PPOè£å‰ªæŸå¤±
                clip_range = self.config['clip_range']
                clipped_ratio = torch.clamp(ratio, 1.0 - clip_range, 1.0 + clip_range)

                advantage_tensor = torch.tensor(advantage, device=self.model.device)
                policy_loss = -torch.min(
                    ratio * advantage_tensor,
                    clipped_ratio * advantage_tensor
                )

                # KLæ­£åˆ™åŒ–
                if self.config.get('use_kl_loss'):
                    kl_loss = self.config['kl_loss_coef'] * (new_log_prob - old_log_prob.to(self.model.device)).pow(2)
                else:
                    kl_loss = 0.0

                # æ€»æŸå¤±
                loss = policy_loss + kl_loss

                # ç´¯ç§¯
                batch_loss += loss
                batch_kl += kl_loss if isinstance(kl_loss, torch.Tensor) else 0.0

            # å¹³å‡
            batch_loss = batch_loss / min(grad_accum_steps, len(workflows) - i)

            # åå‘ä¼ æ’­
            batch_loss.backward()

            total_loss += batch_loss.item()
            total_kl += batch_kl.item() if isinstance(batch_kl, torch.Tensor) else batch_kl
            num_updates += 1

            # ä¼˜åŒ–å™¨æ­¥éª¤
            if (i + grad_accum_steps) % grad_accum_steps == 0:
                # æ¢¯åº¦è£å‰ª
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.get('max_grad_norm', 1.0))
                self.optimizer.step()
                self.optimizer.zero_grad()

        avg_loss = total_loss / max(num_updates, 1)
        avg_kl = total_kl / max(num_updates, 1)

        return avg_loss, avg_kl

    async def _compute_log_prob_trainable(
        self,
        problem: str,
        workflow_code: str,
        problem_type: str
    ) -> torch.Tensor:
        """è®¡ç®—å·¥ä½œæµçš„logæ¦‚ç‡ï¼ˆæ–°ç­–ç•¥ï¼Œå¯è®­ç»ƒï¼‰"""

        # æ„å»ºå®Œæ•´æ–‡æœ¬
        prompt = self.generator._build_generation_prompt(problem, problem_type)
        full_text = prompt + workflow_code

        # Tokenize
        inputs = self.tokenizer(full_text, return_tensors="pt").to(self.model.device)

        # å‰å‘ä¼ æ’­
        outputs = self.model(**inputs, labels=inputs["input_ids"])

        # è´Ÿå¯¹æ•°ä¼¼ç„¶ -> logæ¦‚ç‡
        log_prob = -outputs.loss

        return log_prob

    async def evaluate_on_val_set(self, num_samples: int = 50) -> Dict:
        """
        åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°æ¨¡å‹æ€§èƒ½

        Args:
            num_samples: éªŒè¯æ ·æœ¬æ•°é‡

        Returns:
            éªŒè¯æŒ‡æ ‡å­—å…¸
        """
        print(f"\n{'='*60}")
        print(f"ğŸ§ª éªŒè¯é›†è¯„ä¼° ({num_samples}ä¸ªæ ·æœ¬)")
        print(f"{'='*60}")

        # é‡‡æ ·éªŒè¯é›†
        val_batch = self.data_manager.sample_batch(
            batch_size=num_samples,
            split="val"  # ä½¿ç”¨éªŒè¯é›†
        )

        # ç»Ÿè®¡
        batch_stats = self.data_manager.get_batch_stats(val_batch)
        print(f"ğŸ“¦ éªŒè¯é›†åˆ†å¸ƒ: {batch_stats}")

        # è¯„ä¼°æ¯ä¸ªæ ·æœ¬
        correctness_scores = []
        total_cost = 0.0
        successful_executions = 0

        for idx, sample in enumerate(tqdm(val_batch, desc="éªŒè¯é›†è¯„ä¼°"), 1):
            problem = sample['problem']
            ground_truth = sample['ground_truth']
            problem_type = sample['problem_type']

            try:
                # ä½¿ç”¨å½“å‰ç­–ç•¥ç”Ÿæˆworkflowï¼ˆä½¿ç”¨åŠ¨æ€æç¤ºè¯ï¼‰
                custom_prompt = None
                if self.use_dynamic_prompts:
                    custom_prompt = self.prompt_optimizer.build_dynamic_prompt(
                        problem=problem,
                        problem_type=problem_type
                    )

                result = self.generator.generate_workflow(
                    problem=problem,
                    problem_type=problem_type,
                    temperature=self.config['generation_config']['temperature'],
                    custom_prompt=custom_prompt
                )

                workflow_code = result['workflow_code']

                # æ‰§è¡Œworkflow
                answer, cost, metadata = await self.executor.execute_workflow(
                    workflow_code=workflow_code,
                    problem=problem,
                    problem_type=problem_type,
                    entry_point=sample.get('entry_point', ''),
                    test=sample.get('test', '')  # NEW: pass test cases for HumanEval
                )

                # è®¡ç®—æ­£ç¡®æ€§
                if metadata['success']:
                    correctness = self.reward_computer.compute_reward(
                        problem=problem,
                        prediction=answer,
                        ground_truth=ground_truth,
                        problem_type=problem_type,
                        test=sample.get('test', ''),
                        entry_point=sample.get('entry_point', ''),
                        source=sample.get('source', None)  # ğŸ†• ä¼ é€’æ•°æ®é›†æ¥æº
                    )
                    correctness_scores.append(correctness)
                    total_cost += cost
                    successful_executions += 1

                    is_correct = correctness > 0.5
                    status_icon = "âœ…" if is_correct else "âŒ"
                    if idx <= 5:  # åªæ‰“å°å‰5ä¸ªæ ·æœ¬çš„è¯¦æƒ…
                        print(f"  {status_icon} [{idx}/{num_samples}] æ­£ç¡®æ€§: {correctness:.1f}/1.0")
                else:
                    correctness_scores.append(0.0)
                    if idx <= 5:
                        print(f"  âŒ [{idx}/{num_samples}] æ‰§è¡Œå¤±è´¥")

            except Exception as e:
                print(f"  âš ï¸  [{idx}/{num_samples}] é”™è¯¯: {type(e).__name__}")
                correctness_scores.append(0.0)

        # è®¡ç®—æŒ‡æ ‡
        num_correct = sum(1 for score in correctness_scores if score >= 0.9)  # Binary reward: 0.9 threshold for 1.0 scores
        val_accuracy = (num_correct / num_samples * 100) if num_samples > 0 else 0.0
        avg_correctness = np.mean(correctness_scores) if correctness_scores else 0.0
        avg_cost = total_cost / successful_executions if successful_executions > 0 else 0.0
        success_rate = (successful_executions / num_samples * 100) if num_samples > 0 else 0.0

        metrics = {
            "val_accuracy": val_accuracy,
            "val_num_correct": num_correct,
            "val_num_total": num_samples,
            "val_avg_correctness": avg_correctness,
            "val_avg_cost": avg_cost,
            "val_success_rate": success_rate
        }

        print(f"\nğŸ“Š éªŒè¯é›†ç»“æœ:")
        print(f"  å‡†ç¡®ç‡: {num_correct}/{num_samples} = {val_accuracy:.1f}%")
        print(f"  å¹³å‡æ­£ç¡®æ€§: {avg_correctness:.2f}/10.0")
        print(f"  æ‰§è¡ŒæˆåŠŸç‡: {success_rate:.1f}%")
        print(f"  å¹³å‡æˆæœ¬: ${avg_cost:.4f}")
        print(f"{'='*60}\n")

        return metrics

    async def train(self):
        """å®Œæ•´è®­ç»ƒå¾ªç¯"""
        print("\n" + "=" * 60)
        print("ğŸ“ å¼€å§‹GRPOè®­ç»ƒ")
        print("=" * 60)

        max_steps = self.config['max_steps']
        save_every = self.config.get('save_every', 50)
        log_every = self.config.get('log_every', 5)
        eval_every = self.config.get('eval_every', 10)  # æ¯10æ­¥éªŒè¯ä¸€æ¬¡
        val_samples = self.config.get('val_samples', 50)  # éªŒè¯é›†æ ·æœ¬æ•°

        for step in range(1, max_steps + 1):
            print(f"\n{'=' * 60}")
            print(f"ğŸ“ Step {step}/{max_steps}")
            print(f"{'=' * 60}")

            # è®­ç»ƒæ­¥éª¤
            metrics = await self.train_step(step)

            # æ—¥å¿—
            if step % log_every == 0:
                print(f"\nğŸ“Š Metrics:")
                for key, value in metrics.items():
                    print(f"  {key}: {value:.4f}" if isinstance(value, float) else f"  {key}: {value}")

                # è®°å½•åˆ°wandb
                wandb.log(metrics, step=step)

            # ğŸ§ª éªŒè¯é›†è¯„ä¼°ï¼ˆæ¯Næ­¥ï¼‰
            if step % eval_every == 0:
                val_metrics = await self.evaluate_on_val_set(num_samples=val_samples)

                # åˆå¹¶éªŒè¯æŒ‡æ ‡åˆ°è®­ç»ƒæŒ‡æ ‡
                metrics.update(val_metrics)

                # è®°å½•éªŒè¯æŒ‡æ ‡åˆ°wandb
                wandb.log(val_metrics, step=step)

                print(f"âœ… éªŒè¯é›†è¯„ä¼°å®Œæˆ (Step {step})")

            # ä¿å­˜æ£€æŸ¥ç‚¹
            if step % save_every == 0:
                self.save_checkpoint(step)

        print("\n" + "=" * 60)
        print("âœ… è®­ç»ƒå®Œæˆ")
        print("=" * 60)

    def save_checkpoint(self, step: int):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint_dir = Path(self.config['output_dir']) / f"step_{step}"
        checkpoint_dir.mkdir(parents=True, exist_ok=True)

        # ä¿å­˜LoRAæƒé‡
        self.model.save_pretrained(checkpoint_dir)

        # ğŸ’¾ ä¿å­˜ExperienceBuffer
        self.experience_buffer.save(step=step)

        # ğŸ“Š æ‰“å°ExperienceBufferç»Ÿè®¡ä¿¡æ¯
        buffer_stats = self.experience_buffer.get_stats()
        print(f"\nğŸ“š ExperienceBufferç»Ÿè®¡:")
        for problem_type, stats in buffer_stats.items():
            if stats['count'] > 0:
                print(f"  {problem_type}: {stats['count']}æ ·æœ¬, "
                      f"å¹³å‡å¥–åŠ±={stats['avg_reward']:.2f}, "
                      f"æœ€é«˜å¥–åŠ±={stats['max_reward']:.2f}, "
                      f"å¹³å‡æ­£ç¡®æ€§={stats['avg_correctness']:.2f}")

        print(f"ğŸ’¾ æ£€æŸ¥ç‚¹å·²ä¿å­˜: {checkpoint_dir}")


async def main():
    """ä¸»å‡½æ•°"""
    trainer = GRPOTrainer(config_path="config/training.yaml")
    await trainer.train()


if __name__ == "__main__":
    asyncio.run(main())
