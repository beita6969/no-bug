#!/usr/bin/env python3
"""
å¥–åŠ±è®¡ç®—å™¨ - P0/P1/P2ä¿®å¤ç‰ˆ

ä¿®å¤å†…å®¹:
P0-1: 5æ¡£ç»†ç²’åº¦å¥–åŠ± (0/0.2/0.4/0.7/1.0)
P0-3: ä»£ç æ‰§è¡Œå¤šè¿›ç¨‹éš”ç¦» + éƒ¨åˆ†é€šè¿‡å¥–åŠ±
P0-4: ç­”æ¡ˆæå–é²æ£’æ€§æ”¹è¿›
P1-2: Judgeç¨³å¥æ€§å’Œè°ƒè¯•æ—¥å¿—
P2-1: LLM Judge max_tokensä»200å¢åŠ åˆ°800ï¼Œä¿®å¤reasoningæ¨¡å‹tokenä¸è¶³å¯¼è‡´contentä¸ºç©ºçš„é—®é¢˜
"""
import sys
import re
import threading
import time
import json
import random
import multiprocessing
from multiprocessing import Process, Queue
from typing import Any, Dict, Optional, List, Tuple
from pathlib import Path

# æ·»åŠ AFlowåˆ°è·¯å¾„
sys.path.insert(0, '/home/yijia/.claude/11/AFlow')

# å¯¼å…¥ç­”æ¡ˆæå–å™¨
try:
    from .answer_extractor import AnswerExtractor
    from .judge_prompt_loader import JudgePromptLoader
except ImportError:
    from answer_extractor import AnswerExtractor
    from judge_prompt_loader import JudgePromptLoader


class RewardComputer:
    """
    P0/P1ä¿®å¤ç‰ˆå¥–åŠ±è®¡ç®—å™¨

    ä¿®å¤ç‰¹æ€§:
    1. 5æ¡£ç»†ç²’åº¦å¥–åŠ± (0/0.2/0.4/0.7/1.0) - è§£å†³å¥–åŠ±ç¨€ç–é—®é¢˜
    2. ä»£ç æ‰§è¡Œå¤šè¿›ç¨‹éš”ç¦» - å®‰å…¨æ€§+ç¨³å®šæ€§
    3. éƒ¨åˆ†é€šè¿‡å¥–åŠ± - Codeä»»åŠ¡æŒ‰é€šè¿‡ç”¨ä¾‹æ¯”ä¾‹ç»™åˆ†
    4. ç­”æ¡ˆæå–é²æ£’æ€§ - æ”¯æŒåµŒå¥—boxed/åˆ†æ•°/ç™¾åˆ†æ¯”
    5. Judgeè°ƒè¯•æ—¥å¿— - é‡‡æ ·è®°å½•ç”¨äºè°ƒè¯•
    6. QAä»»åŠ¡F1è¯„åˆ† - æ›¿ä»£ç®€å•åŒ…å«åŒ¹é…
    """

    def __init__(
        self,
        reward_weights: Optional[Dict[str, float]] = None,
        use_answer_extractor: bool = True,  # æ˜¯å¦ä½¿ç”¨ç­”æ¡ˆæå–å™¨
        use_llm_judge: bool = False,  # æ˜¯å¦ä½¿ç”¨LLM Judge
        llm_config: Optional[Dict] = None,  # LLMé…ç½®
        debug_logging: bool = False  # æ˜¯å¦å¯ç”¨è¯¦ç»†è°ƒè¯•æ—¥å¿—
    ):
        """
        Args:
            reward_weights: å¥–åŠ±æƒé‡é…ç½®ï¼ˆä»…ç”¨äºå‘åå…¼å®¹ï¼Œå®é™…ä½¿ç”¨äºŒå…ƒå¥–åŠ±ï¼‰
            use_answer_extractor: æ˜¯å¦ä½¿ç”¨ç­”æ¡ˆæå–å™¨æ¥æ ‡å‡†åŒ–ç­”æ¡ˆ
            use_llm_judge: æ˜¯å¦ä½¿ç”¨LLM Judgeè¿›è¡Œè¯­ä¹‰æ¯”è¾ƒ
            llm_config: LLMé…ç½®ï¼ˆç”¨äºLLM Judgeï¼‰
            debug_logging: æ˜¯å¦å¯ç”¨è¯¦ç»†è°ƒè¯•æ—¥å¿—
        """
        # ä¿ç•™ç”¨äºå‘åå…¼å®¹ï¼Œä½†ä¸å†ä½¿ç”¨
        self.reward_weights = reward_weights or {
            "correctness": 1.0
        }

        # è°ƒè¯•æ—¥å¿—å¼€å…³
        self.debug_logging = debug_logging

        # åˆå§‹åŒ–ç­”æ¡ˆæå–å™¨
        self.use_answer_extractor = use_answer_extractor
        if use_answer_extractor:
            self.extractor = AnswerExtractor(use_llm_fallback=False)  # æš‚æ—¶ä¸ä½¿ç”¨LLMå…œåº•
        else:
            self.extractor = None

        # åˆå§‹åŒ–LLM Judge
        self.use_llm_judge = use_llm_judge
        self.llm_judge_client = None
        self.judge_prompt_loader = None  # æ•°æ®é›†ä¸“å±PromptåŠ è½½å™¨
        if use_llm_judge:
            self._init_llm_judge_client(llm_config)
            # åˆå§‹åŒ–PromptåŠ è½½å™¨
            try:
                self.judge_prompt_loader = JudgePromptLoader()
                stats = self.judge_prompt_loader.get_stats()
                print(f"  âœ… Judge PromptåŠ è½½å™¨åˆå§‹åŒ–æˆåŠŸ")
                print(f"     å·²åŠ è½½ {stats['total_datasets']} ä¸ªæ•°æ®é›†é…ç½®")
                print(f"     å¯ç”¨æ•°æ®é›†: {', '.join(stats['enabled_datasets'][:5])}...")
            except Exception as e:
                print(f"  âš ï¸  Judge PromptåŠ è½½å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
                print(f"     å°†ä½¿ç”¨é€šç”¨Prompt")
                self.judge_prompt_loader = None

        print(f"âœ… 10åˆ†åˆ¶å¥–åŠ±è®¡ç®—å™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"  æ¨¡å¼: 5æ¡£ç»†ç²’åº¦å¥–åŠ± [0, 0.2, 0.4, 0.7, 1.0] (P0ä¿®å¤)")
        print(f"  ç­”æ¡ˆæå–å™¨: {'å¯ç”¨' if use_answer_extractor else 'ç¦ç”¨'}")
        print(f"  LLM Judge: {'å¯ç”¨ (GPT OSS 120B @ port 8002)' if use_llm_judge else 'ç¦ç”¨'}")
        print(f"  è°ƒè¯•æ—¥å¿—: {'å¯ç”¨' if debug_logging else 'ç¦ç”¨'}")
        print(f"  ä»£ç æ‰§è¡Œ: å¤šè¿›ç¨‹éš”ç¦»æ¨¡å¼ (P0ä¿®å¤)")

        # P1-2: Judgeè°ƒè¯•æ—¥å¿—ç›®å½•
        self.judge_log_dir = Path("logs/judge_samples")
        self.judge_log_dir.mkdir(parents=True, exist_ok=True)
        self.judge_log_file = self.judge_log_dir / f"judge_samples_{time.strftime('%Y%m%d_%H%M%S')}.jsonl"

        # åˆå§‹åŒ–ç»Ÿè®¡è®¡æ•°å™¨ï¼ˆç”¨äºè¯Šæ–­ï¼‰
        self.eval_stats = {
            'total_evaluations': 0,
            'llm_judge_success': 0,
            'llm_judge_parse_failures': 0,
            'llm_judge_api_failures': 0,
            'correct_predictions': 0,
            'incorrect_predictions': 0
        }

    def _init_llm_judge_client(self, llm_config: Optional[Dict]):
        """åˆå§‹åŒ–LLM Judgeå®¢æˆ·ç«¯ï¼ˆä½¿ç”¨GPT OSS 120Bï¼‰"""
        try:
            from openai import OpenAI

            # ä½¿ç”¨port 8002çš„GPT OSS 120Bæ¨¡å‹
            default_config = {
                "base_url": "http://localhost:8002/v1",
                "api_key": "sk-dummy",  # vLLMä¸éœ€è¦çœŸå®key
                "model_name": "/home/yijia/lhy/openai/gpt-oss-120b"  # å®Œæ•´æ¨¡å‹è·¯å¾„
            }

            config = llm_config or default_config

            self.llm_judge_client = OpenAI(
                base_url=config.get("base_url", default_config["base_url"]),
                api_key=config.get("api_key", default_config["api_key"])
            )
            self.llm_judge_model = config.get("model_name", default_config["model_name"])

            print(f"  âœ… LLM Judgeå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
            print(f"     æ¨¡å‹: {self.llm_judge_model}")
            print(f"     URL: {config.get('base_url', default_config['base_url'])}")
        except Exception as e:
            print(f"  âš ï¸  LLM Judgeå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
            self.use_llm_judge = False
            self.llm_judge_client = None

    def _llm_judge_compare(
        self,
        problem: str,
        prediction: str,
        ground_truth: str,
        problem_type: str,
        source: Optional[str] = None  # æ–°å¢ï¼šæ•°æ®é›†æ¥æº
    ) -> bool:
        """
        ä½¿ç”¨LLM Judgeè¿›è¡Œè¯­ä¹‰æ¯”è¾ƒï¼ˆæ”¯æŒæ•°æ®é›†ä¸“å±Promptï¼‰

        Args:
            problem: é—®é¢˜æ–‡æœ¬
            prediction: æ¨¡å‹é¢„æµ‹ï¼ˆå®Œæ•´å“åº”ï¼Œæœªæå–ï¼‰
            ground_truth: Ground truthç­”æ¡ˆ
            problem_type: é—®é¢˜ç±»å‹
            source: æ•°æ®é›†æ¥æºï¼ˆå¦‚'gsm8k', 'math', 'hotpotqa'ï¼‰

        Returns:
            bool: Trueè¡¨ç¤ºç­‰ä»·ï¼ŒFalseè¡¨ç¤ºä¸ç­‰ä»·
        """
        self.eval_stats['total_evaluations'] += 1

        if not self.llm_judge_client:
            if self.debug_logging:
                print("âš ï¸  LLM Judgeå®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œé™çº§ä¸ºè§„åˆ™æ¯”è¾ƒ")
            self.eval_stats['llm_judge_api_failures'] += 1
            return False

        # ğŸ†• ä½¿ç”¨æ•°æ®é›†ä¸“å±Promptï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.judge_prompt_loader:
            query_prompt_template = self.judge_prompt_loader.get_judge_prompt(
                source=source,
                problem_type=problem_type
            )
            # æ ¼å¼åŒ–promptï¼ˆæ‰‹åŠ¨æ›¿æ¢ï¼Œé¿å…format()è§£æXMLæ ‡ç­¾ï¼‰
            query_prompt = query_prompt_template.replace('{{problem}}', problem)
            query_prompt = query_prompt.replace('{{prediction}}', prediction)
            query_prompt = query_prompt.replace('{{ground_truth}}', ground_truth)
            if self.debug_logging:
                print(f"  ğŸ“‹ ä½¿ç”¨æ•°æ®é›†ä¸“å±Prompt: source={source}")
        else:
            # Fallback: ä½¿ç”¨åŸæœ‰çš„é€šç”¨prompt
            query_prompt = self._get_legacy_prompt(problem, prediction, ground_truth)
            if self.debug_logging:
                print(f"  ğŸ“‹ ä½¿ç”¨é€šç”¨Prompt (Fallback)")


        try:
            # è°ƒç”¨LLM Judgeï¼ˆæœ€å¤šé‡è¯•1æ¬¡ï¼‰
            for attempt in range(2):  # 0=é¦–æ¬¡, 1=é‡è¯•
                response = self.llm_judge_client.chat.completions.create(
                    model=self.llm_judge_model,
                    messages=[
                        {"role": "system", "content": "You are a precise answer equivalence evaluator."},
                        {"role": "user", "content": query_prompt}
                    ],
                    temperature=0.0,
                    max_tokens=800  # P2ä¿®å¤: å¢åŠ åˆ°800ï¼Œreasoningæ¨¡å‹éœ€è¦æ›´å¤štokenå®Œæˆæ€è€ƒ
                )

                # æ£€æŸ¥å“åº”æ˜¯å¦ä¸ºç©º
                content = response.choices[0].message.content
                if content is None:
                    if attempt == 0:
                        if self.debug_logging:
                            print(f"âš ï¸  LLM Judgeé¦–æ¬¡è¿”å›ç©ºå†…å®¹ï¼Œé‡è¯•ä¸­...")
                        self.eval_stats['llm_judge_api_failures'] += 1
                        continue  # é‡è¯•
                    else:
                        if self.debug_logging:
                            print(f"âš ï¸  LLM Judgeé‡è¯•åä»è¿”å›ç©ºå†…å®¹ï¼Œfallbackåˆ¤å®šä¸ºFalse")
                        self.eval_stats['llm_judge_api_failures'] += 1
                        return False

                # æˆåŠŸè·å–å†…å®¹ï¼Œè·³å‡ºé‡è¯•å¾ªç¯
                result_text = content.strip()
                break

            # è§£æ<true_false>æ ‡ç­¾ - å¢å¼ºçš„é²æ£’æ€§åŒ¹é…
            import re
            # åŒ¹é…å¤šç§æ ¼å¼ï¼ˆæŒ‰ä¼˜å…ˆçº§å°è¯•ï¼‰ï¼š
            # 1. <true_false>True</true_false>
            # 2. <true_false>: True
            # 3. **true_false**: True
            # 4. true_false: True
            # 5. ç›´æ¥åœ¨æ–‡æœ¬ä¸­æŸ¥æ‰¾True/Falseï¼ˆæœ€åæ‰‹æ®µï¼‰

            # å°è¯•1: æ ‡å‡†XMLæ ‡ç­¾
            true_false_match = re.search(
                r'<true_false>\s*(True|False)\s*</true_false>',
                result_text,
                re.IGNORECASE
            )

            # å°è¯•2: å†’å·åˆ†éš”çš„æ ‡ç­¾
            if not true_false_match:
                true_false_match = re.search(
                    r'<true_false>\s*:\s*(True|False)',
                    result_text,
                    re.IGNORECASE
                )

            # å°è¯•3: Markdownç²—ä½“æ ¼å¼
            if not true_false_match:
                true_false_match = re.search(
                    r'\*\*true_false\*\*\s*:?\s*(True|False)',
                    result_text,
                    re.IGNORECASE
                )

            # å°è¯•4: ç®€å•çš„key: valueæ ¼å¼
            if not true_false_match:
                true_false_match = re.search(
                    r'true_false\s*:?\s*(True|False)',
                    result_text,
                    re.IGNORECASE
                )

            # å°è¯•5: æŸ¥æ‰¾ç‹¬ç«‹çš„True/Falseï¼ˆæœ€åæ‰‹æ®µï¼‰
            if not true_false_match:
                # åªåœ¨å“åº”æœ«å°¾æŸ¥æ‰¾ï¼Œé¿å…è¯¯åŒ¹é…åˆ†ææ–‡æœ¬ä¸­çš„True/False
                last_200_chars = result_text[-200:]
                true_false_match = re.search(
                    r'\b(True|False)\b',
                    last_200_chars,
                    re.IGNORECASE
                )

            if true_false_match:
                verdict = true_false_match.group(1).lower() == "true"
                self.eval_stats['llm_judge_success'] += 1

                # æ›´æ–°æ­£ç¡®/é”™è¯¯è®¡æ•°
                if verdict:
                    self.eval_stats['correct_predictions'] += 1
                else:
                    self.eval_stats['incorrect_predictions'] += 1

                # è°ƒè¯•è¾“å‡ºï¼ˆæ ¹æ®debug_loggingå¼€å…³ï¼‰
                if self.debug_logging:
                    import random
                    if random.random() < 0.2:  # 20%é‡‡æ ·
                        print(f"\nğŸ¤– LLM Judgeç»“æœ ({problem_type}):")
                        print(f"  é—®é¢˜: {problem[:60]}...")
                        print(f"  é¢„æµ‹: {str(prediction)[:60]}...")
                        print(f"  çœŸå€¼: {str(ground_truth)[:60]}...")
                        print(f"  åˆ¤å†³: {verdict}")
                        print(f"  LLMå“åº”: {result_text[:150]}...")

                return verdict
            else:
                # å®Œå…¨æ— æ³•è§£ææ—¶ï¼Œæ‰“å°å®Œæ•´å“åº”ç”¨äºè°ƒè¯•
                self.eval_stats['llm_judge_parse_failures'] += 1
                if self.debug_logging:
                    print(f"âš ï¸  æ— æ³•è§£æLLM Judgeå“åº”ï¼ˆå°è¯•äº†5ç§æ ¼å¼ï¼‰")
                    print(f"  å®Œæ•´å“åº”: {result_text}")
                    print(f"  é—®é¢˜: {problem[:100]}")
                    print(f"  é¢„æµ‹: {str(prediction)[:100]}")
                    print(f"  çœŸå€¼: {str(ground_truth)[:100]}")
                return False

        except Exception as e:
            self.eval_stats['llm_judge_api_failures'] += 1
            if self.debug_logging:
                print(f"âš ï¸  LLM Judgeè°ƒç”¨å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
            return False

    def _get_legacy_prompt(self, problem: str, prediction: str, ground_truth: str) -> str:
        """è·å–åŸæœ‰çš„é€šç”¨Promptï¼ˆå‘åå…¼å®¹ï¼‰"""
        return f"""You are a precise mathematical and logical equivalence evaluator. Your task is to determine if the Model Response contains an answer equivalent to the Ground Truth.

**Step 1: Extract the Final Answer**
From the Model Response, extract ONLY the final answer, ignoring all reasoning steps, explanations, and intermediate calculations.

Look for answers in these formats (in order of priority):
1. Inside `\\boxed{{...}}` LaTeX notation
2. After phrases like "The answer is", "Therefore", "So", "Thus", "Final answer:"
3. In `<answer>...</answer>` tags
4. The last number, expression, or entity mentioned

**Step 2: Extract from Ground Truth**
Similarly extract the final answer from Ground Truth, which may contain:
- Step-by-step solutions (extract only the final result)
- Multiple numbers (take the last/final one)
- Explanatory text (ignore and find the answer)

**Step 3: Normalize Both Answers**
Before comparing, normalize both answers:
- **Numbers:** Convert to same format (0.5 == 1/2 == 50%)
- **Units/Currency:** Ignore ($30 == 30, 10 meters == 10)
- **Formatting:** Ignore spaces, case, punctuation
- **LaTeX:** Interpret mathematical meaning (\\frac{{1}}{{2}} == 0.5)

**Step 4: Compare Equivalence**
Answers are equivalent if:
- **Math:** Numerically/algebraically equal (even if different forms)
- **Text:** Same entity/concept (ignore synonyms, case)
- **Precision:** Allow reasonable rounding (42.0 == 42)

**Examples of CORRECT equivalence:**
- "1/2" == "0.5" âœ“
- "$30" == "30" âœ“
- "\\boxed{{42}}" == "42" âœ“
- "x^2+2x+1" == "(x+1)^2" âœ“ (algebraically equivalent)
- "10 meters" == "10" âœ“

**Examples of INCORRECT equivalence:**
- "John Smith" == "Jane Doe" âœ— (different entities)
- "42" == "43" âœ— (different numbers)
- "Paris" == "London" âœ— (different locations)

**Inputs:**
Question: {problem}
Model Response: {prediction}
Ground Truth: {ground_truth}

**Required Output Format:**
<analysis>Your reasoning in 1-2 sentences</analysis>
<true_false>True or False</true_false>

Be LENIENT with formatting differences but STRICT with factual/numerical differences.
"""

    def compute_reward(
        self,
        problem: str,
        prediction: Any,
        ground_truth: Any,
        problem_type: str = "math",
        metadata: Optional[Dict] = None,
        test: Optional[str] = None,
        entry_point: Optional[str] = None,
        source: Optional[str] = None  # ğŸ†• æ–°å¢ï¼šæ•°æ®é›†æ¥æº
    ) -> float:
        """
        è®¡ç®—å¥–åŠ± - P0ä¿®å¤: 5æ¡£ç»†ç²’åº¦å¥–åŠ±

        å¥–åŠ±ç­‰çº§:
        - 1.0: å®Œç¾åŒ¹é…
        - 0.7: æ¥è¿‘æ­£ç¡® (æ•°å€¼è¯¯å·®<5%, éƒ¨åˆ†æµ‹è¯•é€šè¿‡>80%)
        - 0.4: éƒ¨åˆ†æ­£ç¡® (æ ¼å¼æ­£ç¡®ä½†ç­”æ¡ˆæœ‰åå·®, æµ‹è¯•é€šè¿‡>50%)
        - 0.2: æ ¼å¼æ­£ç¡® (æœ‰æ•ˆè¾“å‡ºä½†ç­”æ¡ˆé”™è¯¯, æµ‹è¯•é€šè¿‡>20%)
        - 0.0: å®Œå…¨é”™è¯¯

        Args:
            source: æ•°æ®é›†æ¥æºï¼ˆå¦‚'gsm8k', 'math', 'hotpotqa'ï¼‰- ç”¨äºé€‰æ‹©ä¸“å±Judge Prompt

        Returns:
            reward: 0.0 / 0.2 / 0.4 / 0.7 / 1.0
        """
        metadata = metadata or {}

        # è°ƒè¯•æ—¥å¿—ï¼šè¾“å…¥ä¿¡æ¯
        if self.debug_logging:
            print(f"\nğŸ“Š è¯„ä¼°è¾“å…¥ ({problem_type}, source={source}):")
            print(f"  é—®é¢˜: {str(problem)[:100]}...")
            print(f"  é¢„æµ‹: {str(prediction)[:100]}...")
            print(f"  çœŸå€¼: {str(ground_truth)[:100]}...")

        # P0ä¿®å¤: æ ¹æ®ä»»åŠ¡ç±»å‹ä½¿ç”¨ä¸åŒçš„ç»†ç²’åº¦å¥–åŠ±è®¡ç®—
        if problem_type == "code":
            # ä»£ç ä»»åŠ¡: ä½¿ç”¨å¤šè¿›ç¨‹éš”ç¦»æ‰§è¡Œ + éƒ¨åˆ†é€šè¿‡å¥–åŠ±
            # P6ä¿®å¤: ä¼ å…¥problemç”¨äºå¤„ç†HumanEvalæ ¼å¼(problem=ç­¾å, prediction=å‡½æ•°ä½“)
            reward = self._compute_code_reward(problem, prediction, ground_truth, test, entry_point)
        elif problem_type == "math":
            # æ•°å­¦ä»»åŠ¡: ç»†ç²’åº¦æ•°å€¼æ¯”è¾ƒ
            reward = self._compute_math_reward(problem, prediction, ground_truth, source)
        elif problem_type == "qa":
            # QAä»»åŠ¡: F1è¯„åˆ†
            reward = self._compute_qa_reward(problem, prediction, ground_truth, source)
        else:
            # é€šç”¨ä»»åŠ¡
            reward = self._compute_general_reward(prediction, ground_truth)

        # æ›´æ–°ç»Ÿè®¡
        if reward >= 0.9:
            self.eval_stats['correct_predictions'] += 1
        else:
            self.eval_stats['incorrect_predictions'] += 1

        if metadata is not None:
            metadata['correctness_score'] = reward
            metadata['used_llm_judge'] = self.use_llm_judge
            metadata['is_correct'] = reward >= 0.9
            metadata['reward_level'] = self._get_reward_level(reward)

        # è°ƒè¯•æ—¥å¿—ï¼šè¾“å‡ºç»“æœ
        if self.debug_logging:
            level = self._get_reward_level(reward)
            print(f"  åˆ¤å†³: {level}")
            print(f"  å¥–åŠ±: {reward:.2f}")

        return reward

    def _get_reward_level(self, reward: float) -> str:
        """è·å–å¥–åŠ±ç­‰çº§æè¿°"""
        if reward >= 0.9:
            return "âœ… å®Œç¾ (1.0)"
        elif reward >= 0.6:
            return "ğŸŸ¡ æ¥è¿‘ (0.7)"
        elif reward >= 0.35:
            return "ğŸŸ  éƒ¨åˆ† (0.4)"
        elif reward >= 0.15:
            return "ğŸ”´ æ ¼å¼ (0.2)"
        else:
            return "âŒ é”™è¯¯ (0.0)"

    def _is_correct(
        self,
        prediction: Any,
        ground_truth: Any,
        problem_type: str
    ) -> bool:
        """
        åˆ¤æ–­é¢„æµ‹æ˜¯å¦æ­£ç¡® (ä¼ ç»Ÿè§„åˆ™)
        
        Returns:
            bool: True if correct, False otherwise
        """
        if prediction is None:
            return False

        if problem_type == "math":
            return self._is_math_correct(prediction, ground_truth)
        elif problem_type == "code":
            # Fallback for code if no test cases provided (should generally not happen if trained correctly)
            return False 
        elif problem_type == "qa":
            return self._is_qa_correct(prediction, ground_truth)
        else:
            return self._is_general_correct(prediction, ground_truth)

    # ============== P0ä¿®å¤: ç»†ç²’åº¦å¥–åŠ±è®¡ç®—æ–¹æ³• ==============

    def _compute_math_reward(self, problem: str, prediction: Any, ground_truth: Any, source: Optional[str]) -> float:
        """
        P0ä¿®å¤: Mathä»»åŠ¡5æ¡£ç»†ç²’åº¦å¥–åŠ±

        å¥–åŠ±ç­‰çº§:
        - 1.0: å®Œç¾åŒ¹é…
        - 0.7: æ•°å€¼æ¥è¿‘ (ç›¸å¯¹è¯¯å·®<5%)
        - 0.4: æ•°é‡çº§æ­£ç¡® (ç›¸å¯¹è¯¯å·®<50%)
        - 0.2: æ ¼å¼æ­£ç¡® (æœ‰boxedæˆ–æ•°å­—è¾“å‡º)
        - 0.0: å®Œå…¨é”™è¯¯
        """
        if prediction is None:
            return 0.0

        pred_str = str(prediction).strip()
        gt_str = str(ground_truth).strip()

        # P0-FIX: æ£€æµ‹é¢„æµ‹æ˜¯å¦ä¸ºä»£ç æ ¼å¼ï¼ˆè€Œéæ•°å­¦ç­”æ¡ˆï¼‰
        # å¦‚æœé¢„æµ‹åŒ…å«Pythonä»£ç å…³é”®å­—ï¼Œåˆ¤å®šä¸ºæ ¼å¼é”™è¯¯(0.2)è€Œéè°ƒç”¨LLM Judge
        code_keywords = ['import ', 'def ', 'class ', 'return ', 'print(', 'for ', 'while ', 'if __name__']
        pred_lower = pred_str.lower()
        if any(kw in pred_lower for kw in code_keywords):
            if self.debug_logging:
                print(f"  âš ï¸  P0-FIX: æ£€æµ‹åˆ°ä»£ç æ ¼å¼ç­”æ¡ˆï¼Œåˆ¤å®šä¸ºæ ¼å¼é”™è¯¯(0.2)")
            return 0.2  # æ ¼å¼é”™è¯¯ï¼Œä¸æ˜¯æœ‰æ•ˆçš„æ•°å­¦ç­”æ¡ˆ

        # 1. é¦–å…ˆå°è¯•LLM Judge (å¦‚æœå¯ç”¨)
        if self.use_llm_judge:
            is_correct = self._llm_judge_compare(
                problem=problem,
                prediction=pred_str,
                ground_truth=gt_str,
                problem_type="math",
                source=source
            )
            if is_correct:
                return 1.0

        # 2. è§„åˆ™åŒ¹é…ç»†ç²’åº¦è¯„ä¼°
        # æå–ç­”æ¡ˆ
        pred_answer = self._extract_math_answer(pred_str)
        gt_answer = self._extract_math_answer(gt_str)

        if pred_answer is None:
            # æ²¡æœ‰æœ‰æ•ˆè¾“å‡º
            return 0.0

        if gt_answer is None:
            # æ— æ³•è§£æground truthï¼Œfallbackåˆ°å­—ç¬¦ä¸²åŒ¹é…
            if gt_str.lower() in pred_str.lower():
                return 1.0
            return 0.0

        # 3. æ•°å€¼æ¯”è¾ƒ
        try:
            pred_num = self._parse_number_robust(pred_answer)
            gt_num = self._parse_number_robust(gt_answer)

            if pred_num is not None and gt_num is not None:
                # P7ä¿®å¤: ä¸AFlowä¿æŒä¸€è‡´ï¼Œä½¿ç”¨abs_tol=1e-3
                # AFlow: math.pyä½¿ç”¨abs_tol=1e-3, gsm8k.pyä½¿ç”¨abs_tol=1e-6
                import math

                # GSM8Kä½¿ç”¨æ›´ä¸¥æ ¼çš„å®¹å·®
                if source == 'gsm8k':
                    tolerance = 1e-6
                else:
                    tolerance = 1e-3  # MATHå’Œå…¶ä»–æ•°å­¦æ•°æ®é›†

                if math.isclose(pred_num, gt_num, abs_tol=tolerance):
                    return 1.0

                # ç»å¯¹è¯¯å·®æ£€æŸ¥
                abs_error = abs(pred_num - gt_num)
                if abs_error <= tolerance:
                    return 1.0

                # ç›¸å¯¹è¯¯å·®ï¼ˆä»…å½“gtä¸æ¥è¿‘0æ—¶æœ‰æ„ä¹‰ï¼‰
                if abs(gt_num) > 1e-6:
                    rel_error = abs_error / abs(gt_num)
                    if rel_error < 0.01:  # <1%è¯¯å·®
                        return 1.0
                    elif rel_error < 0.05:  # <5%è¯¯å·®
                        return 0.7
                    elif rel_error < 0.50:  # <50%è¯¯å·®
                        return 0.4
                    else:
                        return 0.2
                else:
                    # gtæ¥è¿‘0æ—¶ç”¨ç»å¯¹è¯¯å·®
                    if abs_error < 0.01:
                        return 0.7
                    elif abs_error < 0.1:
                        return 0.4
                    else:
                        return 0.2
        except:
            pass

        # 4. å­—ç¬¦ä¸²åŒ¹é…fallback
        if pred_answer.lower() == gt_answer.lower():
            return 1.0

        # æœ‰è¾“å‡ºä½†ä¸åŒ¹é…
        return 0.2

    def _compute_code_reward(self, problem: Optional[str], prediction: Any, ground_truth: Any,
                             test: Optional[str], entry_point: Optional[str]) -> float:
        """
        P0ä¿®å¤: Codeä»»åŠ¡å¤šè¿›ç¨‹éš”ç¦»æ‰§è¡Œ + éƒ¨åˆ†é€šè¿‡å¥–åŠ±
        P6ä¿®å¤: æ”¯æŒHumanEvalæ ¼å¼(problem=å‡½æ•°ç­¾å, prediction=å‡½æ•°ä½“)

        å¥–åŠ±ç­‰çº§:
        - 1.0: æ‰€æœ‰æµ‹è¯•é€šè¿‡
        - 0.7: >80%æµ‹è¯•é€šè¿‡
        - 0.4: >50%æµ‹è¯•é€šè¿‡
        - 0.2: >20%æµ‹è¯•é€šè¿‡æˆ–ä»£ç è¯­æ³•æ­£ç¡®
        - 0.0: å®Œå…¨å¤±è´¥
        """
        # P3: æ·»åŠ è¯¦ç»†debug loggingè¯Šæ–­Codeé—®é¢˜
        if self.debug_logging:
            print(f"  ğŸ”¬ [CODE DEBUG] prediction type: {type(prediction).__name__}")
            pred_str = str(prediction)
            print(f"  ğŸ”¬ [CODE DEBUG] prediction[:300]: {pred_str[:300]}")
            print(f"  ğŸ”¬ [CODE DEBUG] entry_point: {entry_point}")
            print(f"  ğŸ”¬ [CODE DEBUG] test exists: {bool(test)}")

        if prediction is None:
            return 0.0

        solution = str(prediction).strip()
        if not solution:
            return 0.0

        # P3: æ£€æµ‹æ˜¯å¦æ˜¯dictæ ¼å¼çš„å­—ç¬¦ä¸² (å¦‚ "{'code': '...'}")
        if solution.startswith("{") and "'code'" in solution:
            try:
                import ast
                parsed = ast.literal_eval(solution)
                if isinstance(parsed, dict) and 'code' in parsed:
                    solution = parsed['code']
                    if self.debug_logging:
                        print(f"  ğŸ”¬ [CODE DEBUG] Extracted code from dict string")
            except:
                pass

        # Sanitize solution (remove markdown blocks if any)
        if "```python" in solution:
            try:
                solution = solution.split("```python")[1].split("```")[0]
                if self.debug_logging:
                    print(f"  ğŸ”¬ [CODE DEBUG] Removed ```python blocks")
            except:
                pass
        elif "```" in solution:
            try:
                solution = solution.split("```")[1].split("```")[0]
                if self.debug_logging:
                    print(f"  ğŸ”¬ [CODE DEBUG] Removed ``` blocks")
            except:
                pass

        # P7ä¿®å¤: æ·»åŠ ä»£ç sanitizeåŠŸèƒ½ï¼ˆå‚è€ƒAFlow sanitize.pyï¼‰
        solution = self._sanitize_code(solution, entry_point)

        # P6ä¿®å¤: HumanEvalæ ¼å¼å¤„ç† - problemåŒ…å«å‡½æ•°ç­¾åï¼ŒpredictionåªåŒ…å«å‡½æ•°ä½“
        # æ£€æµ‹å¹¶åˆå¹¶å‡½æ•°ç­¾åä¸å‡½æ•°ä½“
        if entry_point and problem:
            # æ£€æŸ¥solutionä¸­æ˜¯å¦ç¼ºå°‘å‡½æ•°å®šä¹‰
            has_def_in_solution = f"def {entry_point}" in solution
            has_def_in_problem = f"def {entry_point}" in str(problem)

            if not has_def_in_solution and has_def_in_problem:
                # solutionåªæ˜¯å‡½æ•°ä½“ï¼Œéœ€è¦ä»problemæå–ç­¾åå¹¶åˆå¹¶
                problem_str = str(problem)
                # æ‰¾åˆ°å‡½æ•°ç­¾åç»“æŸä½ç½®ï¼ˆç¬¬ä¸€ä¸ªå†’å·åï¼‰
                import re
                signature_match = re.search(rf'(def\s+{re.escape(entry_point)}\s*\([^)]*\)\s*(?:->\s*[^:]+)?\s*:)', problem_str)
                if signature_match:
                    func_signature = signature_match.group(1)
                    # ç¡®ä¿å‡½æ•°ä½“æœ‰æ­£ç¡®çš„ç¼©è¿›
                    body_lines = solution.split('\n')
                    indented_body = []
                    for line in body_lines:
                        if line.strip():  # éç©ºè¡Œ
                            # å¦‚æœè¡Œæ²¡æœ‰è¶³å¤Ÿçš„ç¼©è¿›ï¼Œæ·»åŠ 4ä¸ªç©ºæ ¼
                            if not line.startswith('    ') and not line.startswith('\t'):
                                indented_body.append('    ' + line)
                            else:
                                indented_body.append(line)
                        else:
                            indented_body.append(line)
                    solution = func_signature + '\n' + '\n'.join(indented_body)
                    if self.debug_logging:
                        print(f"  ğŸ”¬ [CODE DEBUG] P6: Merged function signature from problem")
                        print(f"  ğŸ”¬ [CODE DEBUG] P6: merged solution[:200]: {solution[:200]}")

        if self.debug_logging:
            print(f"  ğŸ”¬ [CODE DEBUG] cleaned solution[:300]: {solution[:300]}")
            # æ£€æŸ¥entry_pointæ˜¯å¦åœ¨solutionä¸­å®šä¹‰
            if entry_point:
                if f"def {entry_point}" in solution:
                    print(f"  ğŸ”¬ [CODE DEBUG] âœ… entry_point '{entry_point}' found in solution")
                else:
                    print(f"  ğŸ”¬ [CODE DEBUG] âŒ entry_point '{entry_point}' NOT found in solution")

        # P0æ ¹æœ¬æ€§ä¿®å¤: ä» test_cases ä¸­æå– entry_point (å¦‚ MBPP æ•°æ®é›†æ²¡æœ‰ entry_point ä½†æœ‰ test_cases)
        if not entry_point and test:
            import re
            # ä» assert func_name(...) æ ¼å¼ä¸­æå–å‡½æ•°å
            match = re.search(r'assert\s+([a-zA-Z_][a-zA-Z0-9_]*)\s*\(', test)
            if match:
                entry_point = match.group(1)
                if self.debug_logging:
                    print(f"  ğŸ”¬ [CODE DEBUG] Extracted entry_point from test_cases: {entry_point}")

        # å¦‚æœæ²¡æœ‰test casesï¼Œä½¿ç”¨LLM Judgeæˆ–fallbackåˆ°è¯­æ³•æ£€æŸ¥
        if not test or not entry_point:
            # P5ä¿®å¤: å¯¹äºæ²¡æœ‰æµ‹è¯•ç”¨ä¾‹çš„ä»£ç ï¼Œä½¿ç”¨LLM Judgeè¿›è¡Œè¯­ä¹‰æ¯”è¾ƒ
            if self.use_llm_judge and ground_truth:
                # ä½¿ç”¨LLM Judgeæ¯”è¾ƒä»£ç çš„è¯­ä¹‰ç­‰ä»·æ€§
                # P8ä¿®å¤: æ·»åŠ ç¼ºå¤±çš„problemå‚æ•°
                is_equivalent = self._llm_judge_compare(
                    problem=str(problem) if problem else "",  # P8: ä¿®å¤ç¼ºå¤±å‚æ•°
                    prediction=solution,
                    ground_truth=str(ground_truth),
                    problem_type="code",
                    source="code_llm_judge"
                )
                if is_equivalent is True:
                    # æ£€æŸ¥è¯­æ³•æ˜¯å¦æ­£ç¡®
                    try:
                        compile(solution, '<string>', 'exec')
                        return 1.0  # LLMåˆ¤å®šç­‰ä»·ä¸”è¯­æ³•æ­£ç¡®
                    except:
                        return 0.4  # LLMåˆ¤å®šç­‰ä»·ä½†è¯­æ³•æœ‰é—®é¢˜
                elif is_equivalent is False:
                    # LLMåˆ¤å®šä¸ç­‰ä»·ï¼Œæ£€æŸ¥è¯­æ³•
                    try:
                        compile(solution, '<string>', 'exec')
                        return 0.2  # è¯­æ³•æ­£ç¡®ä½†LLMåˆ¤å®šä¸ç­‰ä»·
                    except:
                        return 0.0
                # is_equivalent is None (APIå¤±è´¥)ï¼Œfallbackåˆ°è¯­æ³•æ£€æŸ¥

            # Fallback: æ£€æŸ¥æ˜¯å¦ä¸ºæœ‰æ•ˆPythonä»£ç 
            try:
                compile(solution, '<string>', 'exec')
                return 0.2  # è¯­æ³•æ­£ç¡®ä½†æ— æ³•éªŒè¯
            except:
                return 0.0

        # P0ä¿®å¤: ä½¿ç”¨å¤šè¿›ç¨‹éš”ç¦»æ‰§è¡Œ
        pass_rate = self._execute_code_isolated(solution, test, entry_point)

        # æ ¹æ®é€šè¿‡ç‡ç»™åˆ†
        if pass_rate >= 1.0:
            return 1.0
        elif pass_rate >= 0.8:
            return 0.7
        elif pass_rate >= 0.5:
            return 0.4
        elif pass_rate >= 0.2:
            return 0.2
        else:
            # æ£€æŸ¥è¯­æ³•æ˜¯å¦æ­£ç¡®
            try:
                compile(solution, '<string>', 'exec')
                return 0.2  # P1ä¿®å¤: è¯­æ³•æ­£ç¡®ä½†æµ‹è¯•å…¨éƒ¨å¤±è´¥ï¼Œç»™0.2ï¼ˆåŸ0.1ä¸åœ¨5æ¡£å†…ï¼‰
            except:
                return 0.0

    def _execute_code_isolated(self, solution: str, test: str, entry_point: str, timeout: int = 15) -> float:
        """
        P0ä¿®å¤: å¤šè¿›ç¨‹éš”ç¦»æ‰§è¡Œä»£ç 
        P7ä¿®å¤: è¶…æ—¶æ”¹ä¸º15ç§’ä¸AFlowä¸€è‡´ (åŸ10ç§’)

        Returns:
            pass_rate: é€šè¿‡ç‡ [0.0, 1.0]
        """
        def run_tests_in_process(solution: str, test: str, entry_point: str, result_queue: Queue):
            """åœ¨å­è¿›ç¨‹ä¸­æ‰§è¡Œæµ‹è¯•"""
            try:
                global_dict = {
                    "math": __import__("math"),
                    "hashlib": __import__("hashlib"),
                    "re": __import__("re"),
                    "sys": __import__("sys"),
                    "List": List,
                    "Dict": Dict,
                    "Tuple": Tuple,
                    "Optional": Optional,
                    "Any": Any,
                }

                # P7ä¿®å¤: HumanEvalç‰¹æ®Šå‡½æ•°å¤„ç†ï¼ˆå‚è€ƒAFlow humaneval.pyï¼‰
                # æŸäº›æµ‹è¯•å‡½æ•°éœ€è¦å…ˆå®šä¹‰ä¾èµ–å‡½æ•°
                HUMANEVAL_HELPERS = {
                    'decode_cyclic': '''
def encode_cyclic(s: str):
    groups = [s[(3 * i):min((3 * i + 3), len(s))] for i in range((len(s) + 2) // 3)]
    groups = [(group[1:] + group[0]) if len(group) == 3 else group for group in groups]
    return "".join(groups)
''',
                    'decode_shift': '''
def encode_shift(s: str):
    return "".join([chr(((ord(ch) + 5 - ord("a")) % 26) + ord("a")) for ch in s])
''',
                    'find_zero': '''
def poly(xs: list, x: float):
    return sum([coeff * x ** i for i, coeff in enumerate(xs)])
'''
                }

                # å¦‚æœentry_pointéœ€è¦è¾…åŠ©å‡½æ•°ï¼Œå…ˆæ³¨å…¥
                if entry_point in HUMANEVAL_HELPERS:
                    helper_code = HUMANEVAL_HELPERS[entry_point]
                    exec(helper_code, global_dict)

                # æ‰§è¡Œsolution
                exec(solution, global_dict)

                if entry_point not in global_dict:
                    result_queue.put({'pass_rate': 0.0, 'error': 'entry_point not found'})
                    return

                # æ‰§è¡Œtestå¹¶æ•è·æ–­è¨€
                # æ–¹æ³•1: ç›´æ¥æ‰§è¡Œtestä»£ç ï¼ˆå¯èƒ½åŒ…å«å¤šä¸ªassertï¼‰
                try:
                    exec(test, global_dict)

                    # å¦‚æœæœ‰checkå‡½æ•°ï¼Œè°ƒç”¨å®ƒ
                    if "check" in global_dict:
                        check_func = global_dict["check"]
                        check_func(global_dict[entry_point])

                    # æ‰€æœ‰æµ‹è¯•é€šè¿‡
                    result_queue.put({'pass_rate': 1.0, 'error': None})

                except AssertionError as e:
                    # éƒ¨åˆ†æ–­è¨€å¤±è´¥ - å°è¯•ç»Ÿè®¡é€šè¿‡ç‡
                    # ç®€åŒ–å¤„ç†ï¼šæœ‰æ–­è¨€å¤±è´¥å°±ç®—éƒ¨åˆ†é€šè¿‡
                    result_queue.put({'pass_rate': 0.3, 'error': f'AssertionError: {e}'})

                except Exception as e:
                    result_queue.put({'pass_rate': 0.0, 'error': f'{type(e).__name__}: {e}'})

            except SyntaxError as e:
                result_queue.put({'pass_rate': 0.0, 'error': f'SyntaxError: {e}'})
            except Exception as e:
                result_queue.put({'pass_rate': 0.0, 'error': f'{type(e).__name__}: {e}'})

        # åˆ›å»ºç»“æœé˜Ÿåˆ—
        result_queue = multiprocessing.Queue()

        # åˆ›å»ºå­è¿›ç¨‹
        process = multiprocessing.Process(
            target=run_tests_in_process,
            args=(solution, test, entry_point, result_queue)
        )

        try:
            process.start()
            process.join(timeout=timeout)

            # æ£€æŸ¥æ˜¯å¦è¶…æ—¶
            if process.is_alive():
                process.terminate()
                process.join(timeout=2)
                if process.is_alive():
                    process.kill()
                if self.debug_logging:
                    print(f"  â±ï¸ ä»£ç æ‰§è¡Œè¶…æ—¶ ({timeout}s)")
                return 0.2  # P1ä¿®å¤: è¶…æ—¶ç»™0.2ï¼ˆåŸ0.1ä¸åœ¨5æ¡£å†…ï¼‰ï¼Œå› ä¸ºä»£ç å¯èƒ½éƒ¨åˆ†æ­£ç¡®

            # è·å–ç»“æœ
            if not result_queue.empty():
                result = result_queue.get_nowait()
                if self.debug_logging and result.get('error'):
                    print(f"  ğŸ”§ ä»£ç æ‰§è¡Œ: {result.get('error', 'unknown')[:50]}")
                return result.get('pass_rate', 0.0)
            else:
                return 0.0

        except Exception as e:
            if self.debug_logging:
                print(f"  âš ï¸ å¤šè¿›ç¨‹æ‰§è¡Œå¼‚å¸¸: {e}")
            return 0.0
        finally:
            # ç¡®ä¿è¿›ç¨‹è¢«æ¸…ç†
            if process.is_alive():
                process.terminate()

    def _sanitize_code(self, code: str, entry_point: Optional[str] = None) -> str:
        """
        P7ä¿®å¤: ä»£ç æ¸…ç†å‡½æ•°ï¼ˆå‚è€ƒAFlow scripts/utils/sanitize.pyï¼‰

        åŠŸèƒ½:
        1. æå–æœ‰æ•ˆä»£ç æ®µ
        2. ASTè§£æè·å–æ‰€æœ‰å®šä¹‰
        3. å¦‚æœæŒ‡å®šentry_pointï¼Œåªä¿ç•™ç›¸å…³ä¾èµ–

        Args:
            code: åŸå§‹ä»£ç å­—ç¬¦ä¸²
            entry_point: å…¥å£å‡½æ•°åï¼ˆå¯é€‰ï¼‰

        Returns:
            æ¸…ç†åçš„ä»£ç 
        """
        import ast

        if not code or not code.strip():
            return code

        try:
            # å°è¯•è§£æä»£ç 
            tree = ast.parse(code)
        except SyntaxError:
            # è§£æå¤±è´¥ï¼Œè¿”å›åŸå§‹ä»£ç 
            return code

        # æ”¶é›†æ‰€æœ‰å®šä¹‰
        imports = []
        definitions = []  # (name, code, dependencies)

        for node in ast.iter_child_nodes(tree):
            if isinstance(node, (ast.Import, ast.ImportFrom)):
                imports.append(ast.unparse(node))
            elif isinstance(node, ast.FunctionDef):
                # è·å–å‡½æ•°ä¾èµ–
                deps = self._get_dependencies(node)
                definitions.append((node.name, ast.unparse(node), deps))
            elif isinstance(node, ast.ClassDef):
                deps = self._get_dependencies(node)
                definitions.append((node.name, ast.unparse(node), deps))
            elif isinstance(node, ast.Assign):
                for target in node.targets:
                    if isinstance(target, ast.Name):
                        deps = self._get_dependencies(node)
                        definitions.append((target.id, ast.unparse(node), deps))

        # å¦‚æœæ²¡æœ‰æŒ‡å®šentry_pointæˆ–æ‰¾ä¸åˆ°entry_pointï¼Œè¿”å›æ‰€æœ‰ä»£ç 
        if not entry_point:
            return code

        # æ£€æŸ¥entry_pointæ˜¯å¦åœ¨definitionsä¸­
        entry_exists = any(name == entry_point for name, _, _ in definitions)
        if not entry_exists:
            return code

        # æ„å»ºä¾èµ–å›¾ï¼Œæ‰¾åˆ°entry_pointéœ€è¦çš„æ‰€æœ‰å®šä¹‰
        needed = self._find_reachable(entry_point, definitions)

        # ç»„è£…æœ€ç»ˆä»£ç 
        result_parts = imports[:]
        for name, code_str, _ in definitions:
            if name in needed:
                result_parts.append(code_str)

        return '\n'.join(result_parts)

    def _get_dependencies(self, node: 'ast.AST') -> set:
        """è·å–ASTèŠ‚ç‚¹ä¸­å¼•ç”¨çš„åç§°"""
        import ast
        deps = set()
        for child in ast.walk(node):
            if isinstance(child, ast.Name):
                deps.add(child.id)
        return deps

    def _find_reachable(self, entry_point: str, definitions: list) -> set:
        """ä»entry_pointå¼€å§‹ï¼Œæ‰¾åˆ°æ‰€æœ‰å¯è¾¾çš„å®šä¹‰"""
        # æ„å»ºåç§°åˆ°ä¾èµ–çš„æ˜ å°„
        dep_map = {name: deps for name, _, deps in definitions}

        # BFSæ‰¾å¯è¾¾èŠ‚ç‚¹
        visited = set()
        queue = [entry_point]

        while queue:
            current = queue.pop(0)
            if current in visited:
                continue
            visited.add(current)

            if current in dep_map:
                for dep in dep_map[current]:
                    if dep not in visited and dep in dep_map:
                        queue.append(dep)

        return visited

    def _compute_qa_reward(self, problem: str, prediction: Any, ground_truth: Any, source: Optional[str]) -> float:
        """
        P1ä¿®å¤: QAä»»åŠ¡è¯„ä¼° - å‚è€ƒSQuAD/TriviaQAå›½é™…æ ‡å‡†è¯„ä¼°æ–¹æ³•

        å›½é™…æ ‡å‡†æ–¹æ³• (SQuADå®˜æ–¹è¯„ä¼°):
        1. Exact Match (EM): æ ‡å‡†åŒ–åå®Œå…¨åŒ¹é…
        2. F1 Score: Tokençº§åˆ«çš„F1åˆ†æ•°
        3. æ•°å€¼ç­‰ä»·: æ•°å­—çš„è¯­ä¹‰ç­‰ä»·åˆ¤æ–­
        4. åŒ…å«å…³ç³»: ç®€çŸ­ç­”æ¡ˆåŒ…å«åœ¨é•¿ç­”æ¡ˆä¸­
        5. LLM Judge: è¯­ä¹‰ç­‰ä»·åˆ¤æ–­ï¼ˆå¯é€‰ï¼‰

        å¥–åŠ±ç­‰çº§:
        - 1.0: EM=1 æˆ– F1>=0.8 æˆ– æ•°å€¼ç­‰ä»· æˆ– LLMåˆ¤æ–­æ­£ç¡®
        - 0.7: F1>=0.5 æˆ– åŒ…å«å…³ç³»æˆç«‹
        - 0.4: F1>=0.3
        - 0.2: F1>=0.1 (æœ‰éƒ¨åˆ†ç›¸å…³å†…å®¹)
        - 0.0: æ— åŒ¹é…
        """
        if prediction is None:
            return 0.0

        pred_str = str(prediction).strip()
        gt_str = str(ground_truth).strip()

        if not pred_str:
            return 0.0

        # 1. é¦–å…ˆå°è¯•LLM Judge (å¦‚æœå¯ç”¨) - ç”¨äºè¯­ä¹‰ç­‰ä»·åˆ¤æ–­
        if self.use_llm_judge:
            is_correct = self._llm_judge_compare(
                problem=problem,
                prediction=pred_str,
                ground_truth=gt_str,
                problem_type="qa",
                source=source
            )
            if is_correct:
                return 1.0

        # 2. æ ‡å‡†åŒ–ç­”æ¡ˆ (å‚è€ƒSQuADå®˜æ–¹è¯„ä¼°è„šæœ¬)
        pred_normalized = self._normalize_answer_squad(pred_str)
        gt_normalized = self._normalize_answer_squad(gt_str)

        # 3. Exact Match (EM)
        if pred_normalized == gt_normalized:
            return 1.0

        # 4. æ•°å€¼ç­‰ä»·æ£€æŸ¥ (å›½é™…æ ‡å‡†: æ•°å­—è¯­ä¹‰ç­‰ä»·)
        #    ä¾‹å¦‚: "4" vs "four" vs "4 cylinders" åº”è¯¥åŒ¹é…
        if self._check_numeric_equivalence(pred_str, gt_str):
            return 1.0

        # 5. åŒ…å«å…³ç³»æ£€æŸ¥ (å›½é™…æ ‡å‡†: ç®€ç­”åŒ…å«åœ¨é•¿ç­”ä¸­)
        #    ä¾‹å¦‚: "Paris" vs "The capital is Paris"
        if self._check_containment(pred_normalized, gt_normalized):
            return 0.7

        # 6. F1 Scoreè®¡ç®— (SQuADæ ‡å‡†)
        f1 = self._compute_f1_score_squad(pred_normalized, gt_normalized)

        # æ ¹æ®F1åˆ†æ•°è¿”å›å¥–åŠ±
        if f1 >= 0.8:
            return 1.0
        elif f1 >= 0.5:
            return 0.7
        elif f1 >= 0.3:
            return 0.4
        elif f1 >= 0.1:
            return 0.2
        else:
            return 0.0

    def _normalize_answer_squad(self, text: str) -> str:
        """
        SQuADå®˜æ–¹æ ‡å‡†åŒ–æ–¹æ³•
        å‚è€ƒ: https://github.com/allenai/bi-att-flow/blob/master/squad/evaluate-v1.1.py
        """
        import string
        import re

        def remove_articles(text):
            return re.sub(r'\b(a|an|the)\b', ' ', text)

        def white_space_fix(text):
            return ' '.join(text.split())

        def remove_punc(text):
            exclude = set(string.punctuation)
            return ''.join(ch for ch in text if ch not in exclude)

        def lower(text):
            return text.lower()

        return white_space_fix(remove_articles(remove_punc(lower(text))))

    def _check_numeric_equivalence(self, pred: str, gt: str) -> bool:
        """
        æ£€æŸ¥æ•°å€¼è¯­ä¹‰ç­‰ä»·

        å¤„ç†æƒ…å†µ:
        - "4" vs "four" vs "4 cylinders"
        - "1990" vs "in 1990" vs "the year 1990"
        - "$100" vs "100 dollars" vs "100"
        """
        # æ•°å­—è¯æ˜ å°„
        number_words = {
            'zero': 0, 'one': 1, 'two': 2, 'three': 3, 'four': 4,
            'five': 5, 'six': 6, 'seven': 7, 'eight': 8, 'nine': 9,
            'ten': 10, 'eleven': 11, 'twelve': 12, 'thirteen': 13,
            'fourteen': 14, 'fifteen': 15, 'sixteen': 16, 'seventeen': 17,
            'eighteen': 18, 'nineteen': 19, 'twenty': 20, 'thirty': 30,
            'forty': 40, 'fifty': 50, 'sixty': 60, 'seventy': 70,
            'eighty': 80, 'ninety': 90, 'hundred': 100, 'thousand': 1000,
            'million': 1000000, 'billion': 1000000000
        }

        def extract_number(text: str) -> Optional[float]:
            text_lower = text.lower().strip()

            # ç›´æ¥æ•°å­—åŒ¹é…
            num_match = re.search(r'-?\d+\.?\d*', text_lower)
            if num_match:
                try:
                    return float(num_match.group())
                except:
                    pass

            # æ•°å­—è¯åŒ¹é…
            for word, num in number_words.items():
                if word in text_lower:
                    return float(num)

            return None

        pred_num = extract_number(pred)
        gt_num = extract_number(gt)

        if pred_num is not None and gt_num is not None:
            # ç²¾ç¡®åŒ¹é…æˆ–æ¥è¿‘åŒ¹é…
            if pred_num == gt_num:
                return True
            # å…è®¸å°è¯¯å·®
            if gt_num != 0 and abs(pred_num - gt_num) / abs(gt_num) < 0.01:
                return True

        return False

    def _check_containment(self, pred: str, gt: str) -> bool:
        """
        æ£€æŸ¥åŒ…å«å…³ç³» (å›½é™…æ ‡å‡†æ–¹æ³•)

        æƒ…å†µ1: é¢„æµ‹æ˜¯gtçš„å­ä¸² (predç®€çŸ­ä½†æ­£ç¡®)
        æƒ…å†µ2: gtæ˜¯é¢„æµ‹çš„å­ä¸² (gtç®€çŸ­ï¼Œpredæ›´å®Œæ•´)
        æƒ…å†µ3: è¯çº§åˆ«åŒ…å« (å¦‚ "watch" å‡ºç°åœ¨ "pocketwatch" ä¸­)
        """
        # è·³è¿‡å¤ªçŸ­çš„ç­”æ¡ˆï¼ˆé¿å…è¯¯åŒ¹é…ï¼‰
        if len(pred) < 2 or len(gt) < 2:
            return False

        # åŒå‘åŒ…å«æ£€æŸ¥
        if pred in gt or gt in pred:
            # é¢å¤–éªŒè¯ï¼šåŒ…å«çš„éƒ¨åˆ†åº”è¯¥æ˜¯æœ‰æ„ä¹‰çš„æ¯”ä¾‹
            shorter = pred if len(pred) < len(gt) else gt
            longer = gt if len(pred) < len(gt) else pred

            # çŸ­ç­”æ¡ˆåº”è¯¥æ˜¯é•¿ç­”æ¡ˆçš„ä¸»è¦éƒ¨åˆ†ï¼ˆè‡³å°‘30%ï¼‰
            if len(shorter) >= len(longer) * 0.3:
                return True

        # P4ä¿®å¤: è¯çº§åˆ«åŒ…å«æ£€æŸ¥ (å¤„ç†å¤åˆè¯å¦‚ pocketwatch)
        # æ£€æŸ¥predä¸­çš„æ¯ä¸ªè¯æ˜¯å¦å‡ºç°åœ¨gtçš„æŸä¸ªè¯ä¸­ï¼ˆæˆ–åè¿‡æ¥ï¼‰
        pred_words = pred.split()
        gt_words = gt.split()

        for pw in pred_words:
            if len(pw) >= 3:  # è¯é•¿åº¦è‡³å°‘3ï¼Œé¿å…è¯¯åŒ¹é…
                for gw in gt_words:
                    # æ£€æŸ¥è¯çº§åˆ«çš„åŒ…å«ï¼ˆå¦‚ "watch" in "pocketwatch"ï¼‰
                    # æ¡ä»¶: çŸ­è¯è‡³å°‘å é•¿è¯çš„40%ï¼ˆæ”¾å®½ä»¥åŒ¹é…å¤åˆè¯ï¼‰
                    if pw in gw and len(pw) >= len(gw) * 0.4:
                        return True
                    if gw in pw and len(gw) >= len(pw) * 0.4:
                        return True

        return False

    def _compute_f1_score_squad(self, pred: str, gt: str) -> float:
        """
        SQuADæ ‡å‡†F1è®¡ç®—
        å‚è€ƒ: https://rajpurkar.github.io/SQuAD-explorer/
        """
        from collections import Counter

        pred_tokens = pred.split()
        gt_tokens = gt.split()

        # è¾¹ç•Œæƒ…å†µ
        if len(gt_tokens) == 0:
            return 1.0 if len(pred_tokens) == 0 else 0.0
        if len(pred_tokens) == 0:
            return 0.0

        # è®¡ç®—å…±åŒtokens
        common = Counter(pred_tokens) & Counter(gt_tokens)
        num_same = sum(common.values())

        if num_same == 0:
            return 0.0

        precision = num_same / len(pred_tokens)
        recall = num_same / len(gt_tokens)
        f1 = 2 * precision * recall / (precision + recall)

        return f1

    def _normalize_answer(self, text: str) -> str:
        """æ ‡å‡†åŒ–ç­”æ¡ˆç”¨äºæ¯”è¾ƒ"""
        import string
        # å°å†™
        text = text.lower()
        # å»é™¤æ ‡ç‚¹
        text = text.translate(str.maketrans('', '', string.punctuation))
        # å»é™¤å¤šä½™ç©ºæ ¼
        text = ' '.join(text.split())
        return text

    def _compute_f1_score(self, pred: str, gt: str) -> float:
        """P1ä¿®å¤: è®¡ç®—tokençº§åˆ«F1åˆ†æ•°ï¼ˆä½¿ç”¨Counterè€Œésetï¼Œé¿å…å»é‡ä¸¢å¤±ä¿¡æ¯ï¼‰"""
        from collections import Counter

        pred_tokens = Counter(pred.split())
        gt_tokens = Counter(gt.split())

        if sum(gt_tokens.values()) == 0:
            return 1.0 if sum(pred_tokens.values()) == 0 else 0.0

        if sum(pred_tokens.values()) == 0:
            return 0.0

        # è®¡ç®—äº¤é›†ï¼ˆå–æœ€å°è®¡æ•°ï¼‰
        common = pred_tokens & gt_tokens
        num_same = sum(common.values())

        if num_same == 0:
            return 0.0

        precision = num_same / sum(pred_tokens.values())
        recall = num_same / sum(gt_tokens.values())
        f1 = 2 * precision * recall / (precision + recall)

        return f1

    def _compute_general_reward(self, prediction: Any, ground_truth: Any) -> float:
        """é€šç”¨å¥–åŠ±è®¡ç®—"""
        if prediction is None:
            return 0.0

        pred_str = str(prediction).strip().lower()
        gt_str = str(ground_truth).strip().lower()

        if pred_str == gt_str:
            return 1.0
        elif gt_str in pred_str:
            return 0.7
        elif self._compute_f1_score(pred_str, gt_str) > 0.5:
            return 0.4
        else:
            return 0.0

    def _extract_math_answer(self, text: str) -> Optional[str]:
        """
        P0-4ä¿®å¤: é²æ£’çš„æ•°å­¦ç­”æ¡ˆæå–

        æ”¯æŒ:
        - åµŒå¥—boxed: \\boxed{{a \\choose b}}
        - åˆ†æ•°: 5/324
        - ç™¾åˆ†æ¯”: 50%
        - ç§‘å­¦è®¡æ•°æ³•: 1.5e-3
        """
        if not text:
            return None

        # 1. ä¼˜å…ˆæå–boxed (æ”¯æŒåµŒå¥—)
        boxed = self._extract_boxed_robust(text)
        if boxed:
            # P1ä¿®å¤: æ£€æµ‹ä»£ç æ³„æ¼ï¼ˆä¸answer_extractor.pyä¿æŒä¸€è‡´ï¼‰
            code_leak_keywords = ['def ', 'return ', 'import ', 'class ', 'if __name__', 'async def ']
            if any(kw in boxed for kw in code_leak_keywords):
                # ä»£ç æ³„æ¼ï¼Œè·³è¿‡boxedç»§ç»­å°è¯•å…¶ä»–æå–æ–¹æ³•
                pass
            # æ£€æµ‹ç©ºboxed
            elif not boxed.strip():
                pass
            # æ£€æµ‹æ‰§è¡Œé”™è¯¯
            elif boxed.startswith('Error:') or 'Traceback' in boxed or 'SyntaxError' in boxed:
                pass
            else:
                return boxed

        # 2. æŸ¥æ‰¾"ç­”æ¡ˆæ˜¯"ã€"Therefore"ç­‰æ¨¡å¼åçš„å†…å®¹
        answer_patterns = [
            r'ç­”æ¡ˆ[æ˜¯ä¸ºï¼š:]+\s*([\d\./\-]+)',
            r'[Tt]he answer is[:\s]+([\d\./\-]+)',
            r'[Tt]herefore[,\s]+([\d\./\-]+)',
            r'[Ss]o[,\s]+([\d\./\-]+)',
            r'=\s*([\d\./\-]+)\s*$',
        ]

        for pattern in answer_patterns:
            match = re.search(pattern, text)
            if match:
                return match.group(1).strip()

        # 3. æå–æœ€åä¸€ä¸ªæ•°å­—
        numbers = self._extract_numbers(text)
        if numbers:
            return str(numbers[-1])

        # 4. è¿”å›æ•´ä¸ªæ–‡æœ¬ï¼ˆå¦‚æœå¾ˆçŸ­ï¼‰
        if len(text) < 50:
            return text.strip()

        return None

    def _extract_boxed_robust(self, text: str) -> Optional[str]:
        """
        P0-4ä¿®å¤: æ”¯æŒåµŒå¥—èŠ±æ‹¬å·çš„boxedæå–
        """
        # æ”¯æŒåµŒå¥—çš„æ­£åˆ™ï¼ˆæœ€å¤š2å±‚åµŒå¥—ï¼‰
        pattern = r'\\boxed\{((?:[^{}]|\{(?:[^{}]|\{[^{}]*\})*\})*)\}'
        matches = re.findall(pattern, text, re.DOTALL)

        if matches:
            # è¿”å›æœ€åä¸€ä¸ªåŒ¹é…ï¼ˆé€šå¸¸æ˜¯æœ€ç»ˆç­”æ¡ˆï¼‰
            return matches[-1].strip()

        # Fallback: ç®€å•æ¨¡å¼
        simple_match = re.search(r'\\boxed\{([^}]+)\}', text)
        if simple_match:
            return simple_match.group(1).strip()

        return None

    def _parse_number_robust(self, text: str) -> Optional[float]:
        """
        P0-4ä¿®å¤: é²æ£’çš„æ•°å­—è§£æ

        æ”¯æŒ:
        - åˆ†æ•°: 5/324
        - ç™¾åˆ†æ¯”: 50% -> 0.5
        - ç§‘å­¦è®¡æ•°æ³•: 1.5e-3
        - åƒåˆ†ä½: 1,234,567
        """
        if not text:
            return None

        text = text.strip()

        # å»é™¤åƒåˆ†ä½é€—å·
        text = text.replace(',', '')

        # ç™¾åˆ†æ¯”è½¬æ¢
        if '%' in text:
            try:
                num_str = text.replace('%', '').strip()
                return float(num_str) / 100.0
            except:
                pass

        # åˆ†æ•°è½¬æ¢
        if '/' in text:
            try:
                parts = text.split('/')
                if len(parts) == 2:
                    return float(parts[0].strip()) / float(parts[1].strip())
            except:
                pass

        # ç›´æ¥è§£æ
        try:
            return float(text)
        except:
            pass

        # æå–ç¬¬ä¸€ä¸ªæ•°å­—
        match = re.search(r'-?\d+\.?\d*(?:[eE][+-]?\d+)?', text)
        if match:
            try:
                return float(match.group())
            except:
                pass

        return None

    # ============== åŸæœ‰æ–¹æ³•ï¼ˆä¿ç•™å…¼å®¹æ€§ï¼‰ ==============

    def _is_math_correct(self, prediction: str, ground_truth: str) -> bool:
        """
        åˆ¤æ–­æ•°å­¦ç­”æ¡ˆæ˜¯å¦æ­£ç¡®
        
        æ”¯æŒ:
        - æ•°å­—æ¯”è¾ƒï¼ˆå«æµ®ç‚¹è¯¯å·®ï¼‰
        - åˆ†æ•°æ¯”è¾ƒï¼ˆå¦‚ 5/324 vs 0.0154...ï¼‰
        - å­—ç¬¦ä¸²åŒ¹é…
        """
        try:
            pred_str = str(prediction).strip()
            gt_str = str(ground_truth).strip()

            # å­—ç¬¦ä¸²å®Œå…¨åŒ¹é…
            if pred_str == gt_str:
                return True

            # è§£æä¸ºæ•°å€¼æ¯”è¾ƒï¼ˆæ”¯æŒåˆ†æ•°ï¼‰
            def parse_number(s: str) -> float:
                """è§£ææ•°å­—ï¼Œæ”¯æŒåˆ†æ•°æ ¼å¼"""
                if '/' in s:
                    parts = s.split('/')
                    return float(parts[0]) / float(parts[1])
                return float(s)

            try:
                pred_num = parse_number(pred_str)
                gt_num = parse_number(gt_str)

                # ä½¿ç”¨ç›¸å¯¹è¯¯å·®æ¯”è¾ƒï¼ˆå¤„ç†æµ®ç‚¹ç²¾åº¦ï¼‰
                rel_error = abs(pred_num - gt_num) / (abs(gt_num) + 1e-9)
                return rel_error < 1e-6
            except:
                pass

            # æ–¹æ³•1: boxed æ ¼å¼
            pred_boxed = self._extract_boxed(pred_str)
            gt_boxed = self._extract_boxed(gt_str)
            if pred_boxed and gt_boxed:
                try:
                    pred_num = parse_number(pred_boxed)
                    gt_num = parse_number(gt_boxed)
                    rel_error = abs(pred_num - gt_num) / (abs(gt_num) + 1e-9)
                    if rel_error < 1e-6:
                        return True
                except:
                    pass

            # æ–¹æ³•2: æ•°å­—æå–
            pred_numbers = self._extract_numbers(pred_str)
            gt_numbers = self._extract_numbers(gt_str)

            if not gt_numbers:
                # æ— æ³•æå–æ•°å­—ï¼Œç”¨å­—ç¬¦ä¸²åŒ¹é…
                return gt_str.strip().lower() in pred_str.strip().lower()

            if not pred_numbers:
                return False

            # æ¯”è¾ƒæœ€åä¸€ä¸ªæ•°å­—
            pred_answer = pred_numbers[-1]
            gt_answer = gt_numbers[-1]

            return abs(pred_answer - gt_answer) < 1e-4

        except Exception:
            return False

    class TimeoutError(Exception):
        pass

    def run_with_timeout(self, func, args, timeout):
        result = []
        stop_event = threading.Event()

        def target():
            try:
                result.append(func(*args))
            except Exception as e:
                result.append(e)
            finally:
                stop_event.set()

        thread = threading.Thread(target=target)
        thread.start()
        is_timeout = not stop_event.wait(timeout)

        if is_timeout:
            raise self.TimeoutError("Function execution timed out")

        if not result:
            return None
        if isinstance(result[0], Exception):
            raise result[0]
        return result[0]

    def _check_code_solution(self, solution: str, test: str, entry_point: str) -> bool:
        """
        Use execution to check if the code solution is correct.
        Inspired by AFlow's evaluation mechanism.
        """
        if not solution or not test or not entry_point:
            return False

        # Sanitize solution (remove markdown blocks if any)
        if "```python" in solution:
            solution = solution.split("```python")[1].split("```")[0]
        elif "```" in solution:
            solution = solution.split("```")[1].split("```")[0]
        
        try:
            global_dict = {
                "math": __import__("math"),
                "hashlib": __import__("hashlib"),
                "re": __import__("re"),
                "List": List,
                "Dict": Dict,
                "Tuple": Tuple,
                "Optional": Optional,
                "Any": Any,
            }

            # Execute the solution code
            exec(solution, global_dict)

            if entry_point not in global_dict:
                # Try to find if there is a 'solve' function or similar if entry_point is missing
                # But for HumanEval/MBPP, entry_point is strict.
                # If it's a full script, maybe we shouldn't fail immediately, but for now strict is better.
                return False

            # Execute the test code
            # The test code usually contains a 'check' function or assertions
            exec(test, global_dict)

            # Check if 'check' function exists (common in HumanEval)
            if "check" in global_dict:
                check = global_dict["check"]
                try:
                    # Run the check function with timeout
                    self.run_with_timeout(check, (global_dict[entry_point],), 5) # 5 seconds timeout
                    return True
                except Exception as e:
                    if self.debug_logging:
                        print(f"Code execution check failed: {e}")
                    return False
            else:
                # If no check function, assume the test code runs assertions directly
                # If exec(test) didn't raise exception, it might be correct
                return True

        except Exception as e:
            if self.debug_logging:
                print(f"Code execution error: {e}")
            return False

    def _is_code_correct(self, prediction: str, ground_truth: str, test: Optional[str] = None, entry_point: Optional[str] = None) -> bool:
        """åˆ¤æ–­ä»£ç ç­”æ¡ˆæ˜¯å¦æ­£ç¡®"""
        # Prioritize execution-based checking if test cases are available
        if test and entry_point:
            return self._check_code_solution(prediction, test, entry_point)
        
        # Fallback to string matching if execution is not possible
        try:
            pred_str = str(prediction).strip()
            gt_str = str(ground_truth).strip()

            if not pred_str:
                return False

            # ç²¾ç¡®åŒ¹é…
            if pred_str.lower() == gt_str.lower():
                return True

            # åŒ…å«åŒ¹é…
            if gt_str.lower() in pred_str.lower():
                return True

            return False

        except Exception:
            return False

    def _is_qa_correct(self, prediction: str, ground_truth: str) -> bool:
        """åˆ¤æ–­QAç­”æ¡ˆæ˜¯å¦æ­£ç¡®"""
        try:
            pred_str = str(prediction).strip().lower()
            gt_str = str(ground_truth).strip().lower()

            # ç²¾ç¡®åŒ¹é…
            if pred_str == gt_str:
                return True

            # åŒ…å«åŒ¹é…
            if gt_str in pred_str or pred_str in gt_str:
                return True

            # Tokené‡å é˜ˆå€¼ - P1ä¿®å¤: ä½¿ç”¨Counterä»£æ›¿set
            from collections import Counter
            pred_tokens = Counter(pred_str.split())
            gt_tokens = Counter(gt_str.split())

            if sum(gt_tokens.values()) == 0:
                return False

            # è®¡ç®—é‡å 
            common = pred_tokens & gt_tokens
            overlap_ratio = sum(common.values()) / sum(gt_tokens.values())
            return overlap_ratio > 0.8

        except Exception:
            return False

    def _is_general_correct(self, prediction: str, ground_truth: str) -> bool:
        """é€šç”¨æ­£ç¡®æ€§åˆ¤æ–­"""
        try:
            pred_str = str(prediction).strip().lower()
            gt_str = str(ground_truth).strip().lower()

            return pred_str == gt_str or gt_str in pred_str

        except Exception:
            return False

    def _compute_correctness_reward(
        self,
        prediction: Any,
        ground_truth: Any,
        problem_type: str
    ) -> float:
        """
        è®¡ç®—æ­£ç¡®æ€§å¥–åŠ±ï¼ˆä¿ç•™ç”¨äºå‘åå…¼å®¹ï¼‰
        
        Returns:
            reward: 1.0 or 0.0
        """
        # This function is kept for compatibility but compute_reward should be used
        # We map the binary 0/1 back to whatever range was expected if needed, 
        # but here we simply return 1.0 or 0.0 as requested.
        
        if prediction is None:
            return 0.0

        is_correct = False
        if problem_type == "math":
            is_correct = self._is_math_correct(prediction, ground_truth)
        elif problem_type == "code":
            # Without test cases here, we fall back to string matching which is weak
            is_correct = self._is_code_correct(prediction, ground_truth)
        elif problem_type == "qa":
            is_correct = self._is_qa_correct(prediction, ground_truth)
        else:
            is_correct = self._is_general_correct(prediction, ground_truth)
            
        return 1.0 if is_correct else 0.0

    def _extract_boxed(self, text: str) -> Optional[str]:
        """æå–\\boxed{}ä¸­çš„å†…å®¹(ROLLé£æ ¼)"""
        match = re.search(r'\\boxed\{([^}]+)\}', text)
        if match:
            return match.group(1).strip()
        return None

    def _extract_numbers(self, text: str) -> list:
        """ä»æ–‡æœ¬ä¸­æå–æ‰€æœ‰æ•°å­—(æ”¹è¿›ç‰ˆ + æ–‡å­—æ•°å­—è¯†åˆ«)"""
        numbers = []

        # Method 1: Numeric extraction (existing)
        # åŒ¹é…æ•´æ•°ã€å°æ•°ã€è´Ÿæ•°ã€ç§‘å­¦è®¡æ•°æ³•
        pattern = r'-?\d+\.?\d*(?:[eE][+-]?\d+)?'
        matches = re.findall(pattern, text)
        for m in matches:
            if m:
                try:
                    numbers.append(float(m))
                except:
                    pass

        # Method 2: Word-to-number recognition (NEW - fixes ~15-20% QA errors)
        # Aligns with SQuAD/HotpotQA standards for text-based answers
        word_to_num = {
            'zero': 0, 'one': 1, 'two': 2, 'three': 3, 'four': 4,
            'five': 5, 'six': 6, 'seven': 7, 'eight': 8, 'nine': 9,
            'ten': 10, 'eleven': 11, 'twelve': 12, 'thirteen': 13,
            'fourteen': 14, 'fifteen': 15, 'sixteen': 16, 'seventeen': 17,
            'eighteen': 18, 'nineteen': 19, 'twenty': 20, 'thirty': 30,
            'forty': 40, 'fifty': 50, 'sixty': 60, 'seventy': 70,
            'eighty': 80, 'ninety': 90, 'hundred': 100, 'thousand': 1000
        }

        text_lower = text.lower()
        for word, num in word_to_num.items():
            if word in text_lower:
                numbers.append(float(num))

        return numbers

    def _extract_function_names(self, code: str) -> list:
        """ä»ä»£ç ä¸­æå–å‡½æ•°å"""
        pattern = r'def\s+(\w+)\s*\('
        matches = re.findall(pattern, code)
        return matches

    def _compute_efficiency_reward(self, cost: float) -> float:
        return 0.0

    def _compute_simplicity_reward(
        self,
        execution_time: float,
        num_operators: int = 1
    ) -> float:
        return 0.0

    def _compute_format_reward(self, response: str, problem_type: str) -> float:
        return 0.0

    def _compute_repetition_penalty(self, response: str, ngram_size: int = 3) -> float:
        return 0.0

    def print_eval_stats(self):
        """
        æ‰“å°è¯„ä¼°ç»Ÿè®¡ä¿¡æ¯ï¼ˆç”¨äºè°ƒè¯•ï¼‰
        """
        stats = self.eval_stats
        total = stats['total_evaluations']

        if total == 0:
            print("\nğŸ“Š è¯„ä¼°ç»Ÿè®¡: æ— è¯„ä¼°è®°å½•")
            return

        print(f"\nğŸ“Š è¯„ä¼°ç»Ÿè®¡ (æ€»è®¡: {total} æ¬¡):")
        print(f"  âœ… LLM JudgeæˆåŠŸ: {stats['llm_judge_success']} ({stats['llm_judge_success']/total*100:.1f}%)")
        print(f"  âš ï¸  è§£æå¤±è´¥: {stats['llm_judge_parse_failures']} ({stats['llm_judge_parse_failures']/total*100:.1f}%)")
        print(f"  âŒ APIå¤±è´¥: {stats['llm_judge_api_failures']} ({stats['llm_judge_api_failures']/total*100:.1f}%)")
        print(f"\n  åˆ¤å†³ç»“æœ:")
        print(f"    æ­£ç¡®: {stats['correct_predictions']} ({stats['correct_predictions']/total*100:.1f}%)")
        print(f"    é”™è¯¯: {stats['incorrect_predictions']} ({stats['incorrect_predictions']/total*100:.1f}%)")

        # è®¡ç®—å‡†ç¡®ç‡ï¼ˆå¦‚æœæœ‰è¶³å¤Ÿæ•°æ®ï¼‰
        judged = stats['correct_predictions'] + stats['incorrect_predictions']
        if judged > 0:
            accuracy = stats['correct_predictions'] / judged * 100
            print(f"\n  ğŸ¯ é¢„æµ‹å‡†ç¡®ç‡: {accuracy:.1f}% (åŸºäº{judged}æ¬¡æˆåŠŸè¯„ä¼°)")

    def reset_eval_stats(self):
        """é‡ç½®è¯„ä¼°ç»Ÿè®¡è®¡æ•°å™¨"""
        self.eval_stats = {
            'total_evaluations': 0,
            'llm_judge_success': 0,
            'llm_judge_parse_failures': 0,
            'llm_judge_api_failures': 0,
            'correct_predictions': 0,
            'incorrect_predictions': 0
        }
        print("ğŸ”„ è¯„ä¼°ç»Ÿè®¡å·²é‡ç½®")


def test_reward_computer():
    """æµ‹è¯•æ”¹è¿›ç‰ˆå¥–åŠ±è®¡ç®—å™¨"""
    print("\n" + "=" * 60)
    print("ğŸ§ª æµ‹è¯•æ”¹è¿›ç‰ˆå¥–åŠ±è®¡ç®—å™¨")
    print("=" * 60)

    computer = RewardComputer()

    # æµ‹è¯•æ¡ˆä¾‹
    test_cases = [
        {
            "name": "æ•°å­¦ - å®Œç¾æ ¼å¼+æ­£ç¡®",
            "problem": "What is 15 + 27?",
            "prediction": "<think>Let me calculate: 15 + 27 = 42</think><answer>\\boxed{42}</answer>",
            "ground_truth": "42",
            "problem_type": "math",
            "metadata": {"cost": 0.002, "execution_time": 3.5}
        },
        {
            "name": "ä»£ç  - ç®€å•æµ‹è¯•",
            "problem": "Write a function to square a number",
            "prediction": "def square(x):\n    return x * x",
            "ground_truth": "def square(x):\n    return x * x",
            "problem_type": "code",
            "test": "check = lambda func: func(2) == 4",
            "entry_point": "square",
            "metadata": {"cost": 0.003, "execution_time": 5.0}
        }
    ]

    for case in test_cases:
        reward = computer.compute_reward(
            problem=case["problem"],
            prediction=case["prediction"],
            ground_truth=case["ground_truth"],
            problem_type=case["problem_type"],
            metadata=case["metadata"],
            test=case.get("test"),
            entry_point=case.get("entry_point")
        )

        print(f"\nğŸ“ {case['name']}")
        print(f"  é¢„æµ‹: {case['prediction'][:60]}...")
        print(f"  æ­£ç¡®ç­”æ¡ˆ: {case['ground_truth']}")
        print(f"  å¥–åŠ±: {reward:.2f}")


if __name__ == "__main__":
    test_reward_computer()
