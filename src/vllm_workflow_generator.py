#!/usr/bin/env python3
"""
vLLMå·¥ä½œæµç”Ÿæˆå™¨ - ä½¿ç”¨vLLM APIè¿›è¡Œå¹¶å‘æ¨ç†ï¼ˆFallback: ä½¿ç”¨transformersï¼‰
"""
import asyncio
import torch
from openai import AsyncOpenAI
from typing import Dict, List, Optional, Tuple
import json
import ast
from pathlib import Path
from transformers import AutoModelForCausalLM, AutoTokenizer

class VLLMWorkflowGenerator:
    """ä½¿ç”¨vLLM APIç”Ÿæˆä¼˜åŒ–çš„å·¥ä½œæµï¼ˆæ”¯æŒå¹¶å‘ï¼‰

    æ”¯æŒä¸¤ç§æ¨¡å¼ï¼š
    1. vLLM APIæ¨¡å¼ï¼ˆæ¨èï¼‰ï¼šé€šè¿‡AsyncOpenAIå®¢æˆ·ç«¯è°ƒç”¨vLLMæœåŠ¡
    2. Transformersæ¨¡å¼ï¼ˆFallbackï¼‰ï¼šç›´æ¥ä½¿ç”¨transformersåº“
    """

    def __init__(
        self,
        base_url: str = "http://localhost:8003/v1",
        api_key: str = "EMPTY",
        model_name: str = "/home/yijia/verl-agent/models/qwen/Qwen2___5-7B-Instruct",
        max_concurrent: int = 6,
        operator_descriptions_path: Optional[str] = None,
        config: Optional[Dict] = None,
        use_vllm_api: bool = False,  # é»˜è®¤ä½¿ç”¨transformersæ¨¡å¼
        device: str = "cuda:0"
    ):
        """
        Args:
            base_url: vLLMæœåŠ¡å™¨åœ°å€
            api_key: APIå¯†é’¥ï¼ˆvLLMä¸éœ€è¦çœŸå®å¯†é’¥ï¼‰
            model_name: æ¨¡å‹åç§°/è·¯å¾„
            max_concurrent: æœ€å¤§å¹¶å‘è¯·æ±‚æ•°
            operator_descriptions_path: AFlowç®—å­æè¿°æ–‡ä»¶è·¯å¾„
            config: é¢å¤–é…ç½®
            use_vllm_api: æ˜¯å¦ä½¿ç”¨vLLM APIï¼ˆFalseåˆ™ä½¿ç”¨transformersï¼‰
            device: è®¾å¤‡ï¼ˆtransformersæ¨¡å¼ï¼‰
        """
        self.model_name = model_name
        self.max_concurrent = max_concurrent
        self.config = config or {}
        self.use_vllm_api = use_vllm_api
        self.device = device

        # åŠ è½½ç®—å­æè¿°
        self.operator_descriptions = self._load_operator_descriptions(operator_descriptions_path)

        if use_vllm_api:
            # vLLM APIæ¨¡å¼
            self.client = AsyncOpenAI(
                base_url=base_url,
                api_key=api_key,
                timeout=300.0,  # 5åˆ†é’Ÿè¶…æ—¶
                max_retries=2
            )
            self.semaphore = asyncio.Semaphore(max_concurrent)
            print(f"âœ… åˆå§‹åŒ–vLLMå·¥ä½œæµç”Ÿæˆå™¨ï¼ˆAPIæ¨¡å¼ï¼‰")
            print(f"  æœåŠ¡å™¨: {base_url}")
            print(f"  æœ€å¤§å¹¶å‘: {max_concurrent}")
        else:
            # Transformersæ¨¡å¼ï¼ˆç›´æ¥ä½¿ç”¨å·²åŠ è½½çš„æ¨¡å‹ï¼‰
            self.model = None  # å°†ç”±å¤–éƒ¨è®¾ç½®ï¼ˆé¿å…é‡å¤åŠ è½½ï¼‰
            self.tokenizer = None
            # âš ï¸ å…³é”®ä¿®å¤ï¼šä½¿ç”¨é”ä¿æŠ¤GPUè®¿é—®ï¼ˆåŒä¸€æ—¶é—´åªå…è®¸ä¸€ä¸ªæ¨ç†ï¼‰
            self._generation_lock = asyncio.Lock()
            print(f"âœ… åˆå§‹åŒ–workflowç”Ÿæˆå™¨ï¼ˆTransformersæ¨¡å¼ï¼‰")
            print(f"  æ¨¡å‹: {model_name}")
            print(f"  è®¾å¤‡: {device}")
            print(f"  âš ï¸  GPUæ¨ç†å°†ä¸²è¡Œæ‰§è¡Œï¼ˆé¿å…CUDAå†²çªï¼‰")

    def _load_operator_descriptions(self, descriptions_path: Optional[str]) -> Dict:
        """åŠ è½½AFlowç®—å­æè¿°"""
        if descriptions_path and Path(descriptions_path).exists():
            with open(descriptions_path, 'r') as f:
                return json.load(f)

        # é»˜è®¤ç®—å­æè¿°
        return {
            "Custom": {
                "description": "Generates anything based on customized input and instruction.",
                "interface": "custom(input: str, instruction: str) -> dict with key 'response'"
            },
            "AnswerGenerate": {
                "description": "Generates step-by-step reasoning and final answer.",
                "interface": "answer_generate(input: str) -> dict with keys 'thought' and 'answer'"
            },
            "Programmer": {
                "description": "Automatically writes and executes Python code.",
                "interface": "programmer(problem: str, analysis: str = 'None') -> dict with keys 'code' and 'output'"
            },
            "ScEnsemble": {
                "description": "Uses self-consistency to select the most frequent solution.",
                "interface": "sc_ensemble(solutions: List[str], problem: str) -> dict with key 'response'"
            },
            "Review": {
                "description": "Reviews and provides feedback on a solution.",
                "interface": "review(problem: str, solution: str) -> dict with keys 'review_result' and 'feedback'"
            },
            "Revise": {
                "description": "Revises solution based on feedback.",
                "interface": "revise(problem: str, solution: str, feedback: str) -> dict with key 'solution'"
            }
        }

    def _build_generation_prompt(self, problem: str, problem_type: str) -> str:
        """æ„å»ºç”Ÿæˆæç¤ºè¯ - è‡ªç”±æ¢ç´¢ç®—å­ç»„åˆ + TASK_PROMPTä¼˜åŒ–"""
        prompt = f"""Generate a Python Workflow to solve the given problem.

## YOUR GOAL: Design the best workflow by COMBINING operators freely!
- **COMBINE** 1-7 operators in creative ways (not just pick one!)
- Design effective TASK_PROMPT to guide the execution LLM
- The TASK_PROMPT will be automatically injected into the problem input
- Think: What combination of operators would solve this problem best?

## TASK_PROMPT (Required)
Define a TASK_PROMPT that guides the LLM. This prompt will be prepended to the problem.
Good prompts include:
- Problem-solving strategies
- Step-by-step instructions
- Format requirements (e.g., \\boxed{{}} for math answers)
- Domain-specific hints

## Available Operators (COMBINE freely - use 1 or more together!):

1. **Custom(llm)** - Execute with YOUR custom instruction
   Call: await self.custom(input=str, instruction=str)
   Returns: {{'response': str}}

2. **AnswerGenerate(llm)** - Step-by-step reasoning
   Call: await self.answer_generate(input=str)
   Returns: {{'thought': str, 'answer': str}}

3. **Programmer(llm)** - Generate and execute Python code, returns EXECUTION RESULT
   Call: await self.programmer(problem=str, analysis=str)
   Returns: {{'code': str (source code - NEVER use this as answer!), 'output': str (EXECUTION RESULT - ALWAYS use this!)}}
   âš ï¸ CRITICAL: result['output'] = computed answer (e.g., "42"), result['code'] = source code (e.g., "def solve(): return 42")
   âœ… CORRECT: answer = result['output']  # Gets "42"
   âŒ WRONG: answer = result['code']  # Gets "def solve(): return 42" - THIS IS A BUG!

4. **Review(llm)** - Review solution
   Call: await self.review(problem=str, solution=str)
   Returns: {{'review_result': bool, 'feedback': str}}

5. **Revise(llm)** - Revise based on feedback
   Call: await self.revise(problem=str, solution=str, feedback=str)
   Returns: {{'solution': str}}

6. **ScEnsemble(llm)** - Ensemble voting
   Call: await self.sc_ensemble(solutions=list, problem=str)
   Returns: {{'response': str}}

7. **Test(llm)** - Generate and run test cases for code
   Call: await self.test(problem=str, solution=str, entry_point=str)
   Returns: {{'result': bool, 'solution': str}}
   **IMPORTANT**: For code problems, entry_point is provided as __call__ parameter!

## OUTPUT FORMAT:

```python
# === PROMPT_CUSTOM START ===
TASK_PROMPT = \"\"\"Your task-specific prompt here...\"\"\"
# === PROMPT_CUSTOM END ===

class Workflow:
    def __init__(self, name: str, llm_config, dataset: DatasetType):
        self.name = name
        self.llm = create_llm_instance(llm_config)
        # Initialize any operators you need
        self.answer_generate = operator.AnswerGenerate(self.llm)
        self.programmer = operator.Programmer(self.llm)  # For math calculations
        # ... add more operators as needed

    # For code problems: async def __call__(self, problem: str, entry_point: str = "solve"):
    # For math/qa problems: async def __call__(self, problem: str):
    async def __call__(self, problem: str, entry_point: str = "solve"):
        # Your workflow logic - use any operators
        # For code: use entry_point parameter when calling test()
        solution = await self.answer_generate(input=problem)
        return solution['answer'], self.llm.get_usage_summary()["total_cost"]
```

## âš ï¸âš ï¸âš ï¸ CRITICAL BUG PREVENTION - READ THIS âš ï¸âš ï¸âš ï¸
**Programmer returns TWO fields - you MUST use the correct one:**
- `result['output']` = THE ANSWER (computed result like "42", "3.14", "hello")
- `result['code']` = THE SOURCE CODE (like "def solve(): return 42") - NEVER USE THIS AS ANSWER!

```python
# âœ…âœ…âœ… CORRECT - This returns the computed answer:
result = await self.programmer(problem=problem, analysis="Calculate")
final_answer = result['output']  # e.g., "42" - THIS IS CORRECT!
return "\\boxed{{" + final_answer + "}}", cost  # Returns \\boxed{{42}}

# âŒâŒâŒ WRONG - This returns Python source code (BUG!):
result = await self.programmer(problem=problem, analysis="Calculate")
final_answer = result['code']  # e.g., "def solve(): return 42" - THIS IS A BUG!
return "\\boxed{{" + final_answer + "}}", cost  # Returns \boxed{{def solve...}} - WRONG!
```
**REMEMBER: output=answer, code=source. ALWAYS use result['output'] for the final answer!**

## DESIGN FREELY:
- Use 1 operator OR combine multiple operators
- Create iterative loops (while/for) if needed
- Chain outputs as inputs to next operators
- Design conditional logic based on review results

---

Problem to solve: {problem}
Problem type: {problem_type}

Generate your PROMPT_CUSTOM and Workflow class:
"""
        return prompt

    async def generate_workflow(
        self,
        problem: str,
        problem_type: str = "math",
        temperature: float = 0.7,
        max_new_tokens: int = 2048,
        custom_prompt: Optional[str] = None
    ) -> Dict:
        """
        ç”Ÿæˆå•ä¸ªå·¥ä½œæµï¼ˆå¼‚æ­¥ï¼‰

        Returns:
            {
                "workflow_code": "Pythonä»£ç ",
                "valid": bool,
                "error": Optional[str],
                "metadata": {...}
            }
        """
        if self.use_vllm_api:
            return await self._generate_with_vllm_api(
                problem, problem_type, temperature, max_new_tokens, custom_prompt
            )
        else:
            return await self._generate_with_transformers(
                problem, problem_type, temperature, max_new_tokens, custom_prompt
            )

    async def _generate_with_vllm_api(
        self,
        problem: str,
        problem_type: str,
        temperature: float,
        max_tokens: int,
        custom_prompt: Optional[str]
    ) -> Dict:
        """ä½¿ç”¨vLLM APIç”Ÿæˆ"""
        async with self.semaphore:  # æ§åˆ¶å¹¶å‘æ•°
            try:
                # æ„å»ºæç¤ºè¯
                prompt = custom_prompt or self._build_generation_prompt(problem, problem_type)

                # è°ƒç”¨vLLM API
                response = await self.client.chat.completions.create(
                    model=self.model_name,
                    messages=[
                        {"role": "system", "content": "You are a workflow generation expert."},
                        {"role": "user", "content": prompt}
                    ],
                    temperature=temperature,
                    max_tokens=max_tokens,
                    top_p=self.config.get('top_p', 0.95),
                )

                # æå–ç”Ÿæˆçš„ä»£ç 
                generated_text = response.choices[0].message.content
                workflow_code, is_valid, error = self._parse_workflow_code(generated_text, problem_type)

                return {
                    "workflow_code": workflow_code,
                    "valid": is_valid,
                    "error": error,
                    "metadata": {
                        "tokens": response.usage.total_tokens if response.usage else 0,
                        "model": self.model_name
                    }
                }

            except Exception as e:
                return {
                    "workflow_code": "",
                    "valid": False,
                    "error": str(e),
                    "metadata": {}
                }

    async def _generate_with_transformers(
        self,
        problem: str,
        problem_type: str,
        temperature: float,
        max_new_tokens: int,
        custom_prompt: Optional[str]
    ) -> Dict:
        """ä½¿ç”¨transformersç”Ÿæˆï¼ˆä½¿ç”¨é”ä¿æŠ¤GPUè®¿é—®ï¼‰"""
        # âš ï¸ å…³é”®ï¼šä½¿ç”¨é”ç¡®ä¿åŒä¸€æ—¶é—´åªæœ‰ä¸€ä¸ªæ¨ç†åœ¨æ‰§è¡Œ
        async with self._generation_lock:
            loop = asyncio.get_event_loop()

            def _sync_generate():
                """åŒæ­¥ç”Ÿæˆå‡½æ•°ï¼ˆåœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œï¼‰"""
                # æ„å»ºæç¤ºè¯
                prompt = custom_prompt or self._build_generation_prompt(problem, problem_type)

                # Tokenize
                inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device)

                # ç”Ÿæˆ
                with torch.no_grad():
                    outputs = self.model.generate(
                        **inputs,
                        max_new_tokens=max_new_tokens,
                        temperature=temperature,
                        top_p=self.config.get('top_p', 0.95),
                        top_k=self.config.get('top_k', 50),
                        do_sample=True,
                        pad_token_id=self.tokenizer.eos_token_id
                    )

                # è§£ç 
                generated_text = self.tokenizer.decode(
                    outputs[0][inputs['input_ids'].shape[1]:],
                    skip_special_tokens=True
                )

                return generated_text

            try:
                # åœ¨é»˜è®¤executorä¸­è¿è¡Œï¼ˆCPUå¯†é›†å‹æ“ä½œï¼‰
                generated_text = await loop.run_in_executor(None, _sync_generate)

                # è§£æè¾“å‡º
                workflow_code, is_valid, error = self._parse_workflow_code(generated_text, problem_type)

                return {
                    "workflow_code": workflow_code,
                    "valid": is_valid,
                    "error": error,
                    "metadata": {
                        "problem": problem,
                        "problem_type": problem_type,
                        "temperature": temperature
                    }
                }
            except Exception as e:
                return {
                    "workflow_code": "",
                    "valid": False,
                    "error": str(e),
                    "metadata": {}
                }

    def _parse_workflow_code(self, generated_text: str, problem_type: str) -> Tuple[str, bool, Optional[str]]:
        """è§£æç”Ÿæˆçš„æ–‡æœ¬ï¼Œæå–å¹¶éªŒè¯å·¥ä½œæµä»£ç ï¼ˆæ”¯æŒprompt_customï¼‰"""
        import re

        # æå–ä»£ç å—
        code_start = generated_text.find("```python")
        if code_start == -1:
            code_start = generated_text.find("class Workflow:")
            if code_start == -1:
                return self._get_default_workflow(problem_type), False, "No Workflow class found"
            code = generated_text[code_start:]
        else:
            code_start += len("```python\n")
            code_end = generated_text.find("```", code_start)
            code = generated_text[code_start:code_end] if code_end != -1 else generated_text[code_start:]

        code = code.strip()

        # ğŸ”§ è§£æå¹¶æå–prompt_customéƒ¨åˆ†
        # æ£€æŸ¥æ˜¯å¦åŒ…å«PROMPT_CUSTOMæ ‡è®°
        prompt_custom_start = code.find("# === PROMPT_CUSTOM START ===")
        prompt_custom_end = code.find("# === PROMPT_CUSTOM END ===")

        prompt_custom_code = ""
        if prompt_custom_start != -1 and prompt_custom_end != -1:
            # æå–prompt_customéƒ¨åˆ†ï¼ˆåŒ…å«ç»“æŸæ ‡è®°è¡Œï¼‰
            end_line_end = code.find("\n", prompt_custom_end)
            if end_line_end == -1:
                end_line_end = len(code)
            prompt_custom_code = code[prompt_custom_start:end_line_end + 1]
            print(f"  ğŸ“ æ£€æµ‹åˆ°PROMPT_CUSTOMå®šä¹‰")
        else:
            # æ²¡æœ‰æ ‡è®°ï¼Œå°è¯•æ£€æµ‹TASK_PROMPTç­‰å˜é‡å®šä¹‰
            task_prompt_match = re.search(r'^(TASK_PROMPT\s*=\s*["\'\"\"\"].*?["\'\"\"]\s*$)', code, re.MULTILINE | re.DOTALL)
            if task_prompt_match:
                # æå–æ‰€æœ‰é¡¶çº§çš„PROMPTå˜é‡å®šä¹‰
                prompt_patterns = re.findall(
                    r'^([A-Z_]+_PROMPT\s*=\s*(?:["\'\"\"\"].*?["\'\"\"]|f["\'\"\"\"].*?["\'\"\"])\s*)$',
                    code,
                    re.MULTILINE | re.DOTALL
                )
                if prompt_patterns:
                    prompt_custom_code = "\n".join(prompt_patterns)
                    print(f"  ğŸ“ æ£€æµ‹åˆ° {len(prompt_patterns)} ä¸ªPROMPTå˜é‡å®šä¹‰")

        # å¦‚æœæ²¡æœ‰prompt_customï¼Œåˆ›å»ºä¸€ä¸ªé»˜è®¤çš„
        if not prompt_custom_code:
            # ä¸ºä¸åŒé—®é¢˜ç±»å‹åˆ›å»ºåˆé€‚çš„é»˜è®¤prompt
            if problem_type == "math":
                prompt_custom_code = '''TASK_PROMPT = """Solve this mathematical problem step by step.
Show your reasoning clearly and provide the final numerical answer.
Format: First explain your approach, then show calculations, finally state the answer."""
'''
            elif problem_type == "code":
                prompt_custom_code = '''TASK_PROMPT = """Write a Python function to solve this problem.
Requirements:
1. The function should be efficient and handle edge cases
2. Include proper input validation
3. Return the correct type as specified"""
'''
            else:
                prompt_custom_code = '''TASK_PROMPT = """Solve this problem carefully.
Provide a clear, structured answer with reasoning."""
'''
            print(f"  ğŸ“ ä½¿ç”¨é»˜è®¤PROMPT_CUSTOM (é—®é¢˜ç±»å‹: {problem_type})")

        # ç¡®ä¿prompt_customä»£ç åœ¨Workflowç±»ä¹‹å‰
        # ç§»é™¤åŸå§‹ä½ç½®çš„prompt_customï¼Œç„¶åæ·»åŠ åˆ°å¼€å¤´
        if prompt_custom_start != -1 and prompt_custom_end != -1:
            end_line_end = code.find("\n", prompt_custom_end)
            if end_line_end == -1:
                end_line_end = len(code)
            code = code[:prompt_custom_start] + code[end_line_end + 1:]

        # åœ¨importè¯­å¥ä¹‹å‰æ·»åŠ prompt_custom
        import_match = re.search(r'^import |^from ', code, re.MULTILINE)
        if import_match:
            code = prompt_custom_code + "\n" + code
        else:
            # å¦‚æœæ²¡æœ‰importï¼Œåœ¨classä¹‹å‰æ·»åŠ 
            class_match = re.search(r'^class Workflow', code, re.MULTILINE)
            if class_match:
                code = prompt_custom_code + "\n" + code[:class_match.start()] + code[class_match.start():]
            else:
                code = prompt_custom_code + "\n" + code

        # âš ï¸ Auto-Fixï¼šè‡ªåŠ¨ä¿®å¤ç¼ºå¤±çš„operatoråˆå§‹åŒ–
        code = self._validate_and_fix_workflow(code, problem_type)

        # éªŒè¯è¯­æ³•
        try:
            ast.parse(code)
            is_valid = True
            error = None
        except SyntaxError as e:
            is_valid = False
            error = f"Syntax error: {str(e)}"
            code = self._get_default_workflow(problem_type)

        return code, is_valid, error

    def _validate_and_fix_workflow(self, code: str, problem_type: str) -> str:
        """éªŒè¯å¹¶è‡ªåŠ¨ä¿®å¤workflowä¸­ç¼ºå¤±çš„operatoråˆå§‹åŒ–

        Args:
            code: ç”Ÿæˆçš„workflowä»£ç 
            problem_type: é—®é¢˜ç±»å‹

        Returns:
            ä¿®å¤åçš„ä»£ç 
        """
        import re

        # 1. æå–__init__ä¸­å·²åˆå§‹åŒ–çš„operators
        initialized_ops = set()
        init_section = re.search(r'def __init__\([^)]+\):[\s\S]+?(?=\n    async def|\n    def|$)', code)
        if init_section:
            init_code = init_section.group(0)
            # åŒ¹é… self.xxx = operator.XXX(self.llm)
            init_patterns = re.findall(r'self\.(\w+)\s*=\s*operator\.(\w+)\(', init_code)
            for attr_name, op_name in init_patterns:
                initialized_ops.add(attr_name)

        # 2. æå–__call__ä¸­ä½¿ç”¨çš„operators
        used_ops = set()
        call_section = re.search(r'async def __call__\([^)]+\):[\s\S]+', code)
        if call_section:
            call_code = call_section.group(0)
            # åŒ¹é… await self.xxx(...)
            used_patterns = re.findall(r'await self\.(\w+)\(', call_code)
            for op_name in used_patterns:
                used_ops.add(op_name)

        # 3. æ‰¾å‡ºç¼ºå¤±çš„operators
        missing_ops = used_ops - initialized_ops

        if missing_ops:
            print(f"\nâš ï¸  æ£€æµ‹åˆ°ç¼ºå¤±çš„operatoråˆå§‹åŒ–: {missing_ops}")
            print(f"   å·²åˆå§‹åŒ–: {initialized_ops}")
            print(f"   å·²ä½¿ç”¨: {used_ops}")

            # 4. è‡ªåŠ¨æ·»åŠ ç¼ºå¤±çš„åˆå§‹åŒ–ä»£ç 
            # æ‰¾åˆ° self.llm = create_llm_instance(...) çš„ä½ç½®
            llm_init_match = re.search(r'(\s+)(self\.llm = create_llm_instance\([^)]+\))', code)
            if llm_init_match:
                indent = llm_init_match.group(1)
                llm_init_line = llm_init_match.group(2)

                # æ„å»ºç¼ºå¤±çš„åˆå§‹åŒ–ä»£ç 
                missing_inits = []
                for op_name in sorted(missing_ops):
                    # æ¨æ–­operatorç±»åï¼ˆé¦–å­—æ¯å¤§å†™+é©¼å³°å‘½åï¼‰
                    # answer_generate -> AnswerGenerate
                    # review -> Review
                    op_class_name = ''.join(word.capitalize() for word in op_name.split('_'))

                    # æ£€æŸ¥æ˜¯å¦æ˜¯æœ‰æ•ˆçš„operatorï¼ˆä»promptä¸­è·å–ï¼‰
                    valid_operators = ['Custom', 'AnswerGenerate', 'Programmer', 'Test', 'Review', 'Revise', 'ScEnsemble']
                    if op_class_name in valid_operators:
                        missing_inits.append(f"{indent}self.{op_name} = operator.{op_class_name}(self.llm)")

                if missing_inits:
                    # åœ¨ self.llm = ... ä¹‹åæ’å…¥
                    insert_code = '\n' + '\n'.join(missing_inits)
                    code = code.replace(llm_init_line, llm_init_line + insert_code)
                    print(f"âœ… è‡ªåŠ¨æ·»åŠ äº† {len(missing_inits)} ä¸ªç¼ºå¤±çš„operatoråˆå§‹åŒ–")

        return code

    def _get_default_workflow(self, problem_type: str = "math") -> str:
        """é»˜è®¤å·¥ä½œæµ - åŒ…å«TASK_PROMPT"""
        # æ ¹æ®é—®é¢˜ç±»å‹é€‰æ‹©åˆé€‚çš„é»˜è®¤prompt
        if problem_type == "math":
            task_prompt = '''"""Solve this mathematical problem step by step.
Show your complete reasoning process:
1. Identify what the problem is asking
2. List known information and variables
3. Apply relevant formulas or methods
4. Perform calculations carefully
5. State the final numerical answer clearly

IMPORTANT: Always verify your answer before providing it."""'''
        elif problem_type == "code":
            task_prompt = '''"""Write a Python function to solve this problem.
Requirements:
1. Handle all edge cases properly
2. Use efficient algorithms
3. Include proper input validation
4. Return the correct type as specified
5. Add brief comments for complex logic"""'''
        else:
            task_prompt = '''"""Solve this problem carefully and provide a clear answer.
Show your reasoning step by step."""'''

        return f"""# === PROMPT_CUSTOM START ===
TASK_PROMPT = {task_prompt}
# === PROMPT_CUSTOM END ===

import workspace.{problem_type}.workflows.template.operator as operator
from scripts.async_llm import create_llm_instance
from scripts.evaluator import DatasetType

class Workflow:
    def __init__(self, name: str, llm_config, dataset: DatasetType):
        self.name = name
        self.dataset = dataset
        self.llm = create_llm_instance(llm_config)
        self.custom = operator.Custom(self.llm)

    async def __call__(self, problem: str, entry_point: str = "solve"):
        # entry_point used for code problems with Test operator
        solution = await self.custom(input=problem, instruction=TASK_PROMPT)
        return solution['response'], self.llm.get_usage_summary()["total_cost"]
"""

    async def generate_workflows_batch(
        self,
        problems: List[str],
        problem_types: List[str],
        temperatures: List[float],
        custom_prompts: Optional[List[str]] = None
    ) -> List[Dict]:
        """
        æ‰¹é‡å¹¶å‘ç”Ÿæˆå·¥ä½œæµï¼ˆä¼˜åŒ–ç‰ˆï¼šä½¿ç”¨GPU batchæ¨ç†ï¼‰

        Args:
            problems: é—®é¢˜åˆ—è¡¨
            problem_types: é—®é¢˜ç±»å‹åˆ—è¡¨
            temperatures: æ¸©åº¦åˆ—è¡¨
            custom_prompts: è‡ªå®šä¹‰æç¤ºè¯åˆ—è¡¨

        Returns:
            ç»“æœåˆ—è¡¨
        """
        if self.use_vllm_api:
            # vLLM APIæ¨¡å¼ï¼šå¹¶å‘è°ƒç”¨
            tasks = []
            for i in range(len(problems)):
                task = self.generate_workflow(
                    problem=problems[i],
                    problem_type=problem_types[i],
                    temperature=temperatures[i],
                    custom_prompt=custom_prompts[i] if custom_prompts else None
                )
                tasks.append(task)

            results = await asyncio.gather(*tasks, return_exceptions=True)

            processed_results = []
            for result in results:
                if isinstance(result, Exception):
                    processed_results.append({
                        "workflow_code": "",
                        "valid": False,
                        "error": str(result),
                        "metadata": {}
                    })
                else:
                    processed_results.append(result)

            return processed_results
        else:
            # Transformersæ¨¡å¼ï¼šä½¿ç”¨GPU batchæ¨ç†ï¼ˆå…³é”®ä¼˜åŒ–ï¼ï¼‰
            return await self._batch_generate_with_transformers(
                problems, problem_types, temperatures, custom_prompts
            )

    async def _batch_generate_with_transformers(
        self,
        problems: List[str],
        problem_types: List[str],
        temperatures: List[float],
        custom_prompts: Optional[List[str]]
    ) -> List[Dict]:
        """ä½¿ç”¨transformersæ‰¹é‡ç”Ÿæˆï¼ˆGPU batchæ¨ç†ï¼Œæ”¯æŒåˆ†æ‰¹ä»¥é™ä½æ˜¾å­˜ï¼‰"""
        loop = asyncio.get_event_loop()

        # ğŸ”§ æ˜¾å­˜ä¼˜åŒ–ï¼šåˆ†æ‰¹ç”Ÿæˆï¼Œæ¯æ‰¹æœ€å¤š8ä¸ªåºåˆ—
        MAX_BATCH_SIZE = 8  # æ¯æ‰¹æœ€å¤š8ä¸ªï¼Œé™ä½æ˜¾å­˜å³°å€¼

        def _sync_batch_generate(batch_prompts, batch_temp):
            """åŒæ­¥æ‰¹é‡ç”Ÿæˆå‡½æ•°ï¼ˆå•æ‰¹ï¼‰"""
            # æ‰¹é‡tokenizeï¼ˆå…³é”®ï¼špaddingå¯¹é½ï¼‰
            inputs = self.tokenizer(
                batch_prompts,
                return_tensors="pt",
                padding=True,  # å¯¹é½åˆ°æœ€é•¿åºåˆ—
                truncation=True,
                max_length=3072
            ).to(self.device)

            # æ‰¹é‡ç”Ÿæˆ
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=self.config.get('max_new_tokens', 2048),
                    temperature=batch_temp,
                    top_p=self.config.get('top_p', 0.95),
                    top_k=self.config.get('top_k', 50),
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id
                )

            # æ‰¹é‡è§£ç 
            generated_texts = self.tokenizer.batch_decode(
                outputs[:, inputs['input_ids'].shape[1]:],
                skip_special_tokens=True
            )

            # ğŸ”§ æ˜¾å­˜ä¼˜åŒ–ï¼šåŠæ—¶æ¸…ç†
            del inputs, outputs
            torch.cuda.empty_cache()

            return generated_texts

        try:
            # æ„å»ºæ‰€æœ‰prompts
            all_prompts = []
            for i in range(len(problems)):
                if custom_prompts and custom_prompts[i]:
                    prompt = custom_prompts[i]
                else:
                    prompt = self._build_generation_prompt(problems[i], problem_types[i])
                all_prompts.append(prompt)

            # ğŸ”§ åˆ†æ‰¹å¤„ç†ä»¥é™ä½æ˜¾å­˜å³°å€¼
            all_generated_texts = []
            for batch_start in range(0, len(all_prompts), MAX_BATCH_SIZE):
                batch_end = min(batch_start + MAX_BATCH_SIZE, len(all_prompts))
                batch_prompts = all_prompts[batch_start:batch_end]
                batch_temp = temperatures[batch_start]  # å‡è®¾åŒæ‰¹temperatureç›¸åŒ

                print(f"  ğŸ”§ ç”Ÿæˆæ‰¹æ¬¡ {batch_start//MAX_BATCH_SIZE + 1}/{(len(all_prompts)-1)//MAX_BATCH_SIZE + 1} ({len(batch_prompts)}ä¸ªåºåˆ—)")

                # åœ¨çº¿ç¨‹æ± æ‰§è¡Œå•æ‰¹æ¨ç†
                batch_texts = await loop.run_in_executor(
                    None, _sync_batch_generate, batch_prompts, batch_temp
                )
                all_generated_texts.extend(batch_texts)

            # è§£ææ‰€æœ‰ç»“æœ
            results = []
            for i, generated_text in enumerate(all_generated_texts):
                workflow_code, is_valid, error = self._parse_workflow_code(
                    generated_text, problem_types[i]
                )
                results.append({
                    "workflow_code": workflow_code,
                    "valid": is_valid,
                    "error": error,
                    "metadata": {
                        "problem": problems[i],
                        "problem_type": problem_types[i],
                        "temperature": temperatures[i]
                    }
                })

            return results

        except Exception as e:
            # å‡ºé”™æ—¶è¿”å›ç©ºç»“æœ
            return [{
                "workflow_code": "",
                "valid": False,
                "error": str(e),
                "metadata": {}
            } for _ in problems]
