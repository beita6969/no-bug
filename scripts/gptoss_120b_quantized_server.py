#!/usr/bin/env python3
"""
GPT-OSS-120B é‡åŒ–æœåŠ¡å™¨ - OpenAI APIå…¼å®¹
ä½¿ç”¨bitsandbytes 4-bité‡åŒ–ï¼Œæ˜¾å­˜å ç”¨~15-20GB

ç‰¹æ€§:
- 4-bité‡åŒ– (61GB â†’ ~15-20GB)
- OpenAI APIå…¼å®¹
- GPUåŠ é€Ÿ
- é¢„æœŸé€Ÿåº¦: 30-40 tokens/s (MoEæ¶æ„ï¼Œåªæ¿€æ´»5.1Bå‚æ•°)

ç”¨æ³•:
    python gptoss_120b_quantized_server.py --port 8000 --gpu 0
"""

import argparse
import time
import uuid
from typing import List, Optional, Dict, Any
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import uvicorn

# APIæ¨¡å‹å®šä¹‰ï¼ˆOpenAIå…¼å®¹ï¼‰
class Message(BaseModel):
    role: str
    content: str

class ChatCompletionRequest(BaseModel):
    model: str = "gpt-oss-120b"
    messages: List[Message]
    temperature: float = 0.7
    max_tokens: int = 2048
    top_p: float = 1.0
    n: int = 1
    stream: bool = False

class ChatCompletionChoice(BaseModel):
    index: int
    message: Message
    finish_reason: str

class Usage(BaseModel):
    prompt_tokens: int
    completion_tokens: int
    total_tokens: int

class ChatCompletionResponse(BaseModel):
    id: str
    object: str = "chat.completion"
    created: int
    model: str
    choices: List[ChatCompletionChoice]
    usage: Usage

class ModelInfo(BaseModel):
    id: str
    object: str = "model"
    created: int = 1699000000
    owned_by: str = "openai"

class ModelsResponse(BaseModel):
    object: str = "list"
    data: List[ModelInfo]


class GPTOSS120BServer:
    """GPT-OSS-120B 4-bité‡åŒ–æœåŠ¡å™¨"""

    def __init__(
        self,
        model_path: str,
        device: str = "cuda:0",
        max_length: int = 4096,
        quantization_bits: int = 4
    ):
        """
        åˆå§‹åŒ–GPT-OSS-120Bé‡åŒ–æœåŠ¡å™¨

        Args:
            model_path: æ¨¡å‹è·¯å¾„
            device: è®¾å¤‡
            max_length: æœ€å¤§åºåˆ—é•¿åº¦
            quantization_bits: é‡åŒ–ä½æ•° (4 or 8)
        """
        self.model_path = model_path
        self.device = device
        self.max_length = max_length

        print(f"ğŸš€ åˆå§‹åŒ–GPT-OSS-120Bé‡åŒ–æœåŠ¡å™¨...")
        print(f"  æ¨¡å‹: {model_path}")
        print(f"  é‡åŒ–: {quantization_bits}-bit (bitsandbytes)")
        print(f"  è®¾å¤‡: {device}")
        print(f"  é¢„æœŸæ˜¾å­˜: ~15-20GB (vs åŸå§‹61GB)")
        print(f"  é¢„æœŸé€Ÿåº¦: 30-40 tokens/s (MoE 5.1B active)")

        # é…ç½®4-bité‡åŒ–
        if quantization_bits == 4:
            quantization_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_compute_dtype=torch.bfloat16,
                bnb_4bit_use_double_quant=True,  # åŒé‡é‡åŒ–ï¼Œè¿›ä¸€æ­¥èŠ‚çœå†…å­˜
                bnb_4bit_quant_type="nf4"  # NF4é‡åŒ–ç±»å‹ï¼ˆæ¨èï¼‰
            )
        elif quantization_bits == 8:
            quantization_config = BitsAndBytesConfig(
                load_in_8bit=True,
                llm_int8_threshold=6.0
            )
        else:
            raise ValueError(f"Unsupported quantization_bits: {quantization_bits}")

        # åŠ è½½tokenizer (ä½¿ç”¨slow tokenizeré¿å…fast tokenizeré”™è¯¯)
        print("ğŸ“¥ åŠ è½½tokenizer (slow tokenizer)...")
        self.tokenizer = AutoTokenizer.from_pretrained(
            model_path,
            trust_remote_code=True,
            use_fast=False  # ä½¿ç”¨slow tokenizeré¿å…fast tokenizeræŸåé—®é¢˜
        )

        # åŠ è½½é‡åŒ–æ¨¡å‹
        print(f"ğŸ“¥ åŠ è½½{quantization_bits}-bité‡åŒ–æ¨¡å‹ (è¿™å¯èƒ½éœ€è¦2-3åˆ†é’Ÿ)...")
        start_time = time.time()

        self.model = AutoModelForCausalLM.from_pretrained(
            model_path,
            quantization_config=quantization_config,
            device_map=device,
            trust_remote_code=True,
            torch_dtype=torch.bfloat16,
        )
        self.model.eval()

        load_time = time.time() - start_time

        print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ ({load_time:.1f}ç§’)")
        print(f"  æ¶æ„: Mixture-of-Experts (117B total, 5.1B active)")
        print(f"  é‡åŒ–: {quantization_bits}-bit NF4")

        # è·å–å®é™…æ˜¾å­˜å ç”¨
        if torch.cuda.is_available():
            memory_allocated = torch.cuda.memory_allocated(device) / 1e9
            print(f"  å®é™…æ˜¾å­˜: {memory_allocated:.2f}GB")

    def format_messages(self, messages: List[Message]) -> str:
        """
        æ ¼å¼åŒ–æ¶ˆæ¯ä¸ºGPT-OSSæ ¼å¼ (Harmony response format)
        """
        formatted = ""
        for msg in messages:
            role = msg.role
            content = msg.content
            if role == "system":
                formatted += f"<|im_start|>system\n{content}<|im_end|>\n"
            elif role == "user":
                formatted += f"<|im_start|>user\n{content}<|im_end|>\n"
            elif role == "assistant":
                formatted += f"<|im_start|>assistant\n{content}<|im_end|>\n"

        # æ·»åŠ assistantå¼€å§‹æ ‡è®°
        formatted += "<|im_start|>assistant\n"
        return formatted

    @torch.inference_mode()
    def generate(
        self,
        messages: List[Message],
        temperature: float = 0.7,
        max_tokens: int = 2048,
        top_p: float = 1.0
    ) -> Dict[str, Any]:
        """ç”Ÿæˆå›å¤"""
        # æ ¼å¼åŒ–è¾“å…¥
        prompt = self.format_messages(messages)

        # Tokenize
        inputs = self.tokenizer(
            prompt,
            return_tensors="pt",
            truncation=True,
            max_length=self.max_length
        ).to(self.device)

        prompt_tokens = inputs.input_ids.shape[1]

        # ç”Ÿæˆé…ç½®
        gen_config = {
            "max_new_tokens": max_tokens,
            "temperature": temperature,
            "top_p": top_p,
            "do_sample": temperature > 0,
            "pad_token_id": self.tokenizer.pad_token_id or self.tokenizer.eos_token_id,
            "eos_token_id": self.tokenizer.eos_token_id,
        }

        # ç”Ÿæˆ
        start_time = time.time()
        outputs = self.model.generate(
            **inputs,
            **gen_config
        )
        generation_time = time.time() - start_time

        # è§£ç 
        generated_ids = outputs[0][prompt_tokens:]
        generated_text = self.tokenizer.decode(
            generated_ids,
            skip_special_tokens=True
        )

        completion_tokens = len(generated_ids)
        total_tokens = prompt_tokens + completion_tokens
        tokens_per_sec = completion_tokens / generation_time if generation_time > 0 else 0

        print(f"  ç”Ÿæˆ: {completion_tokens} tokens @ {tokens_per_sec:.1f} tok/s")

        return {
            "text": generated_text,
            "prompt_tokens": prompt_tokens,
            "completion_tokens": completion_tokens,
            "total_tokens": total_tokens,
            "generation_time": generation_time,
            "tokens_per_sec": tokens_per_sec
        }


# å…¨å±€æœåŠ¡å™¨å®ä¾‹
server: Optional[GPTOSS120BServer] = None

# FastAPIåº”ç”¨
app = FastAPI(
    title="GPT-OSS-120B Quantized Server",
    description="OpenAI APIå…¼å®¹çš„GPT-OSS-120B 4-bité‡åŒ–æœåŠ¡å™¨",
    version="1.0.0"
)


@app.on_event("startup")
async def startup_event():
    """å¯åŠ¨æ—¶åˆå§‹åŒ–æ¨¡å‹"""
    global server

    model_path = app.state.model_path
    device = app.state.device
    quantization_bits = app.state.quantization_bits

    server = GPTOSS120BServer(
        model_path=model_path,
        device=device,
        max_length=4096,
        quantization_bits=quantization_bits
    )

    print("\n" + "="*60)
    print("âœ… GPT-OSS-120Bé‡åŒ–æœåŠ¡å™¨å°±ç»ª")
    print("="*60)
    print(f"  Base URL: http://127.0.0.1:{app.state.port}/v1")
    print(f"  å¥åº·æ£€æŸ¥: http://127.0.0.1:{app.state.port}/health")
    print(f"  æ¨¡å‹: GPT-OSS-120B (117B params, 5.1B active)")
    print(f"  é‡åŒ–: {quantization_bits}-bit (bitsandbytes)")
    print(f"  é¢„æœŸæ€§èƒ½: 30-40 tokens/s")
    print("="*60 + "\n")


@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    return {"status": "ok", "model_loaded": server is not None}


@app.get("/v1/models")
async def list_models():
    """åˆ—å‡ºå¯ç”¨æ¨¡å‹"""
    return ModelsResponse(
        data=[
            ModelInfo(
                id="gpt-oss-120b",
                owned_by="openai"
            ),
            ModelInfo(
                id="gpt-4o-mini",  # å…¼å®¹æ—§é…ç½®
                owned_by="openai"
            )
        ]
    )


@app.post("/v1/chat/completions")
async def create_chat_completion(request: ChatCompletionRequest) -> ChatCompletionResponse:
    """åˆ›å»ºèŠå¤©è¡¥å…¨ï¼ˆOpenAI APIå…¼å®¹ï¼‰"""
    if server is None:
        raise HTTPException(status_code=503, detail="Model not loaded")

    try:
        result = server.generate(
            messages=request.messages,
            temperature=request.temperature,
            max_tokens=request.max_tokens,
            top_p=request.top_p
        )

        response = ChatCompletionResponse(
            id=f"chatcmpl-{uuid.uuid4().hex[:8]}",
            created=int(time.time()),
            model=request.model,
            choices=[
                ChatCompletionChoice(
                    index=0,
                    message=Message(
                        role="assistant",
                        content=result["text"]
                    ),
                    finish_reason="stop"
                )
            ],
            usage=Usage(
                prompt_tokens=result["prompt_tokens"],
                completion_tokens=result["completion_tokens"],
                total_tokens=result["total_tokens"]
            )
        )

        return response

    except Exception as e:
        print(f"âŒ ç”Ÿæˆå¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="GPT-OSS-120Bé‡åŒ–æœåŠ¡å™¨")
    parser.add_argument(
        "--model",
        type=str,
        default="/home/yijia/lhy/openai/gpt-oss-120b",
        help="æ¨¡å‹è·¯å¾„"
    )
    parser.add_argument(
        "--gpu",
        type=int,
        default=0,
        help="ä½¿ç”¨çš„GPUç¼–å·ï¼ˆé€»è¾‘ç¼–å·ï¼‰"
    )
    parser.add_argument(
        "--quantization-bits",
        type=int,
        default=4,
        choices=[4, 8],
        help="é‡åŒ–ä½æ•° (4 or 8)"
    )
    parser.add_argument(
        "--port",
        type=int,
        default=8000,
        help="æœåŠ¡å™¨ç«¯å£"
    )
    parser.add_argument(
        "--host",
        type=str,
        default="127.0.0.1",
        help="æœåŠ¡å™¨host"
    )

    args = parser.parse_args()

    device = f"cuda:{args.gpu}"

    # å­˜å‚¨é…ç½®åˆ°app state
    app.state.model_path = args.model
    app.state.device = device
    app.state.quantization_bits = args.quantization_bits
    app.state.port = args.port

    # å¯åŠ¨æœåŠ¡å™¨
    uvicorn.run(
        app,
        host=args.host,
        port=args.port,
        log_level="info"
    )


if __name__ == "__main__":
    main()
