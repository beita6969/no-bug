#!/usr/bin/env python3
"""
è½»é‡çº§æœ¬åœ°LLMæœåŠ¡å™¨ - OpenAI APIå…¼å®¹
ä½¿ç”¨HuggingFace transformersç›´æ¥æ¨ç†ï¼Œæ›¿ä»£gpt-4o-miniåŠ é€Ÿè®­ç»ƒ

ç‰¹æ€§:
- OpenAI APIå…¼å®¹æ¥å£
- Flash Attention 2åŠ é€Ÿï¼ˆå¦‚æœå¯ç”¨ï¼‰
- æ‰¹å¤„ç†ä¼˜åŒ–
- é›¶ç½‘ç»œå»¶è¿Ÿ

ç”¨æ³•:
    python local_llm_server.py --port 8000 --gpu 2
"""

import argparse
import asyncio
import time
import uuid
from typing import List, Optional, Dict, Any
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import uvicorn

# APIæ¨¡å‹å®šä¹‰ï¼ˆOpenAIå…¼å®¹ï¼‰
class Message(BaseModel):
    role: str
    content: str

class ChatCompletionRequest(BaseModel):
    model: str = "qwen2.5-7b-local"
    messages: List[Message]
    temperature: float = 0.7
    max_tokens: int = 2048
    top_p: float = 1.0
    n: int = 1
    stream: bool = False

class ChatCompletionChoice(BaseModel):
    index: int
    message: Message
    finish_reason: str

class Usage(BaseModel):
    prompt_tokens: int
    completion_tokens: int
    total_tokens: int

class ChatCompletionResponse(BaseModel):
    id: str
    object: str = "chat.completion"
    created: int
    model: str
    choices: List[ChatCompletionChoice]
    usage: Usage

class ModelInfo(BaseModel):
    id: str
    object: str = "model"
    created: int = 1699000000
    owned_by: str = "local"

class ModelsResponse(BaseModel):
    object: str = "list"
    data: List[ModelInfo]


class LocalLLMServer:
    """æœ¬åœ°LLMæœåŠ¡å™¨"""

    def __init__(
        self,
        model_path: str,
        device: str = "cuda:0",
        max_length: int = 4096,
        use_flash_attention: bool = True
    ):
        """
        åˆå§‹åŒ–æœ¬åœ°LLMæœåŠ¡å™¨

        Args:
            model_path: æ¨¡å‹è·¯å¾„
            device: è®¾å¤‡ï¼ˆcuda:0, cuda:1ç­‰ï¼‰
            max_length: æœ€å¤§åºåˆ—é•¿åº¦
            use_flash_attention: æ˜¯å¦ä½¿ç”¨Flash Attention 2
        """
        self.model_path = model_path
        self.device = device
        self.max_length = max_length

        print(f"ğŸš€ åˆå§‹åŒ–æœ¬åœ°LLMæœåŠ¡å™¨...")
        print(f"  æ¨¡å‹: {model_path}")
        print(f"  è®¾å¤‡: {device}")
        print(f"  æœ€å¤§é•¿åº¦: {max_length}")

        # åŠ è½½tokenizer
        print("ğŸ“¥ åŠ è½½tokenizer...")
        self.tokenizer = AutoTokenizer.from_pretrained(
            model_path,
            trust_remote_code=True
        )

        # æ¨¡å‹åŠ è½½é…ç½®
        model_kwargs = {
            "torch_dtype": torch.bfloat16,
            "device_map": device,
            "trust_remote_code": True,
        }

        # å°è¯•ä½¿ç”¨Flash Attention 2
        if use_flash_attention:
            try:
                model_kwargs["attn_implementation"] = "flash_attention_2"
                print("  å°è¯•å¯ç”¨Flash Attention 2...")
            except Exception as e:
                print(f"  Flash Attention 2ä¸å¯ç”¨: {e}")
                print("  ä½¿ç”¨æ ‡å‡†attention")

        # åŠ è½½æ¨¡å‹
        print("ğŸ“¥ åŠ è½½æ¨¡å‹...")
        self.model = AutoModelForCausalLM.from_pretrained(
            model_path,
            **model_kwargs
        )
        self.model.eval()  # æ¨ç†æ¨¡å¼

        print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
        print(f"  å‚æ•°é‡: {self.model.num_parameters() / 1e9:.2f}B")
        print(f"  å†…å­˜å ç”¨: ~{self.model.get_memory_footprint() / 1e9:.2f}GB")

    def format_messages(self, messages: List[Message]) -> str:
        """
        æ ¼å¼åŒ–æ¶ˆæ¯ä¸ºQwen2.5æ ¼å¼

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨

        Returns:
            æ ¼å¼åŒ–åçš„æç¤ºè¯
        """
        # Qwen2.5ä½¿ç”¨ChatMLæ ¼å¼
        formatted = ""
        for msg in messages:
            role = msg.role
            content = msg.content
            if role == "system":
                formatted += f"<|im_start|>system\n{content}<|im_end|>\n"
            elif role == "user":
                formatted += f"<|im_start|>user\n{content}<|im_end|>\n"
            elif role == "assistant":
                formatted += f"<|im_start|>assistant\n{content}<|im_end|>\n"

        # æ·»åŠ assistantå¼€å§‹æ ‡è®°
        formatted += "<|im_start|>assistant\n"
        return formatted

    @torch.inference_mode()
    def generate(
        self,
        messages: List[Message],
        temperature: float = 0.7,
        max_tokens: int = 2048,
        top_p: float = 1.0
    ) -> Dict[str, Any]:
        """
        ç”Ÿæˆå›å¤

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
            top_p: nucleus samplingå‚æ•°

        Returns:
            åŒ…å«ç”Ÿæˆæ–‡æœ¬å’Œtokenç»Ÿè®¡çš„å­—å…¸
        """
        # æ ¼å¼åŒ–è¾“å…¥
        prompt = self.format_messages(messages)

        # Tokenize
        inputs = self.tokenizer(
            prompt,
            return_tensors="pt",
            truncation=True,
            max_length=self.max_length
        ).to(self.device)

        prompt_tokens = inputs.input_ids.shape[1]

        # ç”Ÿæˆé…ç½®
        gen_config = {
            "max_new_tokens": max_tokens,
            "temperature": temperature,
            "top_p": top_p,
            "do_sample": temperature > 0,
            "pad_token_id": self.tokenizer.pad_token_id or self.tokenizer.eos_token_id,
            "eos_token_id": self.tokenizer.eos_token_id,
        }

        # ç”Ÿæˆ
        start_time = time.time()
        outputs = self.model.generate(
            **inputs,
            **gen_config
        )
        generation_time = time.time() - start_time

        # è§£ç 
        generated_ids = outputs[0][prompt_tokens:]  # åªä¿ç•™æ–°ç”Ÿæˆçš„éƒ¨åˆ†
        generated_text = self.tokenizer.decode(
            generated_ids,
            skip_special_tokens=True
        )

        completion_tokens = len(generated_ids)
        total_tokens = prompt_tokens + completion_tokens

        # è®¡ç®—é€Ÿåº¦
        tokens_per_sec = completion_tokens / generation_time if generation_time > 0 else 0

        print(f"  ç”Ÿæˆ: {completion_tokens} tokens @ {tokens_per_sec:.1f} tok/s")

        return {
            "text": generated_text,
            "prompt_tokens": prompt_tokens,
            "completion_tokens": completion_tokens,
            "total_tokens": total_tokens,
            "generation_time": generation_time
        }


# å…¨å±€æœåŠ¡å™¨å®ä¾‹
server: Optional[LocalLLMServer] = None


# FastAPIåº”ç”¨
app = FastAPI(
    title="Local LLM Server",
    description="OpenAI APIå…¼å®¹çš„æœ¬åœ°LLMæœåŠ¡å™¨",
    version="1.0.0"
)


@app.on_event("startup")
async def startup_event():
    """å¯åŠ¨æ—¶åˆå§‹åŒ–æ¨¡å‹"""
    global server

    # ä»å‘½ä»¤è¡Œå‚æ•°è·å–é…ç½®
    model_path = app.state.model_path
    device = app.state.device

    server = LocalLLMServer(
        model_path=model_path,
        device=device,
        max_length=4096,
        use_flash_attention=True
    )

    print("\n" + "="*60)
    print("âœ… æœ¬åœ°LLMæœåŠ¡å™¨å°±ç»ª")
    print("="*60)
    print(f"  Base URL: http://127.0.0.1:{app.state.port}/v1")
    print(f"  å¥åº·æ£€æŸ¥: http://127.0.0.1:{app.state.port}/health")
    print("="*60 + "\n")


@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    return {"status": "ok", "model_loaded": server is not None}


@app.get("/v1/models")
async def list_models():
    """åˆ—å‡ºå¯ç”¨æ¨¡å‹"""
    return ModelsResponse(
        data=[
            ModelInfo(
                id="qwen2.5-7b-local",
                owned_by="local"
            ),
            ModelInfo(
                id="gpt-4o-mini",  # å…¼å®¹æ—§é…ç½®
                owned_by="local"
            )
        ]
    )


@app.post("/v1/chat/completions")
async def create_chat_completion(request: ChatCompletionRequest) -> ChatCompletionResponse:
    """åˆ›å»ºèŠå¤©è¡¥å…¨ï¼ˆOpenAI APIå…¼å®¹ï¼‰"""
    if server is None:
        raise HTTPException(status_code=503, detail="Model not loaded")

    try:
        # ç”Ÿæˆå›å¤
        result = server.generate(
            messages=request.messages,
            temperature=request.temperature,
            max_tokens=request.max_tokens,
            top_p=request.top_p
        )

        # æ„é€ OpenAIå…¼å®¹å“åº”
        response = ChatCompletionResponse(
            id=f"chatcmpl-{uuid.uuid4().hex[:8]}",
            created=int(time.time()),
            model=request.model,
            choices=[
                ChatCompletionChoice(
                    index=0,
                    message=Message(
                        role="assistant",
                        content=result["text"]
                    ),
                    finish_reason="stop"
                )
            ],
            usage=Usage(
                prompt_tokens=result["prompt_tokens"],
                completion_tokens=result["completion_tokens"],
                total_tokens=result["total_tokens"]
            )
        )

        return response

    except Exception as e:
        print(f"âŒ ç”Ÿæˆå¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="æœ¬åœ°LLMæœåŠ¡å™¨")
    parser.add_argument(
        "--model",
        type=str,
        default="/home/yijia/verl-agent/models/qwen/Qwen2___5-7B-Instruct",
        help="æ¨¡å‹è·¯å¾„"
    )
    parser.add_argument(
        "--gpu",
        type=int,
        default=2,
        help="ä½¿ç”¨çš„GPUç¼–å·"
    )
    parser.add_argument(
        "--port",
        type=int,
        default=8000,
        help="æœåŠ¡å™¨ç«¯å£"
    )
    parser.add_argument(
        "--host",
        type=str,
        default="127.0.0.1",
        help="æœåŠ¡å™¨host"
    )

    args = parser.parse_args()

    # è®¾ç½®CUDAè®¾å¤‡
    device = f"cuda:{args.gpu}"

    # å­˜å‚¨é…ç½®åˆ°app state
    app.state.model_path = args.model
    app.state.device = device
    app.state.port = args.port

    # å¯åŠ¨æœåŠ¡å™¨
    uvicorn.run(
        app,
        host=args.host,
        port=args.port,
        log_level="info"
    )


if __name__ == "__main__":
    main()
