#!/usr/bin/env python3
"""
æµ‹è¯•è¯„ä¼°ç³»ç»Ÿä¿®å¤ - éªŒè¯MATHæ•°æ®é›†å’ŒLLM Judgeä¿®å¤
"""
import sys
import json
sys.path.insert(0, 'src')

from reward_computer import RewardComputer


def test_math_evaluation():
    """
    æµ‹è¯•MATHæ•°æ®é›†è¯„ä¼°ä¿®å¤

    é—®é¢˜ï¼šä¹‹å‰æ¯”è¾ƒçš„æ˜¯å®Œæ•´è§£ç­”æ–‡æœ¬ï¼Œå¯¼è‡´æ­£ç¡®ç­”æ¡ˆè¢«åˆ¤å®šä¸ºé”™è¯¯
    ä¿®å¤ï¼šç°åœ¨æ¯”è¾ƒæ•°å€¼ç­”æ¡ˆ
    """
    print("\n" + "=" * 80)
    print("ğŸ§ª æµ‹è¯• 1: MATHæ•°æ®é›†è¯„ä¼°ä¿®å¤")
    print("=" * 80)

    # ä»å®é™…æ•°æ®é›†åŠ è½½ä¸€ä¸ªMATHæ ·æœ¬
    print("\nğŸ“‚ åŠ è½½MATHæ ·æœ¬...")
    with open('data/mixed/train_mixed_with_math_fixed.jsonl', 'r') as f:
        for line in f:
            sample = json.loads(line)
            if sample.get('source') == 'MATH' and 'answer' in sample:
                break

    print(f"\nğŸ“ æ ·æœ¬ä¿¡æ¯:")
    print(f"  é—®é¢˜: {sample['problem'][:100]}...")
    print(f"  æ•°å€¼ç­”æ¡ˆ (answerå­—æ®µ): {sample.get('answer', 'N/A')}")
    print(f"  å®Œæ•´è§£ç­” (ground_truthå­—æ®µ): {sample.get('ground_truth', 'N/A')[:100]}...")

    # åˆå§‹åŒ–å¥–åŠ±è®¡ç®—å™¨ï¼ˆå¯ç”¨LLM Judgeå’Œè°ƒè¯•æ—¥å¿—ï¼‰
    reward_computer = RewardComputer(
        use_llm_judge=True,
        llm_config={
            "base_url": "http://localhost:8002/v1",
            "api_key": "sk-dummy",
            "model_name": "/home/yijia/lhy/openai/gpt-oss-120b"
        },
        debug_logging=True
    )

    # æµ‹è¯•åœºæ™¯1ï¼šä½¿ç”¨æ­£ç¡®ç­”æ¡ˆï¼ˆæ•°å€¼ï¼‰
    print("\nâœ… åœºæ™¯1: é¢„æµ‹æ­£ç¡®çš„æ•°å€¼ç­”æ¡ˆ")
    correct_answer = sample['answer']
    reward_correct = reward_computer.compute_reward(
        problem=sample['problem'],
        prediction=correct_answer,  # æ¨¡æ‹Ÿæ¨¡å‹è¾“å‡ºæ­£ç¡®ç­”æ¡ˆ
        ground_truth=correct_answer,  # âœ… ä¿®å¤ï¼šä½¿ç”¨'answer'å­—æ®µè€Œé'ground_truth'
        problem_type='math'
    )
    print(f"  é¢„æµ‹: {correct_answer}")
    print(f"  çœŸå€¼: {correct_answer}")
    print(f"  å¥–åŠ±: {reward_correct} (æœŸæœ›: 1.0)")
    assert reward_correct == 1.0, f"âŒ å¤±è´¥: æ­£ç¡®ç­”æ¡ˆåº”å¾—1.0å¥–åŠ±ï¼Œå®é™…å¾—åˆ°{reward_correct}"
    print("  âœ… é€šè¿‡: æ­£ç¡®ç­”æ¡ˆè·å¾—æœ€é«˜å¥–åŠ±")

    # æµ‹è¯•åœºæ™¯2ï¼šä½¿ç”¨é”™è¯¯ç­”æ¡ˆ
    print("\nâŒ åœºæ™¯2: é¢„æµ‹é”™è¯¯çš„æ•°å€¼ç­”æ¡ˆ")
    wrong_answer = "123456789"  # æ˜æ˜¾é”™è¯¯çš„ç­”æ¡ˆ
    reward_wrong = reward_computer.compute_reward(
        problem=sample['problem'],
        prediction=wrong_answer,
        ground_truth=correct_answer,
        problem_type='math'
    )
    print(f"  é¢„æµ‹: {wrong_answer}")
    print(f"  çœŸå€¼: {correct_answer}")
    print(f"  å¥–åŠ±: {reward_wrong} (æœŸæœ›: 0.0)")
    assert reward_wrong == 0.0, f"âŒ å¤±è´¥: é”™è¯¯ç­”æ¡ˆåº”å¾—0.0å¥–åŠ±ï¼Œå®é™…å¾—åˆ°{reward_wrong}"
    print("  âœ… é€šè¿‡: é”™è¯¯ç­”æ¡ˆè·å¾—æœ€ä½å¥–åŠ±")

    # æµ‹è¯•åœºæ™¯3ï¼šä½¿ç”¨å®Œæ•´è§£ç­”æ–‡æœ¬ï¼ˆæ—§bugåœºæ™¯ï¼‰
    print("\nâš ï¸  åœºæ™¯3: ä½¿ç”¨å®Œæ•´è§£ç­”æ–‡æœ¬ä½œä¸ºé¢„æµ‹ï¼ˆæ—§bugæ¨¡æ‹Ÿï¼‰")
    full_solution = sample['ground_truth']
    reward_solution = reward_computer.compute_reward(
        problem=sample['problem'],
        prediction=full_solution,
        ground_truth=correct_answer,  # âœ… ä¿®å¤ï¼šä½¿ç”¨'answer'å­—æ®µ
        problem_type='math'
    )
    print(f"  é¢„æµ‹: {full_solution[:100]}...")
    print(f"  çœŸå€¼: {correct_answer}")
    print(f"  å¥–åŠ±: {reward_solution}")
    print(f"  è¯´æ˜: LLM Judgeåº”èƒ½ä»å®Œæ•´è§£ç­”ä¸­æå–ç­”æ¡ˆå¹¶åˆ¤å®šä¸ºæ­£ç¡®")

    print("\n" + "=" * 80)
    print("âœ… MATHæ•°æ®é›†è¯„ä¼°ä¿®å¤æµ‹è¯•é€šè¿‡")
    print("=" * 80)
    return reward_computer


def test_llm_judge_robustness(reward_computer):
    """
    æµ‹è¯•LLM Judgeçš„é²æ£’æ€§å’Œé”™è¯¯å¤„ç†
    """
    print("\n" + "=" * 80)
    print("ğŸ§ª æµ‹è¯• 2: LLM Judgeé²æ£’æ€§")
    print("=" * 80)

    test_cases = [
        {
            "name": "æ•°å­¦ - ä¸åŒæ ¼å¼çš„ç›¸åŒç­”æ¡ˆ",
            "problem": "What is 1/2 as a decimal?",
            "prediction": "0.5",
            "ground_truth": "1/2",
            "expected_correct": True
        },
        {
            "name": "æ•°å­¦ - LaTeXæ ¼å¼",
            "problem": "Solve x^2 = 4",
            "prediction": "\\boxed{2}",
            "ground_truth": "2",
            "expected_correct": True
        },
        {
            "name": "æ•°å­¦ - å¸¦å•ä½çš„ç­”æ¡ˆ",
            "problem": "Calculate the cost",
            "prediction": "$30",
            "ground_truth": "30",
            "expected_correct": True
        },
        {
            "name": "æ–‡æœ¬ - å¤§å°å†™ä¸åŒ",
            "problem": "What is the capital of France?",
            "prediction": "Paris",
            "ground_truth": "paris",
            "expected_correct": True
        },
        {
            "name": "æ–‡æœ¬ - å®Œå…¨ä¸åŒçš„ç­”æ¡ˆ",
            "problem": "What is the capital of France?",
            "prediction": "London",
            "ground_truth": "Paris",
            "expected_correct": False
        }
    ]

    passed = 0
    failed = 0

    for idx, case in enumerate(test_cases, 1):
        print(f"\n--- æµ‹è¯• {idx}/{len(test_cases)}: {case['name']} ---")
        print(f"  é—®é¢˜: {case['problem']}")
        print(f"  é¢„æµ‹: {case['prediction']}")
        print(f"  çœŸå€¼: {case['ground_truth']}")
        print(f"  æœŸæœ›: {'âœ… æ­£ç¡®' if case['expected_correct'] else 'âŒ é”™è¯¯'}")

        reward = reward_computer.compute_reward(
            problem=case['problem'],
            prediction=case['prediction'],
            ground_truth=case['ground_truth'],
            problem_type='math'
        )

        actual_correct = (reward == 1.0)
        print(f"  å®é™…: {'âœ… æ­£ç¡®' if actual_correct else 'âŒ é”™è¯¯'} (å¥–åŠ±={reward})")

        if actual_correct == case['expected_correct']:
            print(f"  âœ… é€šè¿‡")
            passed += 1
        else:
            print(f"  âŒ å¤±è´¥: åˆ¤å†³ä¸ç¬¦åˆæœŸæœ›")
            failed += 1

    print("\n" + "=" * 80)
    print(f"LLM Judgeé²æ£’æ€§æµ‹è¯•ç»“æœ: {passed}/{len(test_cases)} é€šè¿‡")
    if failed > 0:
        print(f"âš ï¸  {failed} ä¸ªæµ‹ï¿½ï¿½ï¿½å¤±è´¥ - å¯èƒ½éœ€è¦è¿›ä¸€æ­¥è°ƒæ•´prompt")
    else:
        print("âœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡")
    print("=" * 80)


def test_eval_stats(reward_computer):
    """
    æµ‹è¯•è¯„ä¼°ç»Ÿè®¡åŠŸèƒ½
    """
    print("\n" + "=" * 80)
    print("ğŸ§ª æµ‹è¯• 3: è¯„ä¼°ç»Ÿè®¡åŠŸèƒ½")
    print("=" * 80)

    # æ‰“å°å½“å‰ç»Ÿè®¡
    reward_computer.print_eval_stats()

    # é‡ç½®ç»Ÿè®¡
    print("\né‡ç½®ç»Ÿè®¡...")
    reward_computer.reset_eval_stats()
    reward_computer.print_eval_stats()

    print("\n" + "=" * 80)
    print("âœ… è¯„ä¼°ç»Ÿè®¡åŠŸèƒ½æµ‹è¯•é€šè¿‡")
    print("=" * 80)


def main():
    """
    ä¸»æµ‹è¯•å‡½æ•°
    """
    print("\n" + "=" * 80)
    print("ğŸš€ è¯„ä¼°ç³»ç»Ÿä¿®å¤æµ‹è¯•å¥—ä»¶")
    print("=" * 80)
    print("\nä¿®å¤å†…å®¹:")
    print("  1. âœ… MATHæ•°æ®é›†: ä½¿ç”¨'answer'å­—æ®µè€Œé'ground_truth'å­—æ®µ")
    print("  2. âœ… LLM Judge: å¢å¼ºè¾“å‡ºè§£æï¼ˆ5ç§æ ¼å¼ï¼‰")
    print("  3. âœ… é”™è¯¯å¤„ç†: æ·»åŠ é‡è¯•æœºåˆ¶å’Œè¯¦ç»†æ—¥å¿—")
    print("  4. âœ… ç»Ÿè®¡åŠŸèƒ½: è¿½è¸ªæˆåŠŸç‡å’Œå¤±è´¥åŸå› ")

    try:
        # æµ‹è¯•1: MATHæ•°æ®é›†è¯„ä¼°ä¿®å¤
        reward_computer = test_math_evaluation()

        # æµ‹è¯•2: LLM Judgeé²æ£’æ€§
        test_llm_judge_robustness(reward_computer)

        # æµ‹è¯•3: è¯„ä¼°ç»Ÿè®¡åŠŸèƒ½
        test_eval_stats(reward_computer)

        print("\n" + "=" * 80)
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•å®Œæˆ")
        print("=" * 80)
        print("\nâœ… è¯„ä¼°ç³»ç»Ÿä¿®å¤éªŒè¯é€šè¿‡ï¼Œå¯ä»¥å¼€å§‹è®­ç»ƒ")
        print("\nå»ºè®®:")
        print("  1. åœ¨config/training.yamlä¸­å¯ç”¨ reward_computer.debug_logging: trueï¼ˆå¦‚éœ€è¯¦ç»†æ—¥å¿—ï¼‰")
        print("  2. è¿è¡Œè®­ç»ƒæ—¶ç›‘æ§å‡†ç¡®ç‡æŒ‡æ ‡")
        print("  3. æ¯50æ­¥æŸ¥çœ‹è¯„ä¼°ç»Ÿè®¡ (print_eval_stats)")

    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
