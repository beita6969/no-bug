#!/usr/bin/env python3
"""
åˆ›å»º10kå¹³è¡¡æ•°æ®é›†
Math: 40% (4000)
Code: 30% (3000)
QA:   30% (3000)
"""
import json
import random
from typing import List, Dict
from collections import Counter

def load_jsonl(file_path: str) -> List[Dict]:
    samples = []
    with open(file_path, 'r') as f:
        for line in f:
            if line.strip():
                samples.append(json.loads(line))
    return samples

def save_jsonl(samples: List[Dict], file_path: str):
    with open(file_path, 'w') as f:
        for sample in samples:
            f.write(json.dumps(sample, ensure_ascii=False) + '\n')

def identify_sub_dataset(sample: Dict) -> str:
    """è¯†åˆ«æ ·æœ¬æ¥æºçš„å­æ•°æ®é›†"""
    ptype = sample.get('problem_type', 'unknown')

    if ptype == 'math':
        if 'category' in sample or 'difficulty' in sample:
            return 'MATH'
        return 'GSM8K'

    elif ptype == 'code':
        return 'HumanEval'

    elif ptype == 'qa':
        if 'type' in sample and 'context' in sample:
            return 'HotpotQA'
        elif 'passage' in sample:
            if 'answer_type' in sample:
                return 'DROP'
            return 'CommonsenseQA'
        return 'Other-QA'

    return 'Unknown'

def create_balanced_10k_dataset(
    input_file: str,
    output_file: str,
    target_total: int = 10000,
    math_ratio: float = 0.40,
    code_ratio: float = 0.30,
    qa_ratio: float = 0.30
):
    print("="*70)
    print("åˆ›å»º10kå¹³è¡¡æ•°æ®é›†")
    print("="*70)

    # åŠ è½½æ•°æ®
    print(f"\nğŸ“‚ åŠ è½½æ•°æ®: {input_file}")
    all_samples = load_jsonl(input_file)
    print(f"âœ… åŠ è½½å®Œæˆ: {len(all_samples):,} ä¸ªæ ·æœ¬")

    # åˆ†ç±»
    math_samples = [s for s in all_samples if s.get('problem_type') == 'math']
    code_samples = [s for s in all_samples if s.get('problem_type') == 'code']
    qa_samples = [s for s in all_samples if s.get('problem_type') == 'qa']

    print(f"\nğŸ“Š å¯ç”¨æ ·æœ¬:")
    print(f"  Math: {len(math_samples):,}")
    print(f"  Code: {len(code_samples):,}")
    print(f"  QA:   {len(qa_samples):,}")

    # è®¡ç®—ç›®æ ‡æ•°é‡
    target_math = int(target_total * math_ratio)
    target_code = int(target_total * code_ratio)
    target_qa = int(target_total * qa_ratio)

    # è°ƒæ•´ä»¥ç¡®ä¿æ€»æ•°ä¸º10000
    diff = target_total - (target_math + target_code + target_qa)
    target_math += diff

    print(f"\nğŸ¯ ç›®æ ‡åˆ†å¸ƒ:")
    print(f"  Math: {target_math} ({target_math/target_total*100:.1f}%)")
    print(f"  Code: {target_code} ({target_code/target_total*100:.1f}%)")
    print(f"  QA:   {target_qa} ({target_qa/target_total*100:.1f}%)")
    print(f"  æ€»è®¡: {target_total}")

    # é‡‡æ ·
    print(f"\nğŸ² éšæœºé‡‡æ ·...")
    random.seed(42)

    selected_math = random.sample(math_samples, min(target_math, len(math_samples)))
    selected_code = random.sample(code_samples, min(target_code, len(code_samples)))
    selected_qa = random.sample(qa_samples, min(target_qa, len(qa_samples)))

    # åˆå¹¶
    final_samples = selected_math + selected_code + selected_qa
    random.shuffle(final_samples)

    print(f"âœ… é‡‡æ ·å®Œæˆ: {len(final_samples)} ä¸ªæ ·æœ¬")

    # åˆ†æå­æ•°æ®é›†ç»„æˆ
    print(f"\nğŸ“Š å­æ•°æ®é›†åˆ†å¸ƒ:")
    sub_datasets = Counter(identify_sub_dataset(s) for s in final_samples)

    for dataset, count in sub_datasets.most_common():
        print(f"  {dataset:20s}: {count:5d} ({count/len(final_samples)*100:5.1f}%)")

    # éªŒè¯ç±»å‹åˆ†å¸ƒ
    types = Counter(s.get('problem_type') for s in final_samples)
    print(f"\nâœ… æœ€ç»ˆç±»å‹åˆ†å¸ƒ:")
    for ptype, count in types.most_common():
        print(f"  {ptype}: {count} ({count/len(final_samples)*100:.1f}%)")

    # ä¿å­˜
    print(f"\nğŸ’¾ ä¿å­˜: {output_file}")
    save_jsonl(final_samples, output_file)

    # ä¿å­˜ç»Ÿè®¡
    stats = {
        'total': len(final_samples),
        'target_total': target_total,
        'math_count': len(selected_math),
        'code_count': len(selected_code),
        'qa_count': len(selected_qa),
        'math_ratio': len(selected_math) / len(final_samples),
        'code_ratio': len(selected_code) / len(final_samples),
        'qa_ratio': len(selected_qa) / len(final_samples),
        'sub_datasets': dict(sub_datasets)
    }

    stats_file = output_file.replace('.jsonl', '_stats.json')
    with open(stats_file, 'w') as f:
        json.dump(stats, f, indent=2)

    print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯: {stats_file}")

    print("\n" + "="*70)
    print("âœ… 10kå¹³è¡¡æ•°æ®é›†åˆ›å»ºå®Œæˆ!")
    print("="*70)

if __name__ == "__main__":
    create_balanced_10k_dataset(
        input_file="data/train/mixed_dataset_augmented_v2.jsonl",
        output_file="data/train/balanced_10k_dataset.jsonl",
        target_total=10000,
        math_ratio=0.40,
        code_ratio=0.30,
        qa_ratio=0.30
    )
