#!/usr/bin/env python3
"""
æ•°æ®é›† LLM æ·±åº¦æ‰«æä¸æ¸…æ´—è„šæœ¬
ç›®çš„ï¼šåˆ©ç”¨æœ¬åœ° LLM (GPT OSS 120B @ 8002) å¯¹è®­ç»ƒé›†è¿›è¡Œ"ä½“æ£€"ï¼Œå‰”é™¤ Ground Truth æ˜æ˜¾é”™è¯¯æˆ–ä¸é—®é¢˜ä¸åŒ¹é…çš„æ ·æœ¬ã€‚

æ‰«æé€»è¾‘ï¼š
1. æ„é€  Promptï¼šè¯·åˆ¤æ–­ Question å’Œ Ground Truth æ˜¯å¦æ„æˆåˆç†çš„é—®ç­”å¯¹ã€‚
2. å…³æ³¨ç‚¹ï¼š
   - Answer æ˜¯å¦æ˜¯ Question çš„æœ‰æ•ˆç­”æ¡ˆï¼Ÿ
   - Answer æ˜¯å¦æ˜æ˜¾é”™è¯¯ï¼ˆå¦‚ Commercial Paper å®šä¹‰ä¸º length and widthï¼‰ï¼Ÿ
   - Context æ˜¯å¦ç¼ºå¤±å¯¼è‡´æ— æ³•å›ç­”ï¼Ÿ
3. è¾“å‡ºï¼šä¿ç•™é«˜è´¨é‡æ ·æœ¬ï¼Œç”Ÿæˆ bad_samples.jsonl ä¾›å®¡æŸ¥ã€‚
"""

import json
import os
import asyncio
from tqdm import tqdm
from openai import OpenAI

# é…ç½®
INPUT_FILE = "11/integrated_aflow_roll/data/ready_to_train/train.jsonl"
OUTPUT_FILE = "11/integrated_aflow_roll/data/ready_to_train/train_clean_llm.jsonl"
BAD_FILE = "11/integrated_aflow_roll/data/ready_to_train/bad_samples.jsonl"
LLM_BASE_URL = "http://localhost:8002/v1"
LLM_API_KEY = "sk-dummy"
MODEL_NAME = "/home/yijia/lhy/openai/gpt-oss-120b"
CONCURRENCY = 20  # å¹¶å‘è¯·æ±‚æ•°

client = OpenAI(base_url=LLM_BASE_URL, api_key=LLM_API_KEY)

async def check_sample(item, semaphore):
    async with semaphore:
        problem = item['problem']
        answer = item['ground_truth']
        p_type = item['problem_type']
        
        # Code ç±»å‹é€šå¸¸æ¯”è¾ƒå¯é ï¼ˆæ¥è‡ª HumanEval/MBPPï¼‰ï¼Œä¸” LLM éš¾ä»¥ä»…å‡­æ–‡æœ¬åˆ¤æ–­ä»£ç æ­£ç¡®æ€§
        # é™¤éç­”æ¡ˆæ˜æ˜¾å¤ªçŸ­æˆ–éä»£ç 
        if p_type == 'code':
            if len(str(answer)) < 10:
                return False, "Code answer too short"
            return True, ""

        # Math/QA ç±»å‹è¿›è¡Œæ·±åº¦æ£€æŸ¥
        prompt = f"""You are a Data Quality Auditor. Your task is to verify if the following Question and Ground Truth Answer form a valid, logical, and self-contained pair.

Question: {problem}
Ground Truth Answer: {answer}

Evaluation Criteria:
1. **Relevance**: Does the answer actually answer the question? (e.g., Q: "Define X", A: "length" -> INVALID)
2. **Self-containment**: Does the question make sense without external context? (e.g., Q: "What did he say?" -> INVALID)
3. **Correctness**: Is the answer factually plausible? (Ignore minor formatting or date discrepancies, focus on logic).

Respond ONLY with a JSON object:
{{
    "valid": true/false,
    "reason": "short explanation"
}}
"""
        try:
            response = await asyncio.to_thread(
                client.chat.completions.create,
                model=MODEL_NAME,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.0,
                max_tokens=100,
                response_format={"type": "json_object"}
            )
            result = json.loads(response.choices[0].message.content)
            return result['valid'], result.get('reason', 'No reason')
        except Exception as e:
            # å¦‚æœ LLM è°ƒç”¨å¤±è´¥æˆ–è§£æå¤±è´¥ï¼Œä¿å®ˆèµ·è§ä¿ç•™æ ·æœ¬ï¼ˆæˆ–è€…æ˜¯ç½‘ç»œæŠ–åŠ¨ï¼‰
            # ä½†ä¸ºäº†æ¸…æ´—å½»åº•ï¼Œæˆ‘ä»¬å¯ä»¥æ ‡è®°ä¸º False å¹¶åœ¨æœ€åäººå·¥å¤æ ¸
            print(f"âš ï¸ API Error: {e}")
            return True, "API Error (Skipped check)"

async def scan_dataset():
    print(f"ğŸš€ å¼€å§‹ LLM æ•°æ®æ·±åº¦æ‰«æ...")
    print(f"  è¾“å…¥: {INPUT_FILE}")
    print(f"  æ¨¡å‹: {MODEL_NAME}")
    
    if not os.path.exists(INPUT_FILE):
        print("âŒ è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨")
        return

    with open(INPUT_FILE, 'r') as f:
        lines = f.readlines()
    
    print(f"  æ€»æ ·æœ¬æ•°: {len(lines)}")
    
    tasks = []
    semaphore = asyncio.Semaphore(CONCURRENCY)
    
    results = []
    
    # åˆ›å»ºä»»åŠ¡
    for i, line in enumerate(lines):
        item = json.loads(line)
        # ä¸ºæ¯è¡Œç»‘å®šåŸå§‹æ•°æ®å’Œæ£€æŸ¥ä»»åŠ¡
        tasks.append((item, check_sample(item, semaphore)))
    
    # æ‰§è¡Œ
    valid_count = 0
    bad_count = 0
    
    with open(OUTPUT_FILE, 'w') as f_out, open(BAD_FILE, 'w') as f_bad:
        # ä½¿ç”¨ tqdm æ˜¾ç¤ºè¿›åº¦
        for item, task in tqdm(tasks, total=len(tasks), desc="Scanning"):
            is_valid, reason = await task
            
            if is_valid:
                valid_count += 1
                f_out.write(json.dumps(item) + "\n")
            else:
                bad_count += 1
                item['drop_reason'] = reason
                f_bad.write(json.dumps(item) + "\n")
                
    print("\nğŸ“Š æ‰«æå®Œæˆ!")
    print(f"  âœ… æœ‰æ•ˆæ ·æœ¬: {valid_count}")
    print(f"  ğŸ—‘ï¸  å‰”é™¤æ ·æœ¬: {bad_count}")
    print(f"  ğŸ’¾ æ¸…æ´—åæ–‡ä»¶: {OUTPUT_FILE}")
    print(f"  ğŸ“ å‰”é™¤è¯¦æƒ…: {BAD_FILE}")

    # æ‰“å°å‡ ä¸ªå‰”é™¤æ¡ˆä¾‹
    if bad_count > 0:
        print("\nğŸ” å‰”é™¤æ¡ˆä¾‹ç¤ºä¾‹:")
        with open(BAD_FILE, 'r') as f:
            for _ in range(min(5, bad_count)):
                bad = json.loads(f.readline())
                print(f"  Q: {bad['problem'][:50]}...")
                print(f"  A: {bad['ground_truth'][:50]}...")
                print(f"  Reason: {bad['drop_reason']}\n")

if __name__ == "__main__":
    asyncio.run(scan_dataset())

