#!/usr/bin/env python3
"""
GPT-OSS-20B æœ¬åœ°æœåŠ¡å™¨ - OpenAI APIå…¼å®¹
ä½¿ç”¨llama-cpp-pythonåŠ è½½GGUFæ ¼å¼æ¨¡å‹

ç‰¹æ€§:
- OpenAI APIå…¼å®¹æ¥å£
- GGUFæ ¼å¼ä¼˜åŒ–åŠ è½½
- GPUåŠ é€Ÿ (CUDA)
- é›¶ç½‘ç»œå»¶è¿Ÿ
- é¢„æœŸé€Ÿåº¦: 25-30 tokens/s (2x faster than Qwen2.5-7B)

ç”¨æ³•:
    python gptoss_20b_server.py --port 8000 --gpu 2
"""

import argparse
import time
import uuid
from typing import List, Optional, Dict, Any
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import uvicorn

# APIæ¨¡å‹å®šä¹‰ï¼ˆOpenAIå…¼å®¹ï¼‰
class Message(BaseModel):
    role: str
    content: str

class ChatCompletionRequest(BaseModel):
    model: str = "gpt-oss-20b"
    messages: List[Message]
    temperature: float = 0.7
    max_tokens: int = 2048
    top_p: float = 1.0
    n: int = 1
    stream: bool = False

class ChatCompletionChoice(BaseModel):
    index: int
    message: Message
    finish_reason: str

class Usage(BaseModel):
    prompt_tokens: int
    completion_tokens: int
    total_tokens: int

class ChatCompletionResponse(BaseModel):
    id: str
    object: str = "chat.completion"
    created: int
    model: str
    choices: List[ChatCompletionChoice]
    usage: Usage

class ModelInfo(BaseModel):
    id: str
    object: str = "model"
    created: int = 1699000000
    owned_by: str = "openai"

class ModelsResponse(BaseModel):
    object: str = "list"
    data: List[ModelInfo]


class GPTOSSServer:
    """GPT-OSS-20BæœåŠ¡å™¨ (GGUF)"""

    def __init__(
        self,
        model_path: str,
        n_gpu_layers: int = -1,  # -1 = å…¨éƒ¨offloadåˆ°GPU
        n_ctx: int = 4096,
        n_batch: int = 512,
        verbose: bool = False
    ):
        """
        åˆå§‹åŒ–GPT-OSS-20BæœåŠ¡å™¨

        Args:
            model_path: GGUFæ¨¡å‹æ–‡ä»¶è·¯å¾„
            n_gpu_layers: GPUå±‚æ•° (-1è¡¨ç¤ºå…¨éƒ¨)
            n_ctx: ä¸Šä¸‹æ–‡é•¿åº¦
            n_batch: æ‰¹å¤„ç†å¤§å°
            verbose: æ˜¯å¦è¯¦ç»†æ—¥å¿—
        """
        self.model_path = model_path

        print(f"ğŸš€ åˆå§‹åŒ–GPT-OSS-20BæœåŠ¡å™¨...")
        print(f"  æ¨¡å‹: {model_path}")
        print(f"  GPUå±‚æ•°: {n_gpu_layers} (å…¨éƒ¨offloadåˆ°GPU)")
        print(f"  ä¸Šä¸‹æ–‡é•¿åº¦: {n_ctx}")
        print(f"  æ‰¹å¤„ç†: {n_batch}")

        try:
            from llama_cpp import Llama
        except ImportError:
            raise ImportError(
                "éœ€è¦å®‰è£… llama-cpp-python:\n"
                "CMAKE_ARGS=\"-DGGML_CUDA=on\" pip install llama-cpp-python"
            )

        # åŠ è½½GGUFæ¨¡å‹
        print("ğŸ“¥ åŠ è½½GGUFæ¨¡å‹ (GPUåŠ é€Ÿ)...")
        start_time = time.time()

        self.model = Llama(
            model_path=model_path,
            n_gpu_layers=n_gpu_layers,  # å…¨éƒ¨offloadåˆ°GPU
            n_ctx=n_ctx,
            n_batch=n_batch,
            verbose=verbose,
            logits_all=False,  # èŠ‚çœå†…å­˜
            use_mmap=True,  # ä½¿ç”¨å†…å­˜æ˜ å°„
            use_mlock=False,  # ä¸é”å®šå†…å­˜ï¼ˆå…è®¸swapï¼‰
        )

        load_time = time.time() - start_time

        print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ ({load_time:.1f}ç§’)")
        print(f"  æ¶æ„: Mixture-of-Experts (21B total, 3.6B active)")
        print(f"  é‡åŒ–: MXFP4 (~12GB)")
        print(f"  é¢„æœŸé€Ÿåº¦: 25-30 tokens/s")

    def format_messages(self, messages: List[Message]) -> str:
        """
        æ ¼å¼åŒ–æ¶ˆæ¯ä¸ºGPT-OSSæ ¼å¼ (Harmony response format)

        GPT-OSSä½¿ç”¨ç±»ä¼¼ChatMLçš„æ ¼å¼
        """
        formatted = ""
        for msg in messages:
            role = msg.role
            content = msg.content
            if role == "system":
                formatted += f"<|im_start|>system\n{content}<|im_end|>\n"
            elif role == "user":
                formatted += f"<|im_start|>user\n{content}<|im_end|>\n"
            elif role == "assistant":
                formatted += f"<|im_start|>assistant\n{content}<|im_end|>\n"

        # æ·»åŠ assistantå¼€å§‹æ ‡è®°
        formatted += "<|im_start|>assistant\n"
        return formatted

    def generate(
        self,
        messages: List[Message],
        temperature: float = 0.7,
        max_tokens: int = 2048,
        top_p: float = 1.0
    ) -> Dict[str, Any]:
        """
        ç”Ÿæˆå›å¤

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
            top_p: nucleus samplingå‚æ•°

        Returns:
            åŒ…å«ç”Ÿæˆæ–‡æœ¬å’Œtokenç»Ÿè®¡çš„å­—å…¸
        """
        # æ ¼å¼åŒ–è¾“å…¥
        prompt = self.format_messages(messages)

        # ç”Ÿæˆ
        start_time = time.time()
        output = self.model(
            prompt,
            max_tokens=max_tokens,
            temperature=temperature,
            top_p=top_p,
            echo=False,  # ä¸å›æ˜¾è¾“å…¥
            stop=["<|im_end|>", "<|endoftext|>"],
        )
        generation_time = time.time() - start_time

        # æå–ç»“æœ
        generated_text = output["choices"][0]["text"]
        prompt_tokens = output["usage"]["prompt_tokens"]
        completion_tokens = output["usage"]["completion_tokens"]
        total_tokens = output["usage"]["total_tokens"]

        # è®¡ç®—é€Ÿåº¦
        tokens_per_sec = completion_tokens / generation_time if generation_time > 0 else 0

        print(f"  ç”Ÿæˆ: {completion_tokens} tokens @ {tokens_per_sec:.1f} tok/s")

        return {
            "text": generated_text,
            "prompt_tokens": prompt_tokens,
            "completion_tokens": completion_tokens,
            "total_tokens": total_tokens,
            "generation_time": generation_time,
            "tokens_per_sec": tokens_per_sec
        }


# å…¨å±€æœåŠ¡å™¨å®ä¾‹
server: Optional[GPTOSSServer] = None


# FastAPIåº”ç”¨
app = FastAPI(
    title="GPT-OSS-20B Server",
    description="OpenAI APIå…¼å®¹çš„GPT-OSS-20Bæœ¬åœ°æœåŠ¡å™¨",
    version="1.0.0"
)


@app.on_event("startup")
async def startup_event():
    """å¯åŠ¨æ—¶åˆå§‹åŒ–æ¨¡å‹"""
    global server

    # ä»å‘½ä»¤è¡Œå‚æ•°è·å–é…ç½®
    model_path = app.state.model_path
    n_gpu_layers = app.state.n_gpu_layers

    server = GPTOSSServer(
        model_path=model_path,
        n_gpu_layers=n_gpu_layers,
        n_ctx=4096,
        n_batch=512,
        verbose=False
    )

    print("\n" + "="*60)
    print("âœ… GPT-OSS-20BæœåŠ¡å™¨å°±ç»ª")
    print("="*60)
    print(f"  Base URL: http://127.0.0.1:{app.state.port}/v1")
    print(f"  å¥åº·æ£€æŸ¥: http://127.0.0.1:{app.state.port}/health")
    print(f"  æ¨¡å‹: GPT-OSS-20B (21B params, 3.6B active)")
    print(f"  é¢„æœŸæ€§èƒ½: 25-30 tokens/s (2x faster than Qwen2.5-7B)")
    print("="*60 + "\n")


@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    return {"status": "ok", "model_loaded": server is not None}


@app.get("/v1/models")
async def list_models():
    """åˆ—å‡ºå¯ç”¨æ¨¡å‹"""
    return ModelsResponse(
        data=[
            ModelInfo(
                id="gpt-oss-20b",
                owned_by="openai"
            ),
            ModelInfo(
                id="gpt-4o-mini",  # å…¼å®¹æ—§é…ç½®
                owned_by="openai"
            )
        ]
    )


@app.post("/v1/chat/completions")
async def create_chat_completion(request: ChatCompletionRequest) -> ChatCompletionResponse:
    """åˆ›å»ºèŠå¤©è¡¥å…¨ï¼ˆOpenAI APIå…¼å®¹ï¼‰"""
    if server is None:
        raise HTTPException(status_code=503, detail="Model not loaded")

    try:
        # ç”Ÿæˆå›å¤
        result = server.generate(
            messages=request.messages,
            temperature=request.temperature,
            max_tokens=request.max_tokens,
            top_p=request.top_p
        )

        # æ„é€ OpenAIå…¼å®¹å“åº”
        response = ChatCompletionResponse(
            id=f"chatcmpl-{uuid.uuid4().hex[:8]}",
            created=int(time.time()),
            model=request.model,
            choices=[
                ChatCompletionChoice(
                    index=0,
                    message=Message(
                        role="assistant",
                        content=result["text"]
                    ),
                    finish_reason="stop"
                )
            ],
            usage=Usage(
                prompt_tokens=result["prompt_tokens"],
                completion_tokens=result["completion_tokens"],
                total_tokens=result["total_tokens"]
            )
        )

        return response

    except Exception as e:
        print(f"âŒ ç”Ÿæˆå¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="GPT-OSS-20Bæœ¬åœ°æœåŠ¡å™¨")
    parser.add_argument(
        "--model",
        type=str,
        default="models/gpt-oss-20b-gguf/openai_gpt-oss-20b-MXFP4.gguf",
        help="GGUFæ¨¡å‹æ–‡ä»¶è·¯å¾„"
    )
    parser.add_argument(
        "--gpu-layers",
        type=int,
        default=-1,
        help="GPUå±‚æ•° (-1è¡¨ç¤ºå…¨éƒ¨offloadåˆ°GPU)"
    )
    parser.add_argument(
        "--port",
        type=int,
        default=8000,
        help="æœåŠ¡å™¨ç«¯å£"
    )
    parser.add_argument(
        "--host",
        type=str,
        default="127.0.0.1",
        help="æœåŠ¡å™¨host"
    )

    args = parser.parse_args()

    # å­˜å‚¨é…ç½®åˆ°app state
    app.state.model_path = args.model
    app.state.n_gpu_layers = args.gpu_layers
    app.state.port = args.port

    # å¯åŠ¨æœåŠ¡å™¨
    uvicorn.run(
        app,
        host=args.host,
        port=args.port,
        log_level="info"
    )


if __name__ == "__main__":
    main()
