#!/usr/bin/env python3
"""
æ‰©å±•è®­ç»ƒé›†ï¼šæ·»åŠ HumanEvalå’ŒMBPPï¼Œæ‰©å±•codeåˆ°30%
"""

import json
import random
from pathlib import Path
from collections import defaultdict
import copy

random.seed(42)

# è·¯å¾„
data_dir = Path('/home/yijia/.claude/11/integrated_aflow_roll/data')
mixed_dir = data_dir / 'mixed'
humaneval_dir = data_dir / 'humaneval'

def load_jsonl(file_path):
    """åŠ è½½JSONLæ–‡ä»¶"""
    samples = []
    if file_path.exists():
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    samples.append(json.loads(line))
    return samples

def save_jsonl(samples, file_path):
    """ä¿å­˜JSONLæ–‡ä»¶"""
    with open(file_path, 'w', encoding='utf-8') as f:
        for sample in samples:
            f.write(json.dumps(sample, ensure_ascii=False) + '\n')

def standardize_humaneval(sample):
    """æ ‡å‡†åŒ–HumanEvalæ ·æœ¬æ ¼å¼"""
    standardized = {
        'problem': sample.get('prompt', ''),
        'problem_type': 'code',
        'source': 'humaneval',
        'ground_truth': sample.get('canonical_solution', ''),
    }

    # ä¿ç•™é¢å¤–å­—æ®µ
    if 'entry_point' in sample:
        standardized['entry_point'] = sample['entry_point']
    if 'test' in sample:
        standardized['test'] = sample['test']
    if 'task_id' in sample:
        standardized['task_id'] = sample['task_id']

    return standardized

def main():
    print("="*60)
    print("ğŸ“Š æ‰©å±•è®­ç»ƒé›†ï¼šCodeæ ·æœ¬åˆ°30%")
    print("="*60)

    # 1. åŠ è½½ç°æœ‰è®­ç»ƒé›†
    print("\n1. åŠ è½½ç°æœ‰è®­ç»ƒé›†...")
    train_samples = load_jsonl(mixed_dir / 'train_mixed.jsonl')
    print(f"   åŸå§‹è®­ç»ƒé›†: {len(train_samples)} æ ·æœ¬")

    # ç»Ÿè®¡ç°æœ‰åˆ†å¸ƒ
    type_counts = defaultdict(int)
    source_counts = defaultdict(int)
    existing_code_samples = []
    non_code_samples = []

    for sample in train_samples:
        ptype = sample.get('problem_type', 'unknown')
        source = sample.get('source', 'unknown')
        type_counts[ptype] += 1
        source_counts[source] += 1

        if ptype == 'code':
            existing_code_samples.append(sample)
        else:
            non_code_samples.append(sample)

    print(f"\n   ç°æœ‰åˆ†å¸ƒ:")
    for ptype, count in sorted(type_counts.items()):
        print(f"     {ptype}: {count} ({count/len(train_samples)*100:.1f}%)")

    # 2. åŠ è½½HumanEvalæ•°æ®
    print("\n2. åŠ è½½HumanEvalæ•°æ®...")
    humaneval_samples = []
    humaneval_files = [
        ('humaneval_full.jsonl', 164),
        ('humaneval_validate.jsonl', 132),
    ]

    for filename, expected_count in humaneval_files:
        file_path = humaneval_dir / filename
        samples = load_jsonl(file_path)
        print(f"   {filename}: {len(samples)} æ ·æœ¬")

        for sample in samples:
            humaneval_samples.append(standardize_humaneval(sample))

    print(f"   HumanEvalæ€»è®¡: {len(humaneval_samples)} æ ·æœ¬")

    # 3. æ”¶é›†æ‰€æœ‰codeæ ·æœ¬
    print("\n3. æ•´åˆCodeæ ·æœ¬...")
    all_code_samples = existing_code_samples + humaneval_samples
    print(f"   ç°æœ‰MBPP: {len([s for s in existing_code_samples if s.get('source') == 'mbpp'])} æ ·æœ¬")
    print(f"   æ–°å¢HumanEval: {len(humaneval_samples)} æ ·æœ¬")
    print(f"   Codeæ ·æœ¬æ€»è®¡: {len(all_code_samples)} æ ·æœ¬")

    # å»é‡
    unique_code_samples = []
    seen = set()
    for sample in all_code_samples:
        # ä½¿ç”¨problemä½œä¸ºå»é‡é”®
        key = sample.get('problem', '')[:100]  # å‰100å­—ç¬¦
        if key and key not in seen:
            unique_code_samples.append(sample)
            seen.add(key)

    print(f"   å»é‡å: {len(unique_code_samples)} æ ·æœ¬")

    # 4. è®¡ç®—ç›®æ ‡æ•°é‡ï¼ˆ30% codeï¼‰
    print("\n4. è®¡ç®—ç›®æ ‡åˆ†å¸ƒ...")
    # ä¿æŒnon-codeæ ·æœ¬ä¸å˜ï¼Œæ‰©å±•codeåˆ°30%
    # å¦‚æœcodeå 30%ï¼Œnon-codeå 70%
    # total = non_code / 0.7
    non_code_count = len(non_code_samples)
    target_total = int(non_code_count / 0.7)
    target_code_count = target_total - non_code_count

    print(f"   Non-codeæ ·æœ¬: {non_code_count}")
    print(f"   ç›®æ ‡æ€»æ•°: {target_total}")
    print(f"   ç›®æ ‡Codeæ•°: {target_code_count} (30%)")

    # 5. æ‰©å±•codeæ ·æœ¬
    print("\n5. æ‰©å±•Codeæ ·æœ¬...")
    expanded_code_samples = []

    if len(unique_code_samples) >= target_code_count:
        # å¦‚æœæ ·æœ¬å……è¶³ï¼Œéšæœºé€‰æ‹©
        expanded_code_samples = random.sample(unique_code_samples, target_code_count)
        print(f"   éšæœºé€‰æ‹© {target_code_count} ä¸ªæ ·æœ¬")
    else:
        # éœ€è¦å¤åˆ¶
        duplication_factor = (target_code_count // len(unique_code_samples)) + 1
        print(f"   éœ€è¦å¤åˆ¶ {duplication_factor} å€")

        for i in range(duplication_factor):
            for sample in unique_code_samples:
                sample_copy = copy.deepcopy(sample)
                sample_copy['duplication_id'] = i
                expanded_code_samples.append(sample_copy)

        # éšæœºæ‰“ä¹±å¹¶æˆªå–åˆ°ç›®æ ‡æ•°é‡
        random.shuffle(expanded_code_samples)
        expanded_code_samples = expanded_code_samples[:target_code_count]
        print(f"   æ‰©å±•å: {len(expanded_code_samples)} æ ·æœ¬")

    # 6. åˆå¹¶æ•°æ®é›†
    print("\n6. åˆ›å»ºæ–°è®­ç»ƒé›†...")
    new_train_samples = non_code_samples + expanded_code_samples
    random.shuffle(new_train_samples)

    # 7. ç»Ÿè®¡æœ€ç»ˆåˆ†å¸ƒ
    final_type_counts = defaultdict(int)
    final_source_counts = defaultdict(int)

    for sample in new_train_samples:
        final_type_counts[sample.get('problem_type', 'unknown')] += 1
        final_source_counts[sample.get('source', 'unknown')] += 1

    print(f"\n   æœ€ç»ˆåˆ†å¸ƒ:")
    print(f"   æ€»æ ·æœ¬æ•°: {len(new_train_samples)}")
    print(f"\n   æŒ‰ç±»å‹:")
    for ptype, count in sorted(final_type_counts.items()):
        percentage = count / len(new_train_samples) * 100
        print(f"     {ptype}: {count:,} ({percentage:.1f}%)")

    print(f"\n   æŒ‰æ•°æ®æº:")
    for source, count in sorted(final_source_counts.items(), key=lambda x: x[1], reverse=True):
        percentage = count / len(new_train_samples) * 100
        print(f"     {source}: {count:,} ({percentage:.1f}%)")

    # 8. ä¿å­˜æ–°è®­ç»ƒé›†
    output_file = mixed_dir / 'train_mixed_balanced.jsonl'
    save_jsonl(new_train_samples, output_file)
    print(f"\nâœ… æ–°è®­ç»ƒé›†å·²ä¿å­˜åˆ°: {output_file}")
    print(f"   æ ·æœ¬æ•°: {len(new_train_samples)}")

    # 9. åˆ›å»ºç»Ÿè®¡æ–‡ä»¶
    stats = {
        'total_samples': len(new_train_samples),
        'type_distribution': dict(final_type_counts),
        'source_distribution': dict(final_source_counts),
        'original_code_samples': len(unique_code_samples),
        'expanded_code_samples': len(expanded_code_samples),
    }

    stats_file = mixed_dir / 'train_stats_balanced.json'
    with open(stats_file, 'w', encoding='utf-8') as f:
        json.dump(stats, f, indent=2, ensure_ascii=False)

    print(f"\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯å·²ä¿å­˜åˆ°: {stats_file}")
    print("="*60)
    print("âœ… å®Œæˆï¼")
    print("="*60)

if __name__ == '__main__':
    main()
