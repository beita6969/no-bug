#!/usr/bin/env python3
"""
å®Œæ•´æ•°æ®é›†éªŒè¯è„šæœ¬
"""
import json
from collections import Counter
from pathlib import Path

def check_dataset(file_path: str, dataset_name: str):
    """æ£€æŸ¥å•ä¸ªæ•°æ®é›†"""
    print("="*70)
    print(f"{dataset_name}æ£€æŸ¥: {file_path}")
    print("="*70)

    if not Path(file_path).exists():
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨")
        return None

    # åŠ è½½æ•°æ®
    try:
        with open(file_path) as f:
            samples = [json.loads(line) for line in f if line.strip()]
    except Exception as e:
        print(f"âŒ åŠ è½½å¤±è´¥: {e}")
        return None

    print(f"\nâœ… æ€»æ ·æœ¬æ•°: {len(samples)}")

    # ç±»å‹åˆ†å¸ƒ
    types = Counter(s.get('problem_type', 'unknown') for s in samples)
    print("\nğŸ“Š ç±»å‹åˆ†å¸ƒ:")
    for ptype, count in types.most_common():
        print(f"  {ptype:10s}: {count:7d} ({count/len(samples)*100:5.2f}%)")

    # å­—æ®µå®Œæ•´æ€§æ£€æŸ¥
    print("\nğŸ” å­—æ®µå®Œæ•´æ€§æ£€æŸ¥:")
    missing_counts = {
        'problem': 0,
        'problem_type': 0,
        'ground_truth': 0
    }

    code_missing = {
        'entry_point': 0,
        'test': 0
    }

    code_count = 0
    for s in samples:
        if 'problem' not in s or not s['problem']:
            missing_counts['problem'] += 1
        if 'problem_type' not in s:
            missing_counts['problem_type'] += 1
        if 'ground_truth' not in s:
            missing_counts['ground_truth'] += 1

        if s.get('problem_type') == 'code':
            code_count += 1
            if 'entry_point' not in s or not s['entry_point']:
                code_missing['entry_point'] += 1
            if 'test' not in s or not s['test']:
                code_missing['test'] += 1

    all_good = True
    for field, count in missing_counts.items():
        if count > 0:
            print(f"  âš ï¸  {field}: {count}ä¸ªæ ·æœ¬ç¼ºå¤±")
            all_good = False

    if code_count > 0:
        print(f"\nğŸ” Codeæ ·æœ¬å­—æ®µæ£€æŸ¥ ({code_count}ä¸ª):")
        for field, count in code_missing.items():
            if count > 0:
                print(f"  âš ï¸  {field}: {count}ä¸ªæ ·æœ¬ç¼ºå¤±")
                all_good = False
            else:
                print(f"  âœ… {field}: å®Œæ•´")

    if all_good:
        print("  âœ… æ‰€æœ‰å¿…éœ€å­—æ®µå®Œæ•´")

    # Codeæ ·æœ¬ç¤ºä¾‹
    code_samples = [s for s in samples if s.get('problem_type') == 'code']
    if len(code_samples) > 0:
        print(f"\nğŸ“ Codeæ ·æœ¬ç¤ºä¾‹ (å‰2ä¸ª):")
        for i in range(min(2, len(code_samples))):
            s = code_samples[i]
            print(f"\n  æ ·æœ¬{i+1}:")
            print(f"    problemé•¿åº¦: {len(s.get('problem', ''))}")
            print(f"    entry_point: {s.get('entry_point', 'N/A')[:50]}")
            print(f"    testé•¿åº¦: {len(s.get('test', ''))}")
            print(f"    task_id: {s.get('task_id', 'N/A')}")

    return {
        'total': len(samples),
        'types': dict(types),
        'code_count': code_count
    }

def main():
    print("\n" + "#"*70)
    print("# æ•°æ®é›†å®Œæ•´æ€§éªŒè¯")
    print("#"*70 + "\n")

    # æ£€æŸ¥è®­ç»ƒé›†
    train_original = check_dataset(
        "data/train/mixed_dataset.jsonl",
        "åŸå§‹è®­ç»ƒé›†"
    )

    print("\n")

    train_augmented = check_dataset(
        "data/train/mixed_dataset_augmented.jsonl",
        "å¢å¼ºè®­ç»ƒé›†"
    )

    print("\n")

    # æ£€æŸ¥æµ‹è¯•é›†
    test = check_dataset(
        "data/test/mixed_dataset.jsonl",
        "æµ‹è¯•é›†"
    )

    # å¯¹æ¯”æ€»ç»“
    print("\n" + "="*70)
    print("ğŸ“Š æ•°æ®é›†å¯¹æ¯”æ€»ç»“")
    print("="*70)

    if train_original and train_augmented:
        print(f"\nåŸå§‹è®­ç»ƒé›†: {train_original['total']:,} samples")
        print(f"  Code: {train_original['code_count']} ({train_original['code_count']/train_original['total']*100:.2f}%)")

        print(f"\nå¢å¼ºè®­ç»ƒé›†: {train_augmented['total']:,} samples")
        print(f"  Code: {train_augmented['code_count']} ({train_augmented['code_count']/train_augmented['total']*100:.2f}%)")
        print(f"  æå‡: {train_augmented['code_count']/train_original['code_count']:.1f}x")

    if test:
        print(f"\næµ‹è¯•é›†: {test['total']:,} samples")
        print(f"  Code: {test['code_count']} ({test['code_count']/test['total']*100:.2f}%)")
        print(f"  âš ï¸  Codeæµ‹è¯•æ ·æœ¬è¿‡å°‘!")

    print("\n" + "="*70)
    print("ğŸ’¡ å»ºè®®")
    print("="*70)

    if train_original and train_original['code_count'] < 1000:
        print("\nâš ï¸  å½“å‰ä½¿ç”¨çš„æ˜¯åŸå§‹è®­ç»ƒé›† (Codeåªæœ‰0.09%)")
        print("å»ºè®®åˆ‡æ¢åˆ°å¢å¼ºè®­ç»ƒé›†:")
        print("  ./switch_to_augmented_data.sh")
        print("  æˆ–")
        print("  cp data/train/mixed_dataset_augmented.jsonl data/train/mixed_dataset.jsonl")

    if test and test['code_count'] < 100:
        print("\nâš ï¸  æµ‹è¯•é›†Codeæ ·æœ¬è¿‡å°‘ï¼Œè¯„ä¼°ä¸å‡†ç¡®")
        print("å»ºè®®å¢å¼ºæµ‹è¯•é›†æˆ–æ·»åŠ HumanEvalæµ‹è¯•æ ·æœ¬")

if __name__ == "__main__":
    main()
