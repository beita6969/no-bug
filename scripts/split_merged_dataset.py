#!/usr/bin/env python3
"""
å°†merged_aflow_dataset.jsonlåˆ†å‰²ä¸ºè®­ç»ƒ/éªŒè¯/æµ‹è¯•é›†
åˆ†å‰²æ¯”ä¾‹ï¼š80% train / 10% val / 10% test
"""

import json
import random
from pathlib import Path

def split_dataset(input_file: str, train_ratio: float = 0.8, val_ratio: float = 0.1, test_ratio: float = 0.1, seed: int = 42):
    """Split dataset into train/val/test"""
    random.seed(seed)

    # Load all samples
    samples = []
    print(f"æ­£åœ¨è¯»å– {input_file}...")
    with open(input_file, 'r') as f:
        for i, line in enumerate(f):
            try:
                sample = json.loads(line)
                samples.append(sample)
            except json.JSONDecodeError:
                continue

    total_samples = len(samples)
    print(f"âœ“ è¯»å–äº† {total_samples} ä¸ªæ ·æœ¬")

    # Shuffle
    random.shuffle(samples)

    # Split indices
    train_size = int(total_samples * train_ratio)
    val_size = int(total_samples * val_ratio)

    train_samples = samples[:train_size]
    val_samples = samples[train_size:train_size + val_size]
    test_samples = samples[train_size + val_size:]

    # Create directories
    Path("data/train").mkdir(parents=True, exist_ok=True)
    Path("data/val").mkdir(parents=True, exist_ok=True)
    Path("data/test").mkdir(parents=True, exist_ok=True)

    # Write train set
    with open("data/train/mixed_dataset.jsonl", 'w') as f:
        for sample in train_samples:
            f.write(json.dumps(sample) + '\n')
    print(f"âœ… è®­ç»ƒé›†: {len(train_samples)} æ ·æœ¬ â†’ data/train/mixed_dataset.jsonl")

    # Write val set
    with open("data/val/mixed_dataset.jsonl", 'w') as f:
        for sample in val_samples:
            f.write(json.dumps(sample) + '\n')
    print(f"âœ… éªŒè¯é›†: {len(val_samples)} æ ·æœ¬ â†’ data/val/mixed_dataset.jsonl")

    # Write test set
    with open("data/test/mixed_dataset.jsonl", 'w') as f:
        for sample in test_samples:
            f.write(json.dumps(sample) + '\n')
    print(f"âœ… æµ‹è¯•é›†: {len(test_samples)} æ ·æœ¬ â†’ data/test/mixed_dataset.jsonl")

    # Statistics
    print("\n" + "="*70)
    print("ğŸ“Š æ•°æ®é›†åˆ†å‰²ç»Ÿè®¡:")
    print("="*70)
    print(f"æ€»æ ·æœ¬æ•°: {total_samples}")
    print(f"è®­ç»ƒé›†: {len(train_samples)} ({len(train_samples)/total_samples*100:.1f}%)")
    print(f"éªŒè¯é›†: {len(val_samples)} ({len(val_samples)/total_samples*100:.1f}%)")
    print(f"æµ‹è¯•é›†: {len(test_samples)} ({len(test_samples)/total_samples*100:.1f}%)")

    # Count by problem type
    print("\n" + "="*70)
    print("ğŸ“ˆ æŒ‰é—®é¢˜ç±»å‹ç»Ÿè®¡:")
    print("="*70)

    type_stats = {}
    for samples_list, split_name in [(train_samples, "è®­ç»ƒ"), (val_samples, "éªŒè¯"), (test_samples, "æµ‹è¯•")]:
        type_count = {}
        for sample in samples_list:
            ptype = sample.get('problem_type', 'unknown')
            type_count[ptype] = type_count.get(ptype, 0) + 1

        print(f"\n{split_name}é›†:")
        for ptype, count in sorted(type_count.items()):
            percentage = count / len(samples_list) * 100
            print(f"  {ptype}: {count} ({percentage:.1f}%)")


if __name__ == "__main__":
    split_dataset("merged_aflow_dataset.jsonl")
    print("\nâœ¨ æ•°æ®åˆ†å‰²å®Œæˆ!")
