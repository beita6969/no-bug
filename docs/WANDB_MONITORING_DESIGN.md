# WandBç›‘æ§ç³»ç»Ÿè®¾è®¡æ–¹æ¡ˆ

## ğŸ“Š ç³»ç»Ÿç ”ç©¶æ€»ç»“

### 1. å½“å‰è®­ç»ƒæ¶æ„åˆ†æ

#### è®­ç»ƒå¾ªç¯ç»“æ„ (`src/grpo_trainer.py`)
- **ä¸»è®­ç»ƒå¾ªç¯**: `train()` æ–¹æ³• (ç¬¬763-811è¡Œ)
  - æ¯ä¸ªstepè°ƒç”¨ `train_step()`
  - æ”¯æŒéªŒè¯é›†è¯„ä¼° (`evaluate_on_val_set()`)
  - å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹

- **å•æ­¥è®­ç»ƒ**: `train_step()` æ–¹æ³• (ç¬¬278-515è¡Œ)
  - é‡‡æ ·batch (æ”¯æŒæ··åˆæ•°æ®é›†)
  - ä¸ºæ¯ä¸ªé—®é¢˜ç”ŸæˆKä¸ªå·¥ä½œæµ (GRPOç»„)
  - æ‰§è¡Œå·¥ä½œæµå¹¶è®¡ç®—å¥–åŠ±
  - ç­–ç•¥æ¢¯åº¦æ›´æ–°

#### æ•°æ®é›†æ¶æ„
å½“å‰ç³»ç»Ÿæ”¯æŒä»¥ä¸‹æ•°æ®é›†:

| æ•°æ®é›† | ç±»å‹ | sourceå­—æ®µ | è¯„ä¼°æ–¹å¼ |
|--------|------|-----------|----------|
| GSM8K | math | "gsm8k" | æ•°å€¼åŒ¹é… + LLM Judge |
| MATH | math | "math" | æ•°å€¼åŒ¹é… + LLM Judge |
| HumanEval | code | "humaneval" | æµ‹è¯•æ‰§è¡Œ |
| MBPP | code | "mbpp" | æµ‹è¯•æ‰§è¡Œ (å·²è¿‡æ»¤) |
| HotpotQA | qa | "hotpotqa" | TokenåŒ¹é… + LLM Judge |
| CommonsenseQA | qa | "commonsenseqa" | é€‰é¡¹åŒ¹é… + LLM Judge |
| MMLU | qa | "mmlu" | é€‰é¡¹åŒ¹é… + LLM Judge |

#### å¥–åŠ±è®¡ç®—ç³»ç»Ÿ (`src/reward_computer.py`)
- **äºŒå…ƒå¥–åŠ±**: æ­£ç¡®=1.0, é”™è¯¯=0.0
- **LLM Judge**: ä½¿ç”¨GPT OSS 120B @ port 8002
- **æ•°æ®é›†ä¸“å±Prompt**: é€šè¿‡ `source` å­—æ®µé€‰æ‹©ä¸“å±è¯„ä¼°ç­–ç•¥
- **ç»Ÿè®¡è®¡æ•°å™¨**: `eval_stats` è·Ÿè¸ªLLM JudgeæˆåŠŸç‡

#### å½“å‰WandBé›†æˆçŠ¶æ€
- âœ… åŸºç¡€é›†æˆå·²å®Œæˆ (ç¬¬71-132è¡Œ)
- âœ… è®­ç»ƒæŒ‡æ ‡è®°å½• (ç¬¬492-513è¡Œ)
- âœ… æ ·æœ¬çº§è®°å½• (ç¬¬379-385è¡Œ)
- âŒ **ç¼ºå°‘**: æ•°æ®é›†ç»´åº¦çš„ç»†åˆ†ç»Ÿè®¡
- âŒ **ç¼ºå°‘**: LLM Judgeæ€§èƒ½ç›‘æ§
- âŒ **ç¼ºå°‘**: éªŒè¯é›†è¯¦ç»†åˆ†æ

---

## ğŸ¯ ç›‘æ§æŒ‡æ ‡è®¾è®¡

### 1. æ ¸å¿ƒè®­ç»ƒæŒ‡æ ‡ (å·²å®ç°)

```python
wandb.log({
    "train/loss": loss,
    "train/kl_div": kl_div,
    "train/avg_reward": np.mean(all_rewards),
    "train/max_reward": np.max(all_rewards),
    "train/min_reward": np.min(all_rewards),
    "train/accuracy": accuracy,  # æ€»ä½“å‡†ç¡®ç‡
    "train/temperature": current_temp,
    "train/step": step,
})
```

### 2. é—®é¢˜ç±»å‹ç»´åº¦ (å·²å®ç°)

```python
for ptype in ['math', 'code', 'qa']:
    wandb.log({
        f"train/accuracy_{ptype}": stats['accuracy'],
        f"train/avg_score_{ptype}": stats['avg_score'],
        f"train/count_{ptype}": stats['count'],
    })
```

### 3. **æ•°æ®é›†ç»´åº¦ (éœ€æ–°å¢)** â­

è¿™æ˜¯å…³é”®æ”¹è¿›ç‚¹ï¼éœ€è¦ä¸ºæ¯ä¸ªæ•°æ®é›†å•ç‹¬ç»Ÿè®¡å‡†ç¡®ç‡:

```python
# ç›®æ ‡ç›‘æ§ç»“æ„
wandb.log({
    # GSM8K
    "dataset/gsm8k/accuracy": 0.85,
    "dataset/gsm8k/count": 20,
    "dataset/gsm8k/avg_reward": 0.85,

    # MATH
    "dataset/math/accuracy": 0.42,
    "dataset/math/count": 15,
    "dataset/math/avg_reward": 0.42,

    # HotpotQA
    "dataset/hotpotqa/accuracy": 0.68,
    "dataset/hotpotqa/count": 18,
    "dataset/hotpotqa/avg_reward": 0.68,

    # HumanEval
    "dataset/humaneval/accuracy": 0.55,
    "dataset/humaneval/count": 12,
    "dataset/humaneval/avg_reward": 0.55,

    # ... (å…¶ä»–æ•°æ®é›†)
})
```

### 4. LLM Judgeæ€§èƒ½ç›‘æ§ (éœ€æ–°å¢)

```python
wandb.log({
    # JudgeæˆåŠŸç‡
    "judge/success_rate": judge_success / total_evals,
    "judge/parse_failure_rate": parse_failures / total_evals,
    "judge/api_failure_rate": api_failures / total_evals,

    # Judgeåˆ¤å†³åˆ†å¸ƒ
    "judge/correct_ratio": correct_preds / (correct_preds + incorrect_preds),
    "judge/total_calls": total_evals,

    # æŒ‰æ•°æ®é›†çš„Judgeæ€§èƒ½
    "judge/gsm8k_success_rate": ...,
    "judge/hotpotqa_success_rate": ...,
})
```

### 5. éªŒè¯é›†è¯¦ç»†ç›‘æ§ (éœ€å¢å¼º)

```python
# éªŒè¯é›†æ€»ä½“æŒ‡æ ‡
wandb.log({
    "val/accuracy": val_accuracy,
    "val/avg_correctness": avg_correctness,
    "val/success_rate": success_rate,
    "val/avg_cost": avg_cost,
})

# éªŒè¯é›†æŒ‰æ•°æ®é›†åˆ†è§£
for source in ['gsm8k', 'math', 'hotpotqa', 'humaneval', ...]:
    wandb.log({
        f"val/{source}/accuracy": ...,
        f"val/{source}/count": ...,
        f"val/{source}/avg_cost": ...,
    })
```

### 6. æˆæœ¬ç»Ÿè®¡ç›‘æ§

```python
wandb.log({
    "cost/total_cost": cumulative_cost,
    "cost/avg_cost_per_sample": avg_cost,
    "cost/executor_calls": total_executor_calls,
    "cost/judge_calls": total_judge_calls,
})
```

---

## ğŸ”§ å®ç°æ–¹æ¡ˆ

### æ–¹æ¡ˆA: æœ€å°ä¾µå…¥å¼æ”¹è¿› (æ¨è)

åœ¨ `train_step()` ä¸­æ·»åŠ æ•°æ®é›†ç»´åº¦ç»Ÿè®¡:

```python
# åœ¨train_step()ä¸­ (ç¬¬456è¡Œå)

# 3. æŒ‰æ•°æ®é›†ç»Ÿè®¡å‡†ç¡®ç‡ (æ–°å¢)
dataset_stats = {}  # {source: {'correct': [], 'rewards': []}}

for sample_idx, sample in enumerate(batch):
    source = sample.get('source', 'unknown')  # è·å–æ•°æ®é›†æ¥æº

    # åˆå§‹åŒ–æ•°æ®é›†ç»Ÿè®¡
    if source not in dataset_stats:
        dataset_stats[source] = {'correct': [], 'rewards': []}

    # ... (æ‰§è¡Œå·¥ä½œæµï¼Œè®¡ç®—å¥–åŠ±)

    # è®°å½•åˆ°æ•°æ®é›†ç»Ÿè®¡
    dataset_stats[source]['correct'].append(correctness > 0.9)
    dataset_stats[source]['rewards'].append(reward)

# 4. è®¡ç®—æ•°æ®é›†ç»´åº¦æŒ‡æ ‡
for source, stats in dataset_stats.items():
    if stats['correct']:
        dataset_accuracy = sum(stats['correct']) / len(stats['correct']) * 100
        dataset_avg_reward = np.mean(stats['rewards'])

        wandb.log({
            f"dataset/{source}/accuracy": dataset_accuracy,
            f"dataset/{source}/count": len(stats['correct']),
            f"dataset/{source}/avg_reward": dataset_avg_reward,
        }, step=step)
```

### æ–¹æ¡ˆB: å…¨é¢é‡æ„ (å¯é€‰)

åˆ›å»ºä¸“é—¨çš„ `MetricsCollector` ç±»:

```python
class MetricsCollector:
    """ç»Ÿä¸€çš„æŒ‡æ ‡æ”¶é›†å™¨"""

    def __init__(self):
        self.reset()

    def reset(self):
        self.samples = []
        self.by_problem_type = defaultdict(list)
        self.by_dataset = defaultdict(list)
        self.judge_stats = {
            'success': 0, 'parse_failures': 0, 'api_failures': 0
        }

    def add_sample(self, sample: Dict, result: Dict):
        """æ·»åŠ å•ä¸ªæ ·æœ¬ç»“æœ"""
        self.samples.append(result)

        # æŒ‰é—®é¢˜ç±»å‹
        ptype = sample['problem_type']
        self.by_problem_type[ptype].append(result)

        # æŒ‰æ•°æ®é›†
        source = sample.get('source', 'unknown')
        self.by_dataset[source].append(result)

    def get_wandb_logs(self, step: int) -> Dict:
        """ç”ŸæˆWandBæ—¥å¿—å­—å…¸"""
        logs = {'step': step}

        # æ€»ä½“æŒ‡æ ‡
        logs['train/accuracy'] = self._compute_accuracy(self.samples)

        # é—®é¢˜ç±»å‹ç»´åº¦
        for ptype, results in self.by_problem_type.items():
            logs[f'train/accuracy_{ptype}'] = self._compute_accuracy(results)

        # æ•°æ®é›†ç»´åº¦
        for source, results in self.by_dataset.items():
            logs[f'dataset/{source}/accuracy'] = self._compute_accuracy(results)
            logs[f'dataset/{source}/count'] = len(results)

        return logs

    def _compute_accuracy(self, results: List[Dict]) -> float:
        if not results:
            return 0.0
        correct = sum(1 for r in results if r.get('correctness', 0) > 0.9)
        return correct / len(results) * 100
```

---

## ğŸ“ˆ WandBä»ªè¡¨æ¿é…ç½®

### 1. è®­ç»ƒæ€»è§ˆé¢æ¿

```yaml
panels:
  - type: line
    title: "Training Loss & KL Divergence"
    metrics:
      - train/loss
      - train/kl_div

  - type: line
    title: "Overall Accuracy"
    metrics:
      - train/accuracy
      - val/accuracy

  - type: scalar
    title: "Current Step"
    metric: train/step
```

### 2. æ•°æ®é›†æ€§èƒ½é¢æ¿ (æ–°å¢)

```yaml
panels:
  - type: bar
    title: "Accuracy by Dataset (Latest)"
    metrics:
      - dataset/gsm8k/accuracy
      - dataset/math/accuracy
      - dataset/hotpotqa/accuracy
      - dataset/humaneval/accuracy
      - dataset/commonsenseqa/accuracy
      - dataset/mmlu/accuracy

  - type: line
    title: "GSM8K Performance Over Time"
    metrics:
      - dataset/gsm8k/accuracy
      - dataset/gsm8k/avg_reward

  - type: line
    title: "MATH Performance Over Time"
    metrics:
      - dataset/math/accuracy
      - dataset/math/avg_reward

  - type: table
    title: "Dataset Statistics"
    columns:
      - source
      - accuracy
      - count
      - avg_reward
```

### 3. LLM Judgeç›‘æ§é¢æ¿ (æ–°å¢)

```yaml
panels:
  - type: line
    title: "LLM Judge Success Rate"
    metrics:
      - judge/success_rate
      - judge/parse_failure_rate
      - judge/api_failure_rate

  - type: pie
    title: "Judge Verdict Distribution"
    metrics:
      - judge/correct_predictions
      - judge/incorrect_predictions

  - type: bar
    title: "Judge Performance by Dataset"
    metrics:
      - judge/gsm8k_success_rate
      - judge/math_success_rate
      - judge/hotpotqa_success_rate
```

### 4. éªŒè¯é›†è¯¦ç»†é¢æ¿

```yaml
panels:
  - type: line
    title: "Validation Accuracy by Dataset"
    metrics:
      - val/gsm8k/accuracy
      - val/math/accuracy
      - val/hotpotqa/accuracy
      - val/humaneval/accuracy

  - type: scatter
    title: "Validation Cost vs Accuracy"
    x_axis: val/avg_cost
    y_axis: val/accuracy
```

---

## ğŸš€ å®ç°æ­¥éª¤

### Step 1: æ•°æ®é›†ç»´åº¦ç»Ÿè®¡ (ä¼˜å…ˆçº§: P0)

**æ–‡ä»¶**: `src/grpo_trainer.py`

**ä¿®æ”¹ä½ç½®**: `train_step()` æ–¹æ³• (ç¬¬278-515è¡Œ)

**ä»£ç æ”¹åŠ¨**:

```python
# åœ¨ç¬¬307è¡Œåæ·»åŠ 
dataset_stats = defaultdict(lambda: {'correctness': [], 'rewards': []})

# åœ¨ç¬¬312è¡Œçš„å¾ªç¯ä¸­ (for sample_idx, sample in enumerate...)
source = sample.get('source', 'unknown')  # è·å–æ•°æ®é›†æ¥æº

# åœ¨ç¬¬370-393è¡Œ (è®¡ç®—correctnesså)
# è®°å½•åˆ°æ•°æ®é›†ç»Ÿè®¡
dataset_stats[source]['correctness'].append(correctness)
dataset_stats[source]['rewards'].append(reward)

# åœ¨ç¬¬486è¡Œå (metricså­—å…¸å®šä¹‰å) æ·»åŠ 
# è®¡ç®—æ•°æ®é›†ç»´åº¦æŒ‡æ ‡
for source, stats in dataset_stats.items():
    if stats['correctness']:
        source_correct = sum(1 for c in stats['correctness'] if c >= 0.9)
        source_total = len(stats['correctness'])
        source_accuracy = (source_correct / source_total * 100) if source_total > 0 else 0.0
        source_avg_reward = np.mean(stats['rewards'])

        metrics[f'dataset_{source}_accuracy'] = source_accuracy
        metrics[f'dataset_{source}_count'] = source_total
        metrics[f'dataset_{source}_avg_reward'] = source_avg_reward

# åœ¨ç¬¬493è¡Œå (wandb_log_dataå®šï¿½ï¿½å) æ·»åŠ 
# æ·»åŠ æ•°æ®é›†ç»´åº¦æŒ‡æ ‡åˆ°wandb
for source, stats in dataset_stats.items():
    if stats['correctness']:
        source_correct = sum(1 for c in stats['correctness'] if c >= 0.9)
        source_total = len(stats['correctness'])
        source_accuracy = (source_correct / source_total * 100) if source_total > 0 else 0.0

        wandb_log_data[f"dataset/{source}/accuracy"] = source_accuracy
        wandb_log_data[f"dataset/{source}/count"] = source_total
        wandb_log_data[f"dataset/{source}/avg_reward"] = np.mean(stats['rewards'])
```

### Step 2: LLM Judgeæ€§èƒ½ç›‘æ§ (ä¼˜å…ˆçº§: P1)

**æ–‡ä»¶**: `src/reward_computer.py`

**ä¿®æ”¹ä½ç½®**: åœ¨ `train_step()` ä¸­å®šæœŸè¯»å–judgeç»Ÿè®¡

**ä»£ç æ”¹åŠ¨**:

```python
# åœ¨ grpo_trainer.py çš„ train_step() æœ«å°¾ (ç¬¬513è¡Œå)

# è·å–LLM Judgeç»Ÿè®¡ (å¦‚æœå¯ç”¨)
if self.reward_computer.use_llm_judge:
    judge_stats = self.reward_computer.eval_stats
    total = judge_stats['total_evaluations']

    if total > 0:
        wandb_log_data['judge/success_rate'] = judge_stats['llm_judge_success'] / total
        wandb_log_data['judge/parse_failure_rate'] = judge_stats['llm_judge_parse_failures'] / total
        wandb_log_data['judge/api_failure_rate'] = judge_stats['llm_judge_api_failures'] / total
        wandb_log_data['judge/total_calls'] = total

        judged = judge_stats['correct_predictions'] + judge_stats['incorrect_predictions']
        if judged > 0:
            wandb_log_data['judge/correct_ratio'] = judge_stats['correct_predictions'] / judged
```

### Step 3: éªŒè¯é›†æ•°æ®é›†åˆ†è§£ (ä¼˜å…ˆçº§: P1)

**æ–‡ä»¶**: `src/grpo_trainer.py`

**ä¿®æ”¹ä½ç½®**: `evaluate_on_val_set()` æ–¹æ³• (ç¬¬649-761è¡Œ)

**ä»£ç æ”¹åŠ¨**:

```python
# åœ¨ç¬¬674è¡Œåæ·»åŠ 
val_dataset_stats = defaultdict(lambda: {'correctness': [], 'cost': []})

# åœ¨ç¬¬678è¡Œçš„å¾ªç¯ä¸­
source = sample.get('source', 'unknown')

# åœ¨ç¬¬720-732è¡Œ (è®¡ç®—correctnesså)
val_dataset_stats[source]['correctness'].append(correctness)
val_dataset_stats[source]['cost'].append(cost)

# åœ¨ç¬¬752è¡Œå (metricså®šä¹‰å) æ·»åŠ 
# è®¡ç®—éªŒè¯é›†æ•°æ®é›†ç»´åº¦æŒ‡æ ‡
for source, stats in val_dataset_stats.items():
    if stats['correctness']:
        source_correct = sum(1 for c in stats['correctness'] if c >= 0.9)
        source_total = len(stats['correctness'])
        source_accuracy = (source_correct / source_total * 100) if source_total > 0 else 0.0

        metrics[f'val_{source}_accuracy'] = source_accuracy
        metrics[f'val_{source}_count'] = source_total
        metrics[f'val_{source}_avg_cost'] = np.mean(stats['cost'])

# åœ¨ç¬¬800è¡Œ (wandb.logè°ƒç”¨å¤„) æ·»åŠ æ•°æ®é›†ç»´åº¦æ—¥å¿—
val_dataset_logs = {}
for source, stats in val_dataset_stats.items():
    if stats['correctness']:
        source_correct = sum(1 for c in stats['correctness'] if c >= 0.9)
        source_total = len(stats['correctness'])
        source_accuracy = (source_correct / source_total * 100) if source_total > 0 else 0.0

        val_dataset_logs[f"val/{source}/accuracy"] = source_accuracy
        val_dataset_logs[f"val/{source}/count"] = source_total
        val_dataset_logs[f"val/{source}/avg_cost"] = np.mean(stats['cost'])

wandb.log(val_dataset_logs, step=step)
```

### Step 4: æˆæœ¬ç»Ÿè®¡ç´¯ç§¯ (ä¼˜å…ˆçº§: P2)

**æ–‡ä»¶**: `src/grpo_trainer.py`

**ä¿®æ”¹ä½ç½®**: `__init__()` å’Œ `train_step()`

**ä»£ç æ”¹åŠ¨**:

```python
# åœ¨ __init__() ä¸­æ·»åŠ  (ç¬¬79è¡Œå)
self.cumulative_stats = {
    'total_cost': 0.0,
    'total_samples': 0,
    'executor_calls': 0,
    'judge_calls': 0,
}

# åœ¨ train_step() ä¸­ç´¯ç§¯ç»Ÿè®¡ (ç¬¬353è¡Œå)
self.cumulative_stats['total_cost'] += cost
self.cumulative_stats['executor_calls'] += 1

# åœ¨ wandb.log() å‰æ·»åŠ  (ç¬¬513è¡Œå‰)
wandb_log_data['cost/total_cost'] = self.cumulative_stats['total_cost']
wandb_log_data['cost/avg_cost_per_sample'] = (
    self.cumulative_stats['total_cost'] / max(self.cumulative_stats['total_samples'], 1)
)
wandb_log_data['cost/executor_calls'] = self.cumulative_stats['executor_calls']
```

---

## ğŸ“Š é¢„æœŸæ•ˆæœ

### 1. WandB Dashboardé¢„è§ˆ

å®æ–½åï¼ŒWandBä»ªè¡¨æ¿å°†æ˜¾ç¤º:

#### è®­ç»ƒé¢æ¿
- âœ… æ€»ä½“å‡†ç¡®ç‡æ›²çº¿
- âœ… Losså’ŒKL divergenceæ›²çº¿
- âœ… é—®é¢˜ç±»å‹å‡†ç¡®ç‡ (math/code/qa)
- ğŸ†• **æ•°æ®é›†å‡†ç¡®ç‡** (GSM8K, MATH, HotpotQAç­‰)
- ğŸ†• **LLM Judgeæ€§èƒ½ç›‘æ§**

#### éªŒè¯é¢æ¿
- âœ… éªŒè¯é›†æ€»ä½“å‡†ç¡®ç‡
- ğŸ†• **éªŒè¯é›†æ•°æ®é›†åˆ†è§£**
- ğŸ†• **æˆæœ¬vså‡†ç¡®ç‡åˆ†æ**

#### ç»Ÿè®¡é¢æ¿
- ğŸ†• **æ•°æ®é›†æ ·æœ¬åˆ†å¸ƒ**
- ğŸ†• **JudgeæˆåŠŸç‡ç»Ÿè®¡**
- ğŸ†• **ç´¯ç§¯æˆæœ¬è¿½è¸ª**

### 2. å…³é”®é—®é¢˜çš„å¯è§æ€§

å®æ–½åå¯ä»¥å›ç­”:

1. **"GSM8Kå‡†ç¡®ç‡æ˜¯å¤šå°‘ï¼Ÿ"** â†’ `dataset/gsm8k/accuracy`
2. **"MATHæ•°æ®é›†æ€§èƒ½å¦‚ä½•ï¼Ÿ"** â†’ `dataset/math/accuracy`
3. **"HotpotQAçš„LLM JudgeæˆåŠŸç‡ï¼Ÿ"** â†’ `judge/hotpotqa_success_rate`
4. **"éªŒè¯é›†åœ¨å„æ•°æ®é›†çš„è¡¨ç°ï¼Ÿ"** â†’ `val/{source}/accuracy`
5. **"ç´¯ç§¯è®­ç»ƒæˆæœ¬ï¼Ÿ"** â†’ `cost/total_cost`

### 3. æ€§èƒ½å¼€é”€

é¢„è®¡å¼€é”€:
- **è®¡ç®—**: < 1% (ä»…ç»Ÿè®¡æ“ä½œ)
- **WandBä¸Šä¼ **: ~ 2-3 KB/step (æ–°å¢æŒ‡æ ‡)
- **å†…å­˜**: < 10 MB (ä¸´æ—¶ç»Ÿè®¡å­—å…¸)

---

## ğŸ” æµ‹è¯•ä¸éªŒè¯

### æµ‹è¯•æ­¥éª¤

1. **æœ¬åœ°æµ‹è¯•** (offlineæ¨¡å¼)
   ```bash
   # ä¿®æ”¹config/training.yaml
   wandb:
     enabled: true
     mode: offline  # æœ¬åœ°æµ‹è¯•

   # è¿è¡Œè®­ç»ƒ
   python train.py

   # æ£€æŸ¥ç¦»çº¿æ—¥å¿—
   ls wandb/offline-run-*
   ```

2. **éªŒè¯æŒ‡æ ‡å®Œæ•´æ€§**
   ```python
   # æ£€æŸ¥wandbæ—¥å¿—ä¸­æ˜¯å¦åŒ…å«æ‰€æœ‰æ–°æŒ‡æ ‡
   import wandb
   run = wandb.Api().run("project/run_id")

   # æ£€æŸ¥æ•°æ®é›†ç»´åº¦æŒ‡æ ‡
   assert 'dataset/gsm8k/accuracy' in run.summary
   assert 'dataset/math/accuracy' in run.summary
   assert 'dataset/hotpotqa/accuracy' in run.summary

   # æ£€æŸ¥Judgeç»Ÿè®¡
   assert 'judge/success_rate' in run.summary
   ```

3. **åœ¨çº¿æµ‹è¯•** (onlineæ¨¡å¼)
   ```bash
   # é…ç½®æ­£å¼ç¯å¢ƒ
   wandb:
     enabled: true
     mode: online
     project: "aflow-roll-integration"

   # è¿è¡Œè®­ç»ƒ
   python train.py
   ```

### éªŒè¯æ¸…å•

- [ ] è®­ç»ƒæ­¥æ­£å¸¸è®°å½•æ•°æ®é›†ç»´åº¦æŒ‡æ ‡
- [ ] éªŒè¯æ­¥æ­£å¸¸è®°å½•æ•°æ®é›†ç»´åº¦æŒ‡æ ‡
- [ ] LLM Judgeç»Ÿè®¡æ­£ç¡®ç´¯ç§¯
- [ ] æˆæœ¬ç»Ÿè®¡æ­£ç¡®ç´¯ç§¯
- [ ] WandBä»ªè¡¨æ¿å¯æ­£å¸¸å¯è§†åŒ–
- [ ] ç¦»çº¿æ¨¡å¼å¯æ­£å¸¸å·¥ä½œ
- [ ] åœ¨çº¿æ¨¡å¼å¯æ­£å¸¸åŒæ­¥

---

## ğŸ“ é…ç½®æ›´æ–°

**æ–‡ä»¶**: `config/training.yaml`

```yaml
# WandBé…ç½®æ›´æ–°
wandb:
  enabled: true
  project: "aflow-roll-integration"
  entity: "yao110002-sdfsdfsdfsdf-com"
  api_key: "b42ca0000cf06f97b05eba34f58823ad5f3122a4"
  mode: "online"  # onlineæˆ–offline

  # æ–°å¢ï¼šè‡ªå®šä¹‰ä»ªè¡¨æ¿é…ç½®
  dashboard:
    # æ•°æ®é›†åˆ—è¡¨ (ç”¨äºè‡ªåŠ¨ç”Ÿæˆç›‘æ§é¢æ¿)
    datasets:
      - gsm8k
      - math
      - hotpotqa
      - humaneval
      - commonsenseqa
      - mmlu

    # ç›‘æ§é¢‘ç‡
    log_frequency: 1  # æ¯stepè®°å½•

    # æ˜¯å¦å¯ç”¨è¯¦ç»†è°ƒè¯•æ—¥å¿—
    debug_logging: false
```

---

## ğŸ¯ æ€»ç»“

### æ ¸å¿ƒæ”¹è¿›

1. **æ•°æ®é›†ç»´åº¦ç›‘æ§** (P0)
   - ä¸ºæ¯ä¸ªæ•°æ®é›†å•ç‹¬ç»Ÿè®¡å‡†ç¡®ç‡
   - æ”¯æŒGSM8K, MATH, HotpotQAç­‰æ‰€æœ‰æ•°æ®é›†
   - è®­ç»ƒé›†å’ŒéªŒè¯é›†å‡æ”¯æŒ

2. **LLM Judgeæ€§èƒ½ç›‘æ§** (P1)
   - æˆåŠŸç‡ã€å¤±è´¥ç‡ç»Ÿè®¡
   - åˆ¤å†³åˆ†å¸ƒåˆ†æ
   - æŒ‰æ•°æ®é›†çš„Judgeæ€§èƒ½

3. **éªŒè¯é›†è¯¦ç»†åˆ†æ** (P1)
   - æŒ‰æ•°æ®é›†åˆ†è§£çš„éªŒè¯æ€§èƒ½
   - æˆæœ¬vså‡†ç¡®ç‡å…³è”åˆ†æ

4. **æˆæœ¬ç»Ÿè®¡** (P2)
   - ç´¯ç§¯æˆæœ¬è¿½è¸ª
   - å¹³å‡æˆæœ¬peræ ·æœ¬
   - Executor/Judgeè°ƒç”¨æ¬¡æ•°

### å®æ–½ä¼˜å…ˆçº§

1. **P0 (ç«‹å³å®æ–½)**: æ•°æ®é›†ç»´åº¦ç»Ÿè®¡
2. **P1 (çŸ­æœŸ)**: LLM Judgeç›‘æ§ + éªŒè¯é›†åˆ†è§£
3. **P2 (ä¸­æœŸ)**: æˆæœ¬ç»Ÿè®¡ç´¯ç§¯

### é¢„æœŸæ”¶ç›Š

- âœ… **å®Œæ•´çš„æ•°æ®é›†çº§æ€§èƒ½å¯è§æ€§**
- âœ… **LLM Judgeè´¨é‡ç›‘æ§**
- âœ… **éªŒè¯é›†è¯¦ç»†è¯Šæ–­èƒ½åŠ›**
- âœ… **æˆæœ¬è¿½è¸ªå’Œä¼˜åŒ–ä¾æ®**

---

## é™„å½•

### A. æ•°æ®é›†sourceå­—æ®µæ˜ å°„

```python
DATASET_SOURCE_MAPPING = {
    # Math datasets
    'gsm8k': 'math',
    'math': 'math',

    # Code datasets
    'humaneval': 'code',
    'mbpp': 'code',  # (å·²è¿‡æ»¤)

    # QA datasets
    'hotpotqa': 'qa',
    'commonsenseqa': 'qa',
    'mmlu': 'qa',
}
```

### B. WandB APIæŸ¥è¯¢ç¤ºä¾‹

```python
import wandb

api = wandb.Api()
run = api.run("entity/project/run_id")

# æŸ¥è¯¢ç‰¹å®šæ•°æ®é›†çš„å‡†ç¡®ç‡å†å²
history = run.history(keys=['dataset/gsm8k/accuracy'])
print(history)

# æŸ¥è¯¢æœ€æ–°çš„æ‰€æœ‰æ•°æ®é›†å‡†ç¡®ç‡
for source in ['gsm8k', 'math', 'hotpotqa', 'humaneval']:
    accuracy = run.summary.get(f'dataset/{source}/accuracy', 0)
    print(f"{source}: {accuracy:.2f}%")
```

### C. æ•…éšœæ’æŸ¥

**é—®é¢˜1**: æ•°æ®é›†ç»Ÿè®¡ä¸ºç©º
- **åŸå› **: batchä¸­ç¼ºå°‘è¯¥æ•°æ®é›†çš„æ ·æœ¬
- **è§£å†³**: æ£€æŸ¥ `domain_ratios` é…ç½®

**é—®é¢˜2**: LLM Judgeç»Ÿè®¡ä¸æ›´æ–°
- **åŸå› **: `use_llm_judge=False`
- **è§£å†³**: åœ¨ `config/training.yaml` ä¸­å¯ç”¨Judge

**é—®é¢˜3**: WandBç¦»çº¿æ—¥å¿—æœªç”Ÿæˆ
- **åŸå› **: `mode` æœªè®¾ç½®ä¸º `offline`
- **è§£å†³**: ä¿®æ”¹é…ç½®æ–‡ä»¶æˆ–è®¾ç½®ç¯å¢ƒå˜é‡ `WANDB_MODE=offline`

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0
**åˆ›å»ºæ—¶é—´**: 2025-11-23
**ç»´æŠ¤è€…**: AI Training Team
